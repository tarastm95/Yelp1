"""
üîç Vector Search Service for Sample Replies
–°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –ø–æ PDF —á–∞–Ω–∫–∞—Ö –∑ pgvector —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
"""

import logging
from typing import List, Dict, Optional, Tuple
from django.db import connection
from django.db.models import Q
import openai
import os

logger = logging.getLogger(__name__)

class VectorSearchService:
    """–°–µ—Ä–≤—ñ—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É —á–µ—Ä–µ–∑ sample reply chunks"""
    
    def __init__(self):
        self.openai_client = None
        self._init_openai()
    
    def _init_openai(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞ –¥–ª—è query –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤"""
        try:
            from .models import AISettings
            
            # Try environment variable first
            openai_api_key = os.getenv('OPENAI_API_KEY')
            
            if not openai_api_key:
                # Try database settings
                ai_settings = AISettings.objects.first()
                if ai_settings and ai_settings.openai_api_key:
                    openai_api_key = ai_settings.openai_api_key
            
            if openai_api_key:
                self.openai_client = openai.OpenAI(api_key=openai_api_key)
                logger.info("[VECTOR-SEARCH] OpenAI client initialized successfully")
            else:
                logger.error("[VECTOR-SEARCH] No OpenAI API key found")
                
        except Exception as e:
            logger.error(f"[VECTOR-SEARCH] Error in vector search: {e}")
    
    def generate_query_embedding(self, query_text: str) -> List[float]:
        """–ì–µ–Ω–µ—Ä—É—î –µ–º–±–µ–¥—ñ–Ω–≥ –¥–ª—è –ø–æ—à—É–∫–æ–≤–æ–≥–æ –∑–∞–ø–∏—Ç—É"""
        if not self.openai_client:
            raise ValueError("OpenAI client not initialized")
        
        try:
            logger.info(f"[VECTOR-SEARCH] Generating embedding for query: {query_text[:100]}...")
            
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=[query_text],
                dimensions=1536
            )
            
            embedding = response.data[0].embedding
            logger.info(f"[VECTOR-SEARCH] Generated embedding with {len(embedding)} dimensions")
            return embedding
            
        except Exception as e:
            logger.error(f"[VECTOR-SEARCH] Error in vector search: {e}")
            raise
    
    def _get_vector_search_settings(self, business_id: str, phone_available: bool = False) -> Dict:
        """–û—Ç—Ä–∏–º—É—î –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è vector search –∑ AutoResponseSettings"""
        try:
            from .models import AutoResponseSettings, YelpBusiness
            
            business = YelpBusiness.objects.get(business_id=business_id)
            settings = AutoResponseSettings.objects.filter(
                business=business,
                phone_available=phone_available
            ).first()
            
            if settings:
                return {
                    'similarity_threshold': settings.vector_similarity_threshold,
                    'search_limit': settings.vector_search_limit,
                    'chunk_types': settings.vector_chunk_types if settings.vector_chunk_types else None
                }
            else:
                # Fallback –∑–Ω–∞—á–µ–Ω–Ω—è
                return {
                    'similarity_threshold': 0.6,
                    'search_limit': 5,
                    'chunk_types': None
                }
                
        except Exception as e:
            logger.warning(f"[VECTOR-SEARCH] Could not load settings for {business_id}: {e}")
            # Fallback –∑–Ω–∞—á–µ–Ω–Ω—è
            return {
                'similarity_threshold': 0.6,
                'search_limit': 5,
                'chunk_types': None
            }
    
    def search_similar_chunks(
        self,
        query_text: str,
        business_id: str,
        location_id: Optional[str] = None,
        limit: int = 5,
        similarity_threshold: float = 0.6,
        chunk_types: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        –®—É–∫–∞—î —Å—Ö–æ–∂—ñ —á–∞–Ω–∫–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó —Å—Ö–æ–∂–æ—Å—Ç—ñ
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø–∏—Ç—É –ª—ñ–¥–∞ –¥–ª—è –ø–æ—à—É–∫—É
            business_id: –§—ñ–ª—å—Ç—Ä –ø–æ business ID
            location_id: –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π —Ñ—ñ–ª—å—Ç—Ä –ª–æ–∫–∞—Ü—ñ—ó
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            similarity_threshold: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —Ä—ñ–≤–µ–Ω—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ—ó —Å—Ö–æ–∂–æ—Å—Ç—ñ
            chunk_types: –§—ñ–ª—å—Ç—Ä –ø–æ —Ç–∏–ø–∞—Ö —á–∞–Ω–∫—ñ–≤ (inquiry, response, example, general)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å—Ö–æ–∂–∏—Ö —á–∞–Ω–∫—ñ–≤ –∑ –æ—Ü—ñ–Ω–∫–∞–º–∏ —Å—Ö–æ–∂–æ—Å—Ç—ñ —Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
        """
        
        logger.info(f"[VECTOR-SEARCH] ========== VECTOR SEARCH ==========")
        logger.info(f"[VECTOR-SEARCH] Query: {query_text[:200]}...")
        logger.info(f"[VECTOR-SEARCH] Business ID: {business_id}")
        logger.info(f"[VECTOR-SEARCH] Location ID: {location_id}")
        logger.info(f"[VECTOR-SEARCH] Limit: {limit}")
        logger.info(f"[VECTOR-SEARCH] Similarity threshold: {similarity_threshold}")
        logger.info(f"[VECTOR-SEARCH] Chunk types filter: {chunk_types}")
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self.generate_query_embedding(query_text)
            
            logger.info(f"[VECTOR-SEARCH] üß† Using Python-based similarity calculation (no giant SQL!)")
            
            # PYTHON-BASED approach: –æ—Ç—Ä–∏–º—É—î–º–æ chunks —ñ —Ä–∞—Ö—É—î–º–æ similarity
            from .vector_models import VectorChunk
            import numpy as np
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è similarity calculation
            chunks_queryset = VectorChunk.objects.select_related('document').filter(
                document__business_id=business_id,
                document__processing_status='completed'
            ).exclude(embedding__isnull=True)
            
            if location_id:
                chunks_queryset = chunks_queryset.filter(document__location_id=location_id)
            
            if chunk_types:
                chunks_queryset = chunks_queryset.filter(chunk_type__in=chunk_types)
                logger.info(f"[VECTOR-SEARCH] Filtering by chunk types: {chunk_types}")
            
            logger.info(f"[VECTOR-SEARCH] üìä Candidate chunks: {chunks_queryset.count()}")
            
            # –û–±—á–∏—Å–ª—é—î–º–æ similarity –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ chunk –≤ Python
            results = []
            query_embedding_np = np.array(query_embedding)
            processed_count = 0
            
            for chunk in chunks_queryset:
                try:
                    if chunk.embedding is not None and len(chunk.embedding) == 1536:
                        # Cosine similarity –≤ Python (–Ω–∞–¥—ñ–π–Ω–∏–π –º–µ—Ç–æ–¥)
                        chunk_embedding_np = np.array(chunk.embedding)
                        
                        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è cosine similarity
                        dot_product = np.dot(chunk_embedding_np, query_embedding_np)
                        norm_chunk = np.linalg.norm(chunk_embedding_np)
                        norm_query = np.linalg.norm(query_embedding_np)
                        
                        if float(norm_chunk) > 0.0 and float(norm_query) > 0.0:
                            similarity = float(dot_product / (norm_chunk * norm_query))
                            processed_count += 1
                            
                            if similarity >= similarity_threshold:
                                results.append({
                                    'chunk_id': chunk.id,
                                    'content': chunk.content,
                                    'page_number': chunk.page_number,
                                    'chunk_index': chunk.chunk_index,
                                    'token_count': chunk.token_count,
                                    'chunk_type': chunk.chunk_type,
                                    'metadata': chunk.metadata,
                                    'filename': chunk.document.filename,
                                    'business_id': chunk.document.business_id,
                                    'location_id': chunk.document.location_id,
                                    'similarity_score': similarity
                                })
                                
                                logger.info(f"[VECTOR-SEARCH] ‚úÖ Chunk {chunk.id}: similarity={similarity:.4f} >= {similarity_threshold}")
                        else:
                            logger.warning(f"[VECTOR-SEARCH] ‚ö†Ô∏è Chunk {chunk.id}: zero norm vectors")
                            
                except Exception as chunk_error:
                    logger.warning(f"[VECTOR-SEARCH] ‚ö†Ô∏è Error processing chunk {chunk.id}: {chunk_error}")
                    continue
            
            logger.info(f"[VECTOR-SEARCH] üìä Processed {processed_count} chunks with embeddings")
            logger.info(f"[VECTOR-SEARCH] üéØ Found {len(results)} chunks above threshold {similarity_threshold}")
            
            # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ similarity score (–Ω–∞–π–∫—Ä–∞—â—ñ —Å–ø–æ—á–∞—Ç–∫—É)
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            results = results[:limit]
            
            logger.info(f"[VECTOR-SEARCH] üèÜ Returning top {len(results)} results")
            
            # –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            for i, result in enumerate(results[:3]):
                logger.info(f"[VECTOR-SEARCH] Result {i+1}:")
                logger.info(f"[VECTOR-SEARCH] - Similarity: {result['similarity_score']:.3f}")
                logger.info(f"[VECTOR-SEARCH] - Type: {result['chunk_type']}")
                logger.info(f"[VECTOR-SEARCH] - Content: {result['content'][:100]}...")
                logger.info(f"[VECTOR-SEARCH] - Page: {result['page_number']}")
            
            logger.info(f"[VECTOR-SEARCH] =================================")
            
            return results
                
        except Exception as e:
            logger.error(f"[VECTOR-SEARCH] Error in vector search: {e}")
            logger.exception("Vector search error details")
            return []
    
    def search_inquiry_response_pairs(
        self,
        query_text: str,
        business_id: str,
        location_id: Optional[str] = None,
        limit: int = 5,
        similarity_threshold: float = 0.6
    ) -> List[Dict]:
        """
        üéØ –ü–†–ê–í–ò–õ–¨–ù–ò–ô –ü–Ü–î–•–Ü–î: –ó–Ω–∞—Ö–æ–¥–∏—Ç—å —Å—Ö–æ–∂—ñ inquiry chunks, –ø–æ—Ç—ñ–º —ó—Ö –ø–∞—Ä–Ω—ñ response chunks
        
        Args:
            query_text: –ù–æ–≤–∏–π inquiry –≤—ñ–¥ –∫–ª—ñ—î–Ω—Ç–∞
            business_id: ID –±—ñ–∑–Ω–µ—Å—É
            location_id: –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π —Ñ—ñ–ª—å—Ç—Ä –ª–æ–∫–∞—Ü—ñ—ó
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å inquiry‚Üíresponse –ø–∞—Ä
            similarity_threshold: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —Ä—ñ–≤–µ–Ω—å —Å—Ö–æ–∂–æ—Å—Ç—ñ
            
        Returns:
            –°–ø–∏—Å–æ–∫ inquiry‚Üíresponse –ø–∞—Ä –∑ –æ—Ü—ñ–Ω–∫–∞–º–∏ —Å—Ö–æ–∂–æ—Å—Ç—ñ
        """
        
        logger.info(f"[VECTOR-SEARCH] ========== INQUIRY‚ÜíRESPONSE PAIR MATCHING ==========")
        logger.info(f"[VECTOR-SEARCH] Query: {query_text[:200]}...")
        logger.info(f"[VECTOR-SEARCH] Business ID: {business_id}")
        logger.info(f"[VECTOR-SEARCH] Limit: {limit}")
        logger.info(f"[VECTOR-SEARCH] Similarity threshold: {similarity_threshold}")
        
        try:
            # –ï–¢–ê–ü 1: –ó–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂—ñ inquiry chunks
            logger.info(f"[VECTOR-SEARCH] üîç –ï–¢–ê–ü 1: –ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö inquiry chunks...")
            
            similar_inquiries = self.search_similar_chunks(
                query_text=query_text,
                business_id=business_id,
                location_id=location_id,
                limit=limit * 2,  # –®—É–∫–∞—î–º–æ –±—ñ–ª—å—à–µ inquiry –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏–±–æ—Ä—É
                similarity_threshold=similarity_threshold,
                chunk_types=['inquiry']  # ‚úÖ –¢–Ü–õ–¨–ö–ò INQUIRY CHUNKS
            )
            
            if not similar_inquiries:
                logger.warning("[VECTOR-SEARCH] ‚ö†Ô∏è No similar inquiry chunks found above threshold")
                logger.info(f"[VECTOR-SEARCH] üìä Search parameters:")
                logger.info(f"[VECTOR-SEARCH]   - Query: {query_text[:100]}...")
                logger.info(f"[VECTOR-SEARCH]   - Threshold: {similarity_threshold}")
                logger.info(f"[VECTOR-SEARCH]   - Limit: {limit}")
                logger.info(f"[VECTOR-SEARCH] üí° Suggestions:")
                logger.info(f"[VECTOR-SEARCH]   - Lower similarity_threshold (e.g., 0.4-0.5)")
                logger.info(f"[VECTOR-SEARCH]   - Add more relevant examples to Sample Replies")
                logger.info(f"[VECTOR-SEARCH]   - Check if Sample Replies cover this topic")
                logger.info(f"[VECTOR-SEARCH] üîÑ System will fallback to Custom Instructions")
                return []
            
            logger.info(f"[VECTOR-SEARCH] Found {len(similar_inquiries)} similar inquiry chunks")
            
            # –ï–¢–ê–ü 2: –ó–Ω–∞–π—Ç–∏ –ø–∞—Ä–Ω—ñ response chunks
            logger.info(f"[VECTOR-SEARCH] üîó –ï–¢–ê–ü 2: –ü–æ—à—É–∫ –ø–∞—Ä–Ω–∏—Ö response chunks...")
            
            from .vector_models import VectorChunk
            inquiry_response_pairs = []
            
            for inquiry_chunk in similar_inquiries[:limit]:
                try:
                    # –ó–Ω–∞–π—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–∏–π response chunk –ø—ñ—Å–ª—è —Ü—å–æ–≥–æ inquiry
                    inquiry_chunk_obj = VectorChunk.objects.get(id=inquiry_chunk['chunk_id'])
                    
                    # –ü–æ—à—É–∫ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ response chunk –≤ —Ç–æ–º—É –∂ –¥–æ–∫—É–º–µ–Ω—Ç—ñ
                    next_response = VectorChunk.objects.filter(
                        document=inquiry_chunk_obj.document,
                        chunk_index__gt=inquiry_chunk_obj.chunk_index,  # –ü—ñ—Å–ª—è inquiry
                        chunk_type='response'  # –¢—ñ–ª—å–∫–∏ response
                    ).order_by('chunk_index').first()
                    
                    if next_response:
                        logger.info(f"[VECTOR-SEARCH] ‚úÖ Found pair: inquiry#{inquiry_chunk_obj.chunk_index} ‚Üí response#{next_response.chunk_index}")
                        
                        pair = {
                            'inquiry': {
                                'similarity_score': inquiry_chunk['similarity_score'],
                                'content': inquiry_chunk['content'],
                                'chunk_index': inquiry_chunk_obj.chunk_index,
                                'chunk_type': 'inquiry'
                            },
                            'response': {
                                'content': next_response.content,
                                'chunk_index': next_response.chunk_index,
                                'chunk_type': 'response',
                                'quality': next_response.metadata.get('chunk_quality', 'basic') if next_response.metadata else 'basic'
                            },
                            'pair_similarity': inquiry_chunk['similarity_score'],  # –ë–∞–∑—É—î—Ç—å—Å—è –Ω–∞ inquiry similarity
                            'pair_quality': self._assess_pair_quality(inquiry_chunk['content'], next_response.content)
                        }
                        
                        inquiry_response_pairs.append(pair)
                        
                    else:
                        logger.warning(f"[VECTOR-SEARCH] ‚ö†Ô∏è No response chunk found for inquiry#{inquiry_chunk_obj.chunk_index}")
                        
                except Exception as pair_error:
                    logger.error(f"[VECTOR-SEARCH] Error finding response pair: {pair_error}")
                    continue
            
            logger.info(f"[VECTOR-SEARCH] üéØ Found {len(inquiry_response_pairs)} complete inquiry‚Üíresponse pairs")
            
            # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ similarity —ñ —è–∫—ñ—Å—Ç—é
            inquiry_response_pairs.sort(key=lambda x: (x['pair_similarity'], x['pair_quality'] == 'excellent'), reverse=True)
            
            # –ï–¢–ê–ü 3: –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            logger.info(f"[VECTOR-SEARCH] üìä INQUIRY‚ÜíRESPONSE PAIRS RESULTS:")
            for i, pair in enumerate(inquiry_response_pairs[:3]):
                logger.info(f"[VECTOR-SEARCH] Pair {i+1}: similarity={pair['pair_similarity']:.3f}, quality={pair['pair_quality']}")
                logger.info(f"[VECTOR-SEARCH]   Inquiry: {pair['inquiry']['content'][:100]}...")
                logger.info(f"[VECTOR-SEARCH]   Response: {pair['response']['content'][:100]}...")
            
            logger.info(f"[VECTOR-SEARCH] =================================")
            return inquiry_response_pairs
            
        except Exception as e:
            logger.error(f"[VECTOR-SEARCH] Error in inquiry‚Üíresponse pair matching: {e}")
            logger.exception("Pair matching error details")
            return []
    
    def _assess_pair_quality(self, inquiry_content: str, response_content: str) -> str:
        """üéØ –û—Ü—ñ–Ω—é—î —è–∫—ñ—Å—Ç—å inquiry‚Üíresponse –ø–∞—Ä–∏"""
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ response –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î inquiry
        inquiry_lower = inquiry_content.lower()
        response_lower = response_content.lower()
        
        # –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ inquiry –º–∞—é—Ç—å –±—É—Ç–∏ –∑–≥–∞–¥–∞–Ω—ñ –≤ response
        service_keywords = ['roof', 'foundation', 'remodel', 'deck', 'bathroom', 'kitchen', 'addition']
        inquiry_services = [kw for kw in service_keywords if kw in inquiry_lower]
        response_mentions = [kw for kw in inquiry_services if kw in response_lower]
        
        service_match_ratio = len(response_mentions) / len(inquiry_services) if inquiry_services else 0
        
        # –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ response
        has_professional_greeting = any(x in response_lower for x in ['good afternoon', 'good morning', 'hello', 'hi'])
        has_business_info = any(x in response_lower for x in ['available', 'hours', 'monday', 'friday'])
        has_personal_touch = any(x in response_lower for x in ['talk soon', 'norma', 'best', 'glad'])
        
        quality_score = service_match_ratio + sum([has_professional_greeting, has_business_info, has_personal_touch])
        
        if quality_score >= 3:
            return 'excellent'
        elif quality_score >= 2:
            return 'good'
        else:
            return 'basic'
    
    def generate_contextual_response_from_pairs(
        self,
        lead_inquiry: str,
        customer_name: str,
        inquiry_response_pairs: List[Dict],
        business_name: str = "",
        max_response_length: int = None  # ‚úÖ –ó—Ä–æ–±–∏–º–æ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–º –¥–ª—è auto-detect
    ) -> str:
        """
        üéØ –ì–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ inquiry‚Üíresponse –ø–∞—Ä (–ü–†–ê–í–ò–õ–¨–ù–ò–ô –ü–Ü–î–•–Ü–î)
        
        Args:
            lead_inquiry: –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –∑–∞–ø–∏—Ç –∫–ª—ñ—î–Ω—Ç–∞
            customer_name: –Ü–º'—è –∫–ª—ñ—î–Ω—Ç–∞
            inquiry_response_pairs: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ inquiry‚Üíresponse pair matching
            business_name: –ù–∞–∑–≤–∞ –±—ñ–∑–Ω–µ—Å—É
            max_response_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ (None = auto-detect)
            
        Returns:
            –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É —Å—Ç–∏–ª—ñ –Ω–∞–π–∫—Ä–∞—â–∏—Ö inquiry‚Üíresponse –ø–∞—Ä
        """
        
        if not self.openai_client or not inquiry_response_pairs:
            logger.warning("[VECTOR-SEARCH] No OpenAI client or inquiry‚Üíresponse pairs for generation")
            return ""
        
        logger.info(f"[VECTOR-SEARCH] ========== CONTEXTUAL RESPONSE FROM PAIRS ==========")
        logger.info(f"[VECTOR-SEARCH] Lead inquiry: {lead_inquiry[:100]}...")
        logger.info(f"[VECTOR-SEARCH] Customer: {customer_name}")
        logger.info(f"[VECTOR-SEARCH] Business: {business_name}")
        logger.info(f"[VECTOR-SEARCH] Inquiry‚ÜíResponse pairs: {len(inquiry_response_pairs)}")
        
        # üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ–≤–∂–∏–Ω–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ response chunks –∑ inquiry_response_pairs
        response_chunks_for_length = []
        for pair in inquiry_response_pairs[:5]:
            if pair.get('response'):
                response_chunks_for_length.append({
                    'chunk_type': 'response',
                    'content': pair['response'].get('content', '')
                })
        
        avg_length, min_length, max_length = self.calculate_average_response_length(response_chunks_for_length)
        
        if max_response_length:
            target_length = min(avg_length, max_response_length)
            logger.info(f"[VECTOR-SEARCH] üìè Using capped length: {target_length} (max: {max_response_length})")
        else:
            target_length = avg_length
            logger.info(f"[VECTOR-SEARCH] üéØ Using AUTO-DETECTED length from pairs: {target_length} chars (range: {min_length}-{max_length})")
        
        try:
            # –ó–±–∏—Ä–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∑—ñ inquiry‚Üíresponse –ø–∞—Ä
            context_parts = []
            for i, pair in enumerate(inquiry_response_pairs[:3]):  # –¢–æ–ø 3 –ø–∞—Ä–∏
                similarity_score = pair['pair_similarity']
                pair_quality = pair['pair_quality']
                inquiry_content = pair['inquiry']['content']
                response_content = pair['response']['content']
                
                context_parts.append(
                    f"""EXAMPLE {i+1} (similarity: {similarity_score:.2f}, quality: {pair_quality}):
CUSTOMER INQUIRY: {inquiry_content}
BUSINESS RESPONSE: {response_content}
---"""
                )
            
            context = "\n".join(context_parts)
            
            # –°–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑ –ø–∞—Ä
            system_prompt = f"""You are a professional business communication assistant for {business_name}.

TASK: Generate a personalized response using INQUIRY‚ÜíRESPONSE pairs as training examples.

METHODOLOGY: You will receive examples showing how the business responds to specific customer inquiries. Study these inquiry‚Üíresponse patterns to understand:
1. How the business addresses different service types
2. The professional tone and communication style used
3. The structure and elements of good responses
4. How to personalize responses while maintaining consistency

TRAINING EXAMPLES (ranked by similarity to current inquiry):
{context}

INSTRUCTIONS:
1. Analyze the new customer inquiry and find the most similar training inquiry
2. Study how the business responded to that similar inquiry
3. Generate a NEW response following the same pattern, tone, LENGTH, and structure
4. Personalize with the customer's name and specific details
5. Match the NATURAL LENGTH shown in examples (approximately {avg_length} characters, ranging {min_length}-{max_length})
6. Maintain the professional, helpful tone shown in examples

CRITICAL: 
- Use the inquiry‚Üíresponse patterns as your guide
- Match the natural response length from the examples (don't artificially shorten or extend)
- The response should feel natural and personal while following the learned communication style"""
            
            # üïê Get time-based greeting
            from .utils import get_time_based_greeting
            from .models import YelpBusiness
            
            time_greeting = "Good morning"  # default
            try:
                business_obj = YelpBusiness.objects.filter(name=business_name).first()
                if business_obj:
                    time_greeting = get_time_based_greeting(business_obj.business_id)
            except Exception as e:
                logger.warning(f"[VECTOR-SEARCH] Could not get time greeting: {e}")
            
            logger.info(f"[VECTOR-SEARCH] üïê Time-based greeting: '{time_greeting}'")
            
            user_prompt = f"""NEW CUSTOMER INQUIRY:
Customer Name: {customer_name}
Inquiry Text: "{lead_inquiry}"
Business: {business_name}
Time-appropriate greeting: {time_greeting}

IMPORTANT: Start your response with "{time_greeting} {customer_name}" followed by a casual personal touch (e.g., "hope your day's going well").

Based on the training examples above, generate a professional response that:
1. Addresses this specific customer inquiry
2. Follows the communication patterns learned from similar inquiries
3. Matches the tone and style of the most similar business responses
4. Is personalized for {customer_name} and their specific needs"""
            
            logger.info(f"[VECTOR-SEARCH] Generating response from {len(inquiry_response_pairs)} inquiry‚Üíresponse pairs...")
            logger.info(f"[VECTOR-SEARCH] Top pair similarities: {[p['pair_similarity'] for p in inquiry_response_pairs[:3]]}")
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ max_tokens –Ω–∞ –æ—Å–Ω–æ–≤—ñ target_length
            estimated_tokens = max(50, target_length // 3)
            logger.info(f"[VECTOR-SEARCH] Estimated tokens for {target_length} chars: {estimated_tokens}")
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=estimated_tokens,  # ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Ä–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω–∏–π –ª—ñ–º—ñ—Ç
                temperature=0.7
            )
            
            generated_response = response.choices[0].message.content.strip()
            
            logger.info(f"[VECTOR-SEARCH] ==================== GENERATED RESPONSE ====================")
            logger.info(f"[VECTOR-SEARCH] üì§ Response: {generated_response}")
            logger.info(f"[VECTOR-SEARCH] " + "="*60)
            logger.info(f"[VECTOR-SEARCH] üìä Response length: {len(generated_response)} chars")
            logger.info(f"[VECTOR-SEARCH] üìä Used {len(inquiry_response_pairs)} pairs")
            
            # üîç –ê–ù–ê–õ–Ü–ó –î–û–¢–†–ò–ú–ê–ù–ù–Ø –ü–†–ê–í–ò–õ
            logger.info(f"[VECTOR-SEARCH] ==================== COMPLIANCE ANALYSIS ====================")
            
            # Check 1: Time-based greeting
            has_correct_greeting = generated_response.lower().startswith(time_greeting.lower())
            logger.info(f"[VECTOR-SEARCH] ‚úÖ Rule 1 - Time-based Greeting:")
            logger.info(f"[VECTOR-SEARCH]    Expected: '{time_greeting}'")
            logger.info(f"[VECTOR-SEARCH]    Response starts with it: {has_correct_greeting}")
            if not has_correct_greeting:
                logger.warning(f"[VECTOR-SEARCH]    ‚ö†Ô∏è AI IGNORED time-based greeting!")
                logger.warning(f"[VECTOR-SEARCH]    Response starts with: '{generated_response[:50]}'")
            
            # Check 2: Customer name
            has_name = customer_name.lower() in generated_response.lower() if customer_name != 'there' else True
            logger.info(f"[VECTOR-SEARCH] ‚úÖ Rule 2 - Customer Name:")
            logger.info(f"[VECTOR-SEARCH]    Name used: {has_name}")
            if not has_name and customer_name != 'there':
                logger.warning(f"[VECTOR-SEARCH]    ‚ö†Ô∏è AI FORGOT customer name!")
            
            # Check 3: Length comparison
            logger.info(f"[VECTOR-SEARCH] ‚úÖ Rule 3 - Length Matching:")
            logger.info(f"[VECTOR-SEARCH]    Target length: {target_length} chars")
            logger.info(f"[VECTOR-SEARCH]    Generated length: {len(generated_response)} chars")
            logger.info(f"[VECTOR-SEARCH]    Difference: {abs(len(generated_response) - target_length)} chars")
            
            # Check 4: Signature
            has_signature = '-Ben' in generated_response or '- Ben' in generated_response or generated_response.strip().endswith('Ben')
            logger.info(f"[VECTOR-SEARCH] ‚úÖ Rule 4 - Signature:")
            logger.info(f"[VECTOR-SEARCH]    Signature present: {has_signature}")
            if not has_signature:
                logger.warning(f"[VECTOR-SEARCH]    ‚ö†Ô∏è AI FORGOT signature!")
            
            # Compliance score
            compliance_score = sum([has_correct_greeting, has_name, has_signature])
            logger.info(f"[VECTOR-SEARCH] üìä COMPLIANCE SCORE: {compliance_score}/3")
            if compliance_score < 3:
                logger.warning(f"[VECTOR-SEARCH] ‚ö†Ô∏è LOW COMPLIANCE - AI not fully following examples!")
            else:
                logger.info(f"[VECTOR-SEARCH] ‚úÖ GOOD COMPLIANCE")
            
            logger.info(f"[VECTOR-SEARCH] ========================================================")
            
            return generated_response
            
        except Exception as e:
            logger.error(f"[VECTOR-SEARCH] Error generating response from pairs: {e}")
            logger.exception("Pair-based response generation error")
            return ""
    
    def calculate_average_response_length(self, similar_chunks: List[Dict]) -> Tuple[int, int, int]:
        """
        üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î –æ–ø—Ç–∏–º–∞–ª—å–Ω—É –¥–æ–≤–∂–∏–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        
        Returns:
            (avg_length, min_length, max_length) - —Å–µ—Ä–µ–¥–Ω—è, –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞
        """
        response_lengths = []
        
        for chunk in similar_chunks:
            # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ response chunks
            if chunk.get('chunk_type') == 'response':
                content = chunk.get('content', '')
                # –®—É–∫–∞—î–º–æ —Å–µ–∫—Ü—ñ—é Reply:
                if 'reply:' in content.lower():
                    reply_start = content.lower().find('reply:')
                    reply_text = content[reply_start:].replace('Reply:', '').replace('reply:', '').strip()
                    response_lengths.append(len(reply_text))
                else:
                    # –Ø–∫—â–æ —Ü–µ —á–∏—Å—Ç–∏–π response chunk (–±–µ–∑ –º–∞—Ä–∫–µ—Ä–∞)
                    response_lengths.append(len(content))
        
        if not response_lengths:
            logger.warning("[VECTOR-SEARCH] No response chunks found, using default length")
            return (200, 150, 300)  # –†–æ–∑—É–º–Ω—ñ –¥–µ—Ñ–æ–ª—Ç–∏
        
        avg_length = sum(response_lengths) // len(response_lengths)
        min_length = min(response_lengths)
        max_length = max(response_lengths)
        
        logger.info(f"[VECTOR-SEARCH] üìè Calculated response lengths from {len(response_lengths)} examples:")
        logger.info(f"[VECTOR-SEARCH]   Average: {avg_length} chars")
        logger.info(f"[VECTOR-SEARCH]   Range: {min_length} - {max_length} chars")
        
        return (avg_length, min_length, max_length)
    
    def generate_contextual_response(
        self,
        lead_inquiry: str,
        customer_name: str,
        similar_chunks: List[Dict],
        business_name: str = "",
        max_response_length: int = None  # ‚úÖ –ó—Ä–æ–±–∏–º–æ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–º
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å—Ö–æ–∂–∏—Ö sample replies
        
        Args:
            lead_inquiry: –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –∑–∞–ø–∏—Ç –∫–ª—ñ—î–Ω—Ç–∞
            customer_name: –Ü–º'—è –∫–ª—ñ—î–Ω—Ç–∞
            similar_chunks: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ—à—É–∫—É
            business_name: –ù–∞–∑–≤–∞ –±—ñ–∑–Ω–µ—Å—É
            max_response_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            
        Returns:
            –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É —Å—Ç–∏–ª—ñ sample replies
        """
        
        if not self.openai_client or not similar_chunks:
            logger.warning("[VECTOR-SEARCH] No OpenAI client or similar chunks for response generation")
            return ""
        
        logger.info(f"[VECTOR-SEARCH] ========== CONTEXTUAL RESPONSE GENERATION ==========")
        logger.info(f"[VECTOR-SEARCH] Lead inquiry: {lead_inquiry[:100]}...")
        logger.info(f"[VECTOR-SEARCH] Customer: {customer_name}")
        logger.info(f"[VECTOR-SEARCH] Business: {business_name}")
        logger.info(f"[VECTOR-SEARCH] Similar chunks: {len(similar_chunks)}")
        
        try:
            # üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
            avg_length, min_length, max_length = self.calculate_average_response_length(similar_chunks)
            
            # –Ø–∫—â–æ max_response_length —è–≤–Ω–æ –≤–∫–∞–∑–∞–Ω–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ —è–∫ –æ–±–º–µ–∂–µ–Ω–Ω—è
            if max_response_length:
                # –û–±–º–µ–∂—É—î–º–æ —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –º–∞–∫—Å–∏–º—É–º–æ–º
                target_length = min(avg_length, max_response_length)
                logger.info(f"[VECTOR-SEARCH] üìè Using capped length: {target_length} (max: {max_response_length})")
            else:
                # –Ü–Ω–∞–∫—à–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Ä–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω—É –¥–æ–≤–∂–∏–Ω—É
                target_length = avg_length
                logger.info(f"[VECTOR-SEARCH] üéØ Using AUTO-DETECTED length: {target_length} chars (from examples)")
            
            # –ó–±–∏—Ä–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∑—ñ —Å—Ö–æ–∂–∏—Ö —á–∞–Ω–∫—ñ–≤
            context_parts = []
            for i, chunk in enumerate(similar_chunks[:5]):  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–ø 5 —á–∞–Ω–∫—ñ–≤
                similarity_score = chunk['similarity_score']
                chunk_type = chunk['chunk_type']
                content = chunk['content']
                
                context_parts.append(
                    f"Example {i+1} (similarity: {similarity_score:.2f}, type: {chunk_type}):\n{content}\n"
                )
            
            context = "\n".join(context_parts)
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            system_prompt = f"""You are a professional business communication assistant for {business_name}.

TASK: Generate a personalized response to a customer inquiry using the most similar sample replies as your style guide.

MOST SIMILAR SAMPLE REPLIES (ranked by semantic similarity):
{context}

INSTRUCTIONS:
1. Analyze the customer inquiry and the similar examples above
2. Generate a NEW response that matches the TONE, STYLE, LENGTH, and APPROACH of the most similar examples
3. Personalize it with the customer's name and specific inquiry details
4. Match the NATURAL LENGTH of responses in the examples (approximately {avg_length} characters, ranging {min_length}-{max_length})
5. Be professional, helpful, and follow the communication patterns learned from the examples
6. Focus especially on the highest similarity examples for style guidance

IMPORTANT: 
- Use the examples as style guides, not templates to copy
- Generate original content that sounds natural and personal
- Match the professionalism, helpfulness, AND TYPICAL LENGTH shown in the examples
- Don't artificially shorten or extend - match the natural conversation length from examples"""
            
            # üïê Get time-based greeting
            from .utils import get_time_based_greeting
            from .models import YelpBusiness
            
            time_greeting = "Good morning"  # default
            try:
                business_obj = YelpBusiness.objects.filter(name=business_name).first()
                if business_obj:
                    time_greeting = get_time_based_greeting(business_obj.business_id)
            except Exception as e:
                logger.warning(f"[VECTOR-SEARCH] Could not get time greeting: {e}")
            
            logger.info(f"[VECTOR-SEARCH] üïê Time-based greeting: '{time_greeting}'")
            
            user_prompt = f"""Customer Information:
- Name: {customer_name}
- Inquiry: "{lead_inquiry}"
- Business: {business_name}
- Time-appropriate greeting: {time_greeting}

IMPORTANT: Start your response with "{time_greeting} {customer_name}" followed by a casual personal touch.

Based on the similar sample replies ranked above (especially the highest similarity ones), generate a personalized response that addresses this specific customer's inquiry while matching the learned communication style."""
            
            logger.info(f"[VECTOR-SEARCH] Generating contextual response with {len(similar_chunks)} similar examples...")
            logger.info(f"[VECTOR-SEARCH] Top similarity scores: {[c['similarity_score'] for c in similar_chunks[:3]]}")
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ max_tokens –Ω–∞ –æ—Å–Ω–æ–≤—ñ target_length
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—É –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—é: 1 token ‚âà 3-4 characters
            estimated_tokens = max(50, target_length // 3)
            logger.info(f"[VECTOR-SEARCH] Estimated tokens for {target_length} chars: {estimated_tokens}")
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=estimated_tokens,  # ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Ä–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω–∏–π –ª—ñ–º—ñ—Ç
                temperature=0.7
            )
            
            generated_response = response.choices[0].message.content.strip()
            
            logger.info(f"[VECTOR-SEARCH] ‚úÖ Generated contextual response:")
            logger.info(f"[VECTOR-SEARCH] - Length: {len(generated_response)} chars")
            logger.info(f"[VECTOR-SEARCH] - Response: {generated_response}")
            logger.info(f"[VECTOR-SEARCH] - Used {len(similar_chunks)} similar examples")
            logger.info(f"[VECTOR-SEARCH] ========================================")
            
            return generated_response
            
        except Exception as e:
            logger.error(f"[VECTOR-SEARCH] Error in vector search: {e}")
            logger.exception("Response generation error details")
            return ""
    
    def get_chunk_statistics(self, business_id: str, location_id: Optional[str] = None) -> Dict:
        """–û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ —á–∞–Ω–∫–∏ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É"""
        
        try:
            from .vector_models import VectorDocument, VectorChunk
            
            # –§—ñ–ª—å—Ç—Ä–∏
            doc_filter = Q(business_id=business_id, processing_status='completed')
            if location_id:
                doc_filter &= Q(location_id=location_id)
            
            documents = VectorDocument.objects.filter(doc_filter)
            
            if not documents.exists():
                return {
                    'total_documents': 0,
                    'total_chunks': 0,
                    'total_tokens': 0,
                    'chunk_types': {},
                    'avg_similarity_ready': False
                }
            
            total_chunks = sum(doc.chunk_count for doc in documents)
            total_tokens = sum(doc.total_tokens for doc in documents)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞—Ö —á–∞–Ω–∫—ñ–≤
            chunk_type_stats = {}
            for doc in documents:
                doc_chunk_types = doc.metadata.get('processing_info', {}).get('chunks_by_type', {})
                for chunk_type, count in doc_chunk_types.items():
                    chunk_type_stats[chunk_type] = chunk_type_stats.get(chunk_type, 0) + count
            
            return {
                'total_documents': len(documents),
                'total_chunks': total_chunks,
                'total_tokens': total_tokens,
                'chunk_types': chunk_type_stats,
                'avg_similarity_ready': total_chunks > 0,
                'documents': [
                    {
                        'filename': doc.filename,
                        'chunks': doc.chunk_count,
                        'tokens': doc.total_tokens,
                        'created': doc.created_at.isoformat()
                    }
                    for doc in documents
                ]
            }
            
        except Exception as e:
            logger.error(f"[VECTOR-SEARCH] Error in vector search: {e}")
            return {
                'total_documents': 0,
                'total_chunks': 0,
                'total_tokens': 0,
                'chunk_types': {},
                'avg_similarity_ready': False,
                'error': str(e)
            }
    
    def test_vector_search(
        self,
        business_id: str, 
        test_query: str = "roof repair",
        location_id: Optional[str] = None
    ) -> Dict:
        """–¢–µ—Å—Ç–æ–≤–∏–π –≤–µ–∫—Ç–æ—Ä–Ω–∏–π –ø–æ—à—É–∫ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        
        logger.info(f"[VECTOR-SEARCH] ========== TEST VECTOR SEARCH ==========")
        logger.info(f"[VECTOR-SEARCH] Test query: {test_query}")
        logger.info(f"[VECTOR-SEARCH] Business: {business_id}")
        
        try:
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –ø–æ—à—É–∫—É
            search_settings = self._get_vector_search_settings(business_id)
            logger.info(f"[VECTOR-SEARCH] Using settings: threshold={search_settings['similarity_threshold']}, limit={search_settings['search_limit']}")
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            stats = self.get_chunk_statistics(business_id, location_id)
            
            if stats['total_chunks'] == 0:
                return {
                    'success': False,
                    'message': 'No vector chunks available for this business',
                    'stats': stats
                }
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫ –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏
            results = self.search_similar_chunks(
                query_text=test_query,
                business_id=business_id,
                location_id=location_id,
                limit=search_settings['search_limit'],
                similarity_threshold=search_settings['similarity_threshold'],
                chunk_types=search_settings['chunk_types']
            )
            
            if not results:
                return {
                    'success': False,
                    'message': 'No similar chunks found (try lowering similarity threshold)',
                    'stats': stats,
                    'query': test_query
                }
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ—Å—Ç–æ–≤–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            test_response = self.generate_contextual_response(
                lead_inquiry=test_query,
                customer_name="Test Customer",
                similar_chunks=results,
                business_name=business_id,
                max_response_length=160
            )
            
            logger.info(f"[VECTOR-SEARCH] ‚úÖ Test completed successfully")
            logger.info(f"[VECTOR-SEARCH] =================================")
            
            return {
                'success': True,
                'message': 'Vector search test completed successfully',
                'stats': stats,
                'query': test_query,
                'results_count': len(results),
                'top_similarity': results[0]['similarity_score'] if results else 0,
                'generated_response': test_response,
                'sample_results': results[:2]  # –¢–æ–ø 2 –¥–ª—è preview
            }
            
        except Exception as e:
            logger.error(f"[VECTOR-SEARCH] Error in vector search: {e}")
            return {
                'success': False,
                'message': f'Test failed: {str(e)}',
                'error': str(e)
            }

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π —ñ–Ω—Å—Ç–∞–Ω—Å
vector_search_service = VectorSearchService()
