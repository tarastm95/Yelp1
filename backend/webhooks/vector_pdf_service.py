"""
üîç Vector-Enhanced PDF Processing Service for Sample Replies
–°–µ–º–∞–Ω—Ç–∏—á–Ω–µ —á–∞–Ω–∫—É–≤–∞–Ω–Ω—è, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ OpenAI —Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
"""

import logging
import re
import hashlib
import asyncio
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass
from django.db import transaction
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
import openai
import os

# PDF processing imports
try:
    import fitz  # PyMuPDF
    import tiktoken
    import numpy as np
    PDF_PROCESSING_AVAILABLE = True
except ImportError:
    PDF_PROCESSING_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç—É"""
    content: str
    page_number: int
    chunk_index: int
    token_count: int
    chunk_type: str
    metadata: Dict

class VectorPDFService:
    """üîç –í–µ–∫—Ç–æ—Ä–Ω–∏–π —Å–µ—Ä–≤—ñ—Å –¥–ª—è –æ–±—Ä–æ–±–∫–∏ PDF –∑ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–º —á–∞–Ω–∫—É–≤–∞–Ω–Ω—è–º —Ç–∞ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏"""
    
    def __init__(self):
        self.openai_client = None
        self.encoding = None
        self._init_openai()
        self._init_tokenizer()
    
    def _init_openai(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞"""
        try:
            from .models import AISettings
            
            # Try environment variable first
            openai_api_key = os.getenv('OPENAI_API_KEY')
            
            if not openai_api_key:
                # Try database settings
                ai_settings = AISettings.objects.first()
                if ai_settings and ai_settings.openai_api_key:
                    openai_api_key = ai_settings.openai_api_key
            
            if openai_api_key:
                self.openai_client = openai.OpenAI(api_key=openai_api_key)
                logger.info("[VECTOR-PDF] OpenAI client initialized successfully")
            else:
                logger.error("[VECTOR-PDF] No OpenAI API key found")
                
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Failed to initialize OpenAI client: {e}")
    
    def _init_tokenizer(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è tiktoken encoder –¥–ª—è text-embedding-3-small"""
        try:
            if PDF_PROCESSING_AVAILABLE:
                self.encoding = tiktoken.get_encoding("cl100k_base")
                logger.info("[VECTOR-PDF] Tiktoken encoder initialized")
            else:
                logger.warning("[VECTOR-PDF] Tiktoken not available, using fallback token counting")
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Failed to initialize tokenizer: {e}")
    
    def extract_text_from_pdf_bytes(self, pdf_bytes: bytes, filename: str) -> Dict:
        """üìÑ Simple PDF extraction using PyMuPDF"""
        
        logger.info(f"[VECTOR-PDF] üìÑ Extracting text from PDF: {filename}")
        
        if not PDF_PROCESSING_AVAILABLE:
            raise ValueError("PDF processing libraries not available.")
        
        try:
            doc = fitz.open("pdf", pdf_bytes)
            pages_data = []
            all_text_parts = []
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                if text.strip():
                    pages_data.append({
                        'page_number': page_num + 1,
                        'text': text,
                        'char_count': len(text)
                    })
                    all_text_parts.append(text)
                    
                    logger.info(f"[VECTOR-PDF] Page {page_num + 1}: {len(text)} chars")
            
            doc.close()
            
            all_text = "\n\n".join(all_text_parts)
            
            # –ê–Ω–∞–ª—ñ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ PDF
            text_lower = all_text.lower()
            structure_indicators = {
                'inquiry_information': 'inquiry information:' in text_lower,
                'response_marker': 'response:' in text_lower,
                'example_marker': 'example #' in text_lower,
                'customer_names': len(re.findall(r'name:\s*[a-z]+ [a-z]\.?', text_lower)),
                'norma_signatures': len(re.findall(r'talk soon,?\s*norma', text_lower, re.IGNORECASE))
            }
            
            logger.info(f"[VECTOR-PDF] üìä PDF Structure Analysis:")
            logger.info(f"[VECTOR-PDF]   Text length: {len(all_text)} chars")
            logger.info(f"[VECTOR-PDF]   Pages extracted: {len(pages_data)}")
            for key, value in structure_indicators.items():
                logger.info(f"[VECTOR-PDF]   {key}: {value}")
            
            return {
                'success': True,
                'text': all_text,
                'pages': pages_data,
                'parser_used': 'pymupdf',
                'structure_preserved': True,
                'sections_detected': [f"{k}: {v}" for k, v in structure_indicators.items() if v],
                'structure_quality_score': sum(1 for v in structure_indicators.values() if v)
            }
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] PDF extraction error: {e}")
            return {
                'success': False,
                'text': '',
                'pages': [],
                'parser_used': None,
                'structure_preserved': False,
                'sections_detected': [],
                'errors': [str(e)]
            }
    

            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Fallback extraction error: {e}")
            return {
                'success': False,
                'text': '',
                'pages': [],
                'parser_used': None,
                'structure_preserved': False,
                'sections_detected': [],
                'errors': [str(e)]
            }
    
    def extract_text_from_uploaded_file(self, file_content: bytes, filename: str) -> str:
        """–ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª—É –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é PDF —Ç–∞ text"""
        
        logger.info(f"[VECTOR-PDF] üìÑ Processing {filename} ({len(file_content)} bytes)")
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Ü–µ PDF —Ñ–∞–π–ª
            if b'%PDF' in file_content[:100] or filename.lower().endswith('.pdf'):
                if not PDF_PROCESSING_AVAILABLE:
                    logger.warning(f"[VECTOR-PDF] ‚ö†Ô∏è PDF detected but PyMuPDF not available: {filename}")
                    return "PDF_BINARY_DETECTED_NO_PARSER"
                
                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É –¥–ª—è PyMuPDF
                temp_path = f"temp_pdfs/{hashlib.md5(file_content).hexdigest()}_{filename}"
                saved_path = default_storage.save(temp_path, ContentFile(file_content))
                full_path = default_storage.path(saved_path)
                
                try:
                    # Using new extract_text_from_pdf_bytes method instead
                    all_text = "\n\n".join(page['text'] for page in pages_data)
                    return all_text
                finally:
                    # –û—á–∏—â–µ–Ω–Ω—è —Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É
                    try:
                        default_storage.delete(saved_path)
                    except:
                        pass
            else:
                # –°–ø—Ä–æ–±–∞ –≤–∏—Ç—è–≥—Ç–∏ —Ç–µ–∫—Å—Ç —è–∫ plain text
                text_content = file_content.decode('utf-8', errors='ignore')
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Ñ–∞–π–ª –Ω–µ –ø–æ—Ä–æ–∂–Ω—ñ–π
                if not text_content.strip():
                    logger.warning(f"[VECTOR-PDF] ‚ö†Ô∏è Empty file: {filename}")
                    return "EMPTY_FILE"
                
                logger.info(f"[VECTOR-PDF] ‚úÖ Successfully extracted {len(text_content)} characters")
                return text_content
                
        except UnicodeDecodeError:
            logger.error(f"[VECTOR-PDF] ‚ùå Unicode decode error: {filename}")
            return "ENCODING_ERROR"
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] ‚ùå Error processing {filename}: {e}")
            return "PROCESSING_ERROR"
    
    def create_semantic_chunks(self, text: str, max_tokens: int = 800) -> List[DocumentChunk]:
        """üéØ –†–æ–∑—É–º–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è chunks –∑ –¥–µ—Ç–µ–∫—Ü—ñ—î—é Sample Replies —Å—Ç—Ä—É–∫—Ç—É—Ä–∏"""
        
        logger.info(f"[VECTOR-PDF] üß† SMART CHUNKING: Analyzing document structure...")
        logger.info(f"[VECTOR-PDF] Text length: {len(text)} chars, max_tokens: {max_tokens}")
        
        # –î–µ—Ç–µ–∫—Ü—ñ—è —á–∏ —Ü–µ Sample Replies –¥–æ–∫—É–º–µ–Ω—Ç
        is_sample_replies = self._detect_sample_replies_document(text)
        
        if is_sample_replies:
            logger.info(f"[VECTOR-PDF] üéØ SAMPLE REPLIES DETECTED - Using specialized chunker")
            return self._create_sample_replies_chunks(text, max_tokens)
        else:
            logger.info(f"[VECTOR-PDF] üìÑ REGULAR DOCUMENT - Using standard chunking")
            return self._create_standard_chunks(text, max_tokens)
    
    def _detect_sample_replies_document(self, text: str) -> bool:
        """–î–µ—Ç–µ–∫—Ç—É—î —á–∏ —Ü–µ –¥–æ–∫—É–º–µ–Ω—Ç Sample Replies"""
        
        text_lower = text.lower()
        
        # –ö–ª—é—á–æ–≤—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ Sample Replies
        indicators = [
            'inquiry information:' in text_lower,
            'response:' in text_lower,
            'example #' in text_lower or 'example#' in text_lower,
            bool(re.search(r'name:\s*[a-z]+ [a-z]\.?', text_lower)),  # Name: John D. - –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ boolean
            'talk soon' in text_lower
        ]
        
        detected_count = sum(indicators)
        is_sample_replies = detected_count >= 2  # –ú—ñ–Ω—ñ–º—É–º 2 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
        
        logger.info(f"[VECTOR-PDF] üîç Sample Replies detection:")
        logger.info(f"[VECTOR-PDF]   Indicators found: {detected_count}/5")
        logger.info(f"[VECTOR-PDF]   Is Sample Replies: {is_sample_replies}")
        
        return is_sample_replies
    
    def _create_sample_replies_chunks(self, text: str, max_tokens: int) -> List[DocumentChunk]:
        """üîÑ Fallback: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π chunking –¥–ª—è Sample Replies"""
        
        logger.warning("[VECTOR-PDF] ‚ö†Ô∏è Specialized chunker not available - using enhanced standard method")
        return self._create_standard_chunks(text, max_tokens)

    def _create_standard_chunks(self, text: str, max_tokens: int) -> List[DocumentChunk]:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è chunks (original method)"""
        
        logger.info(f"[VECTOR-PDF] üìÑ Using standard chunking method")
        
        chunks = []
        chunk_index = 0
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Å–µ–∫—Ü—ñ—ó/–ø—Ä–∏–∫–ª–∞–¥–∏ —Å–ø–æ—á–∞—Ç–∫—É
        sections = self._split_by_sections(text)
        
        for section in sections:
            if not section.strip():
                continue
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —è–∫—â–æ —Å–µ–∫—Ü—ñ—è –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∞
            section_chunks = self._split_long_text(section, max_tokens)
            
            for chunk_text in section_chunks:
                token_count = self._count_tokens(chunk_text)
                
                if token_count > 0:
                    chunk_type = self._identify_chunk_type(chunk_text)
                    
                    logger.info(f"[VECTOR-PDF] üß© CREATING CHUNK #{chunk_index}:")
                    logger.info(f"[VECTOR-PDF]   Text length: {len(chunk_text)} chars")
                    logger.info(f"[VECTOR-PDF]   Token count: {token_count}")
                    logger.info(f"[VECTOR-PDF]   Classified as: {chunk_type}")
                    logger.info(f"[VECTOR-PDF]   Content preview: {chunk_text[:150]}...")
                    
                    # –î–µ—Ç–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ metadata
                    has_inquiry = 'Inquiry information:' in chunk_text or 'Name:' in chunk_text
                    has_response = 'Response:' in chunk_text
                    has_customer_name = bool(re.search(r'Name:\s*\w+', chunk_text))
                    has_service_type = any(word in chunk_text.lower() for word in ['roof', 'repair', 'construction', 'service'])
                    
                    logger.info(f"[VECTOR-PDF]   Metadata analysis:")
                    logger.info(f"[VECTOR-PDF]     - has_inquiry: {has_inquiry}")
                    logger.info(f"[VECTOR-PDF]     - has_response: {has_response}")
                    logger.info(f"[VECTOR-PDF]     - has_customer_name: {has_customer_name}")
                    logger.info(f"[VECTOR-PDF]     - has_service_type: {has_service_type}")
                    
                    chunks.append(DocumentChunk(
                        content=chunk_text.strip(),
                        page_number=1,  # Default for text input
                        chunk_index=chunk_index,
                        token_count=token_count,
                        chunk_type=chunk_type,
                        metadata={
                            'has_inquiry': has_inquiry,
                            'has_response': has_response,
                            'has_customer_name': has_customer_name,
                            'has_service_type': has_service_type,
                            'section_length': len(chunk_text),
                            'specialized_chunking': False
                        }
                    ))
                    chunk_index += 1
        
        logger.info(f"[VECTOR-PDF] Created {len(chunks)} standard chunks")
        return chunks
    
    def _split_by_sections(self, text: str) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç –Ω–∞ –ª–æ–≥—ñ—á–Ω—ñ —Å–µ–∫—Ü—ñ—ó"""
        
        logger.info(f"[VECTOR-PDF] üìÑ SPLITTING TEXT INTO SECTIONS:")
        logger.info(f"[VECTOR-PDF] Original text length: {len(text)} chars")
        logger.info(f"[VECTOR-PDF] Text preview (first 300 chars): {text[:300]}...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω–∏ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Sample Replies
        patterns = [
            r'Example\s*#\d+',
            r'Inquiry information:',
            r'Response:',
            r'\n\n\n+',  # –ú–Ω–æ–∂–∏–Ω–Ω—ñ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        ]
        
        sections = [text]
        
        for i, pattern in enumerate(patterns):
            logger.info(f"[VECTOR-PDF] üîÑ Applying pattern {i+1}: {pattern}")
            new_sections = []
            for section in sections:
                parts = re.split(f'({pattern})', section, flags=re.IGNORECASE)
                logger.info(f"[VECTOR-PDF]   Split into {len(parts)} parts")
                for j, part in enumerate(parts):
                    if part.strip():
                        logger.info(f"[VECTOR-PDF]     Part {j+1}: {len(part)} chars - '{part[:50]}...'")
                        new_sections.append(part.strip())
            sections = new_sections
            logger.info(f"[VECTOR-PDF] After pattern {i+1}: {len(sections)} sections")
        
        filtered_sections = [s for s in sections if len(s.strip()) > 20]
        logger.info(f"[VECTOR-PDF] üìã FINAL SECTIONS: {len(filtered_sections)} (after filtering < 20 chars)")
        
        for i, section in enumerate(filtered_sections):
            logger.info(f"[VECTOR-PDF] Section {i+1}: {len(section)} chars")
            logger.info(f"[VECTOR-PDF]   Content: {section[:100]}...")
        
        return filtered_sections
    
    def _split_long_text(self, text: str, max_tokens: int) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç —â–æ –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π –Ω–∞ –º–µ–Ω—à—ñ —á–∞–Ω–∫–∏"""
        if not self.encoding:
            # Fallback: —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ —Å–∏–º–≤–æ–ª–∞–º–∏
            max_chars = max_tokens * 4
            if len(text) <= max_chars:
                return [text]
            
            chunks = []
            for i in range(0, len(text), max_chars):
                chunks.append(text[i:i + max_chars])
            return chunks
        
        token_count = len(self.encoding.encode(text))
        
        if token_count <= max_tokens:
            return [text]
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ —Ä–µ—á–µ–Ω–Ω—è–º–∏
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            test_chunk = f"{current_chunk} {sentence}".strip()
            if len(self.encoding.encode(test_chunk)) <= max_tokens:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _count_tokens(self, text: str) -> int:
        """–ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤ –≤ —Ç–µ–∫—Å—Ç—ñ"""
        if not self.encoding:
            return len(text) // 4  # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
        return len(self.encoding.encode(text))
    
    def _identify_chunk_type(self, text: str) -> str:
        """üöÄ Enterprise Hybrid Classification: Rules ‚Üí Scoring ‚Üí Zero-shot ‚Üí ML"""
        logger.info(f"[VECTOR-PDF] üéØ CLASSIFYING CHUNK:")
        logger.info(f"[VECTOR-PDF] Text length: {len(text)} chars")
        logger.info(f"[VECTOR-PDF] Text preview: {text[:150]}...")
        
        # –®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω–∏—Ö –º–∞—Ä–∫–µ—Ä—ñ–≤ –ø–µ—Ä–µ–¥ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—î—é
        text_lower = text.lower()
        quick_markers = {
            'inquiry_marker': 'inquiry information:' in text_lower,
            'response_marker': 'response:' in text_lower,
            'name_marker': 'name:' in text_lower,
            'good_afternoon': 'good afternoon' in text_lower,
            'good_morning': 'good morning' in text_lower,
            'thanks_reaching': 'thanks for reaching' in text_lower,
            'talk_soon': 'talk soon' in text_lower
        }
        logger.info(f"[VECTOR-PDF] Quick markers: {quick_markers}")
        
        try:
            # üéØ HYBRID APPROACH: Professional multi-stage classification
            from .hybrid_chunk_classifier import hybrid_classifier
            
            classification_result = hybrid_classifier.classify_chunk_hybrid(text)
            
            logger.info(f"[VECTOR-PDF] üèÜ Hybrid Classification:")
            logger.info(f"[VECTOR-PDF]   - Type: {classification_result.predicted_type}")
            logger.info(f"[VECTOR-PDF]   - Confidence: {classification_result.confidence_score:.2f}")
            logger.info(f"[VECTOR-PDF]   - Method: {classification_result.method_used}")
            logger.info(f"[VECTOR-PDF]   - Rule matches: {classification_result.rule_matches}")
            
            return classification_result.predicted_type
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] ‚ùå Hybrid classification failed: {e}")
            logger.warning("[VECTOR-PDF] üîÑ Using basic fallback classification...")
            
            # Basic fallback (minimal rules)
            text_lower = text.lower().strip()
            
            if 'response:' in text_lower:
                return 'response'
            elif 'inquiry information:' in text_lower:
                return 'inquiry'
            elif ('inquiry information:' in text_lower and 'response:' in text_lower):
                return 'example'
            elif any(phrase in text_lower for phrase in ['good afternoon', 'thanks for reaching', "we'd be glad", 'talk soon']):
                return 'response'
            elif any(phrase in text_lower for phrase in ['name:', 'lead created:', 'ca 9', 'what kind of']):
                return 'inquiry'
            else:
                return 'general'
    
    def generate_embeddings(self, chunks: List[DocumentChunk]) -> List[Tuple[DocumentChunk, List[float]]]:
        """–ì–µ–Ω–µ—Ä—É—î OpenAI –µ–º–±–µ–¥—ñ–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫—ñ–≤"""
        if not self.openai_client:
            raise ValueError("OpenAI client not initialized")
        
        logger.info(f"[VECTOR-PDF] Generating embeddings for {len(chunks)} chunks")
        
        embeddings_data = []
        batch_size = 100  # OpenAI batch limit
        
        try:
            for i in range(0, len(chunks), batch_size):
                batch_chunks = chunks[i:i + batch_size]
                texts = [chunk.content for chunk in batch_chunks]
                
                logger.info(f"[VECTOR-PDF] Processing batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}")
                
                # Generate embeddings using text-embedding-3-small
                response = self.openai_client.embeddings.create(
                    model="text-embedding-3-small",
                    input=texts,
                    dimensions=1536  # Standard dimension for text-embedding-3-small
                )
                
                for chunk, embedding_obj in zip(batch_chunks, response.data):
                    embeddings_data.append((chunk, embedding_obj.embedding))
            
            logger.info(f"[VECTOR-PDF] Generated {len(embeddings_data)} embeddings successfully")
            return embeddings_data
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Error generating embeddings: {e}")
            raise
    
    def calculate_file_hash(self, file_content: bytes) -> str:
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î SHA-256 —Ö–µ—à –≤–º—ñ—Å—Ç—É —Ñ–∞–π–ª—É"""
        return hashlib.sha256(file_content).hexdigest()
    
    def process_pdf_file(
        self, 
        file_content: bytes, 
        filename: str, 
        business_id: str,
        location_id: Optional[str] = None
    ) -> Dict:
        """–ì–æ–ª–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ PDF —Ñ–∞–π–ª—É —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏"""
        
        logger.info(f"[VECTOR-PDF] ======== PROCESSING PDF FOR VECTOR STORAGE ========")
        logger.info(f"[VECTOR-PDF] File: {filename}")
        logger.info(f"[VECTOR-PDF] Business ID: {business_id}")
        logger.info(f"[VECTOR-PDF] Location ID: {location_id}")
        logger.info(f"[VECTOR-PDF] File size: {len(file_content)} bytes")
        
        try:
            # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ —Ñ–∞–π–ª—É
            if b'%PDF' in file_content[:100] or filename.lower().endswith('.pdf'):
                logger.info(f"[VECTOR-PDF] üìÑ PDF detected: {filename}")
                
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π PDF parser
                pdf_result = self.extract_text_from_pdf_bytes(file_content, filename)
                
                if not pdf_result['success']:
                    raise ValueError("Failed to extract text from PDF with all available parsers")
                
                all_text = pdf_result['text']
                page_count = len(pdf_result['pages'])
                
                logger.info(f"[VECTOR-PDF] üìä PDF Extraction Results:")
                logger.info(f"[VECTOR-PDF]   Parser used: {pdf_result['parser_used']}")
                logger.info(f"[VECTOR-PDF]   Pages: {page_count}")
                logger.info(f"[VECTOR-PDF]   Text length: {len(all_text)} chars")
                logger.info(f"[VECTOR-PDF]   Structure preserved: {pdf_result['structure_preserved']}")
                
                # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø–∞—Ä—Å–µ—Ä —É metadata
                file_hash = self.calculate_file_hash(file_content)
            else:
                # Plain text file
                all_text = file_content.decode('utf-8', errors='ignore')
                page_count = 1
                file_hash = self.calculate_file_hash(file_content)
            
            if not all_text.strip():
                raise ValueError("No text content found in file")
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤
            chunks = self.create_semantic_chunks(all_text)
            
            if not chunks:
                raise ValueError("No valid chunks created from document")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤
            embeddings_data = self.generate_embeddings(chunks)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö
            from .vector_models import VectorDocument, VectorChunk
            
            with transaction.atomic():
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
                document = VectorDocument.objects.create(
                    business_id=business_id,
                    location_id=location_id,
                    filename=filename,
                    file_hash=file_hash,
                    file_size=len(file_content),
                    page_count=page_count,
                    chunk_count=len(chunks),
                    total_tokens=sum(chunk.token_count for chunk in chunks),
                    processing_status='completed',
                    embedding_model='text-embedding-3-small',
                    embedding_dimensions=1536,
                    metadata={
                        'processing_info': {
                            'total_chunks': len(chunks),
                            'chunks_by_type': {
                                chunk_type: len([c for c in chunks if c.chunk_type == chunk_type])
                                for chunk_type in set(chunk.chunk_type for chunk in chunks)
                            },
                            'avg_tokens_per_chunk': sum(chunk.token_count for chunk in chunks) / len(chunks)
                        },
                        'pdf_parsing_info': {
                            'parser_used': pdf_result.get('parser_used', 'unknown'),
                            'structure_preserved': pdf_result.get('structure_preserved', False),
                            'sections_detected': pdf_result.get('sections_detected', []),
                            'pages_parsed': page_count,
                            'structure_quality_score': pdf_result.get('structure_quality_score', 0)
                        } if 'pdf_result' in locals() else {}
                    }
                )
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏
                chunk_objects = []
                for chunk, embedding in embeddings_data:
                    chunk_obj = VectorChunk(
                        document=document,
                        content=chunk.content,
                        page_number=chunk.page_number,
                        chunk_index=chunk.chunk_index,
                        token_count=chunk.token_count,
                        embedding=embedding,  # pgvector –æ–±—Ä–æ–±–∏—Ç—å —Ü–µ
                        chunk_type=chunk.chunk_type,
                        metadata=chunk.metadata
                    )
                    chunk_objects.append(chunk_obj)
                
                # Bulk create chunks
                VectorChunk.objects.bulk_create(chunk_objects)
                
                logger.info(f"[VECTOR-PDF] ‚úÖ Successfully processed PDF:")
                logger.info(f"[VECTOR-PDF] - Document ID: {document.id}")
                logger.info(f"[VECTOR-PDF] - Chunks created: {len(chunk_objects)}")
                logger.info(f"[VECTOR-PDF] - Total tokens: {sum(chunk.token_count for chunk in chunks)}")
                logger.info(f"[VECTOR-PDF] - Chunk types: {document.metadata['processing_info']['chunks_by_type']}")
            
            logger.info(f"[VECTOR-PDF] ================================================")
            
            return {
                'document_id': document.id,
                'chunks_count': len(chunk_objects),
                'total_tokens': sum(chunk.token_count for chunk in chunks),
                'pages_count': page_count,
                'chunk_types': document.metadata['processing_info']['chunks_by_type']
            }
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Error processing PDF: {e}")
            logger.exception("PDF processing error details")
            raise
    
    def create_semantic_chunks(self, text: str, max_tokens: int = 800) -> List[DocumentChunk]:
        """üéØ –†–æ–∑—É–º–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è chunks –∑ –¥–µ—Ç–µ–∫—Ü—ñ—î—é Sample Replies —Å—Ç—Ä—É–∫—Ç—É—Ä–∏"""
        
        logger.info(f"[VECTOR-PDF] üß† SMART CHUNKING: Analyzing document structure...")
        logger.info(f"[VECTOR-PDF] Text length: {len(text)} chars, max_tokens: {max_tokens}")
        
        # –î–µ—Ç–µ–∫—Ü—ñ—è —á–∏ —Ü–µ Sample Replies –¥–æ–∫—É–º–µ–Ω—Ç
        is_sample_replies = self._detect_sample_replies_document(text)
        
        if is_sample_replies:
            logger.info(f"[VECTOR-PDF] üéØ SAMPLE REPLIES DETECTED - Using specialized chunker")
            return self._create_sample_replies_chunks(text, max_tokens)
        else:
            logger.info(f"[VECTOR-PDF] üìÑ REGULAR DOCUMENT - Using standard chunking")
            return self._create_standard_chunks(text, max_tokens)
    
    def _detect_sample_replies_document(self, text: str) -> bool:
        """–î–µ—Ç–µ–∫—Ç—É—î —á–∏ —Ü–µ –¥–æ–∫—É–º–µ–Ω—Ç Sample Replies"""
        
        text_lower = text.lower()
        
        # –ö–ª—é—á–æ–≤—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ Sample Replies
        indicators = [
            'inquiry information:' in text_lower,
            'response:' in text_lower,
            'example #' in text_lower or 'example#' in text_lower,
            bool(re.search(r'name:\s*[a-z]+ [a-z]\.?', text_lower)),  # Name: John D. - –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ boolean
            'talk soon' in text_lower
        ]
        
        detected_count = sum(indicators)
        is_sample_replies = detected_count >= 2  # –ú—ñ–Ω—ñ–º—É–º 2 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
        
        logger.info(f"[VECTOR-PDF] üîç Sample Replies detection:")
        logger.info(f"[VECTOR-PDF]   Indicators found: {detected_count}/5")
        logger.info(f"[VECTOR-PDF]   Is Sample Replies: {is_sample_replies}")
        
        return is_sample_replies
    
    def _create_sample_replies_chunks(self, text: str, max_tokens: int) -> List[DocumentChunk]:
        """üîÑ Fallback: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π chunking –¥–ª—è Sample Replies"""
        
        logger.warning("[VECTOR-PDF] ‚ö†Ô∏è Specialized chunker not available - using enhanced standard method")
        return self._create_standard_chunks(text, max_tokens)

    def _create_standard_chunks(self, text: str, max_tokens: int) -> List[DocumentChunk]:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è chunks (original method)"""
        
        logger.info(f"[VECTOR-PDF] üìÑ Using standard chunking method")
        
        chunks = []
        chunk_index = 0
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Å–µ–∫—Ü—ñ—ó/–ø—Ä–∏–∫–ª–∞–¥–∏ —Å–ø–æ—á–∞—Ç–∫—É
        sections = self._split_by_sections(text)
        
        for section in sections:
            if not section.strip():
                continue
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —è–∫—â–æ —Å–µ–∫—Ü—ñ—è –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∞
            section_chunks = self._split_long_text(section, max_tokens)
            
            for chunk_text in section_chunks:
                token_count = self._count_tokens(chunk_text)
                
                if token_count > 0:
                    chunk_type = self._identify_chunk_type(chunk_text)
                    
                    logger.info(f"[VECTOR-PDF] üß© CREATING CHUNK #{chunk_index}:")
                    logger.info(f"[VECTOR-PDF]   Text length: {len(chunk_text)} chars")
                    logger.info(f"[VECTOR-PDF]   Token count: {token_count}")
                    logger.info(f"[VECTOR-PDF]   Classified as: {chunk_type}")
                    logger.info(f"[VECTOR-PDF]   Content preview: {chunk_text[:150]}...")
                    
                    # –î–µ—Ç–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ metadata
                    has_inquiry = 'Inquiry information:' in chunk_text or 'Name:' in chunk_text
                    has_response = 'Response:' in chunk_text
                    has_customer_name = bool(re.search(r'Name:\s*\w+', chunk_text))
                    has_service_type = any(word in chunk_text.lower() for word in ['roof', 'repair', 'construction', 'service'])
                    
                    logger.info(f"[VECTOR-PDF]   Metadata analysis:")
                    logger.info(f"[VECTOR-PDF]     - has_inquiry: {has_inquiry}")
                    logger.info(f"[VECTOR-PDF]     - has_response: {has_response}")
                    logger.info(f"[VECTOR-PDF]     - has_customer_name: {has_customer_name}")
                    logger.info(f"[VECTOR-PDF]     - has_service_type: {has_service_type}")
                    
                    chunks.append(DocumentChunk(
                        content=chunk_text.strip(),
                        page_number=1,  # Default for text input
                        chunk_index=chunk_index,
                        token_count=token_count,
                        chunk_type=chunk_type,
                        metadata={
                            'has_inquiry': has_inquiry,
                            'has_response': has_response,
                            'has_customer_name': has_customer_name,
                            'has_service_type': has_service_type,
                            'section_length': len(chunk_text),
                            'specialized_chunking': False
                        }
                    ))
                    chunk_index += 1
        
        logger.info(f"[VECTOR-PDF] Created {len(chunks)} standard chunks")
        return chunks
    
    def _split_by_sections(self, text: str) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç –Ω–∞ –ª–æ–≥—ñ—á–Ω—ñ —Å–µ–∫—Ü—ñ—ó"""
        
        logger.info(f"[VECTOR-PDF] üìÑ SPLITTING TEXT INTO SECTIONS:")
        logger.info(f"[VECTOR-PDF] Original text length: {len(text)} chars")
        logger.info(f"[VECTOR-PDF] Text preview (first 300 chars): {text[:300]}...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω–∏ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Sample Replies
        patterns = [
            r'Example\s*#\d+',
            r'Inquiry information:',
            r'Response:',
            r'\n\n\n+',  # –ú–Ω–æ–∂–∏–Ω–Ω—ñ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        ]
        
        sections = [text]
        
        for i, pattern in enumerate(patterns):
            logger.info(f"[VECTOR-PDF] üîÑ Applying pattern {i+1}: {pattern}")
            new_sections = []
            for section in sections:
                parts = re.split(f'({pattern})', section, flags=re.IGNORECASE)
                logger.info(f"[VECTOR-PDF]   Split into {len(parts)} parts")
                for j, part in enumerate(parts):
                    if part.strip():
                        logger.info(f"[VECTOR-PDF]     Part {j+1}: {len(part)} chars - '{part[:50]}...'")
                        new_sections.append(part.strip())
            sections = new_sections
            logger.info(f"[VECTOR-PDF] After pattern {i+1}: {len(sections)} sections")
        
        filtered_sections = [s for s in sections if len(s.strip()) > 20]
        logger.info(f"[VECTOR-PDF] üìã FINAL SECTIONS: {len(filtered_sections)} (after filtering < 20 chars)")
        
        for i, section in enumerate(filtered_sections):
            logger.info(f"[VECTOR-PDF] Section {i+1}: {len(section)} chars")
            logger.info(f"[VECTOR-PDF]   Content: {section[:100]}...")
        
        return filtered_sections
    
    def _split_long_text(self, text: str, max_tokens: int) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç —â–æ –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π –Ω–∞ –º–µ–Ω—à—ñ —á–∞–Ω–∫–∏"""
        if not self.encoding:
            # Fallback: —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ —Å–∏–º–≤–æ–ª–∞–º–∏
            max_chars = max_tokens * 4
            if len(text) <= max_chars:
                return [text]
            
            chunks = []
            for i in range(0, len(text), max_chars):
                chunks.append(text[i:i + max_chars])
            return chunks
        
        token_count = len(self.encoding.encode(text))
        
        if token_count <= max_tokens:
            return [text]
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ —Ä–µ—á–µ–Ω–Ω—è–º–∏
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            test_chunk = f"{current_chunk} {sentence}".strip()
            if len(self.encoding.encode(test_chunk)) <= max_tokens:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _count_tokens(self, text: str) -> int:
        """–ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤ –≤ —Ç–µ–∫—Å—Ç—ñ"""
        if not self.encoding:
            return len(text) // 4  # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
        return len(self.encoding.encode(text))
    
    def _identify_chunk_type(self, text: str) -> str:
        """–í–∏–∑–Ω–∞—á–∞—î —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å–µ–∫—Ü—ñ—ó"""
        text_lower = text.lower()
        
        if 'inquiry information:' in text_lower:
            return 'inquiry'
        elif 'response:' in text_lower:
            return 'response'  
        elif ('inquiry information:' in text_lower and 'response:' in text_lower):
            return 'example'
        else:
            return 'general'
    
    def format_sample_replies(self, raw_content: str) -> str:
        """–§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è sample replies –¥–ª—è AI –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è"""
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∫–æ–¥–∏ –ø–æ–º–∏–ª–æ–∫
        if raw_content in ["PDF_BINARY_DETECTED", "PROCESSING_ERROR", "EMPTY_FILE", "ENCODING_ERROR", "PDF_BINARY_DETECTED_NO_PARSER"]:
            return raw_content
        
        if not raw_content or not raw_content.strip():
            return "EMPTY_CONTENT"
        
        # –ë–∞–∑–æ–≤–µ –æ—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
        cleaned = raw_content.strip()
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)  # –ó–∞–º—ñ–Ω—é—î–º–æ 3+ –ø–µ—Ä–µ–Ω–æ—Å–∏ –Ω–∞ 2
        cleaned = re.sub(r' +', ' ', cleaned)  # –ó–∞–º—ñ–Ω—é—î–º–æ –±–∞–≥–∞—Ç–æ –ø—Ä–æ–±—ñ–ª—ñ–≤ –Ω–∞ –æ–¥–∏–Ω
        cleaned = re.sub(r'\t+', ' ', cleaned)  # –ó–∞–º—ñ–Ω—é—î–º–æ —Ç–∞–±—É–ª—è—Ü—ñ—ó –Ω–∞ –ø—Ä–æ–±—ñ–ª
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ä—è–¥–∫—ñ–≤
        lines = cleaned.split('\n')
        lines = [line.strip() for line in lines]
        cleaned = '\n'.join(lines)
        
        # –í–∏–¥–∞–ª—è—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ —Ä—è–¥–∫–∏ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ç–∞ –≤ –∫—ñ–Ω—Ü—ñ
        cleaned = cleaned.strip()
        
        logger.info(f"[VECTOR-PDF] üîß Formatted sample replies:")
        logger.info(f"[VECTOR-PDF] - Original length: {len(raw_content)} chars")
        logger.info(f"[VECTOR-PDF] - Cleaned length: {len(cleaned)} chars")
        logger.info(f"[VECTOR-PDF] - Preview: {cleaned[:200]}...")
        
        return cleaned
    
    def validate_sample_replies_content(self, content: str) -> tuple[bool, Optional[str]]:
        """–í–∞–ª—ñ–¥—É—î —á–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è Sample Replies"""
        
        if not content or not content.strip():
            return False, "–ö–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π"
        
        if len(content.strip()) < 50:
            return False, "–ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π (–º—ñ–Ω—ñ–º—É–º 50 —Å–∏–º–≤–æ–ª—ñ–≤)"
        
        if len(content) > 100000:  # –ó–±—ñ–ª—å—à–∏–ª–∏ –ª—ñ–º—ñ—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è
            return False, "–ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π (–º–∞–∫—Å–∏–º—É–º 100,000 —Å–∏–º–≤–æ–ª—ñ–≤)"
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —î —Ö–æ—á–∞ –± –¥–µ—è–∫—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è Sample Replies
        content_lower = content.lower()
        keywords = [
            'inquiry', 'response', 'customer', 'hello', 'thank you', 
            'service', 'business', 'contact', 'help', 'question',
            '–∑–∞–ø–∏—Ç', '–≤—ñ–¥–ø–æ–≤—ñ–¥—å', '–∫–ª—ñ—î–Ω—Ç', '–¥—è–∫—É—é', '–ø–æ—Å–ª—É–≥–∞'
        ]
        
        has_keywords = any(keyword in content_lower for keyword in keywords)
        
        if not has_keywords:
            return False, "–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ —Å—Ö–æ–∂–∏–π –Ω–∞ Sample Replies (–Ω–µ–º–∞—î –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤)"
        
        logger.info(f"[VECTOR-PDF] ‚úÖ Content validation passed")
        return True, None

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π —ñ–Ω—Å—Ç–∞–Ω—Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–µ—Ä–≤—ñ—Å—É
vector_pdf_service = VectorPDFService()

# Legacy —ñ–Ω—Å—Ç–∞–Ω—Å –¥–ª—è backward compatibility
simple_pdf_service = vector_pdf_service
