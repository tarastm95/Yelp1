"""
üîç Vector-Enhanced PDF Processing Service for Sample Replies
–°–µ–º–∞–Ω—Ç–∏—á–Ω–µ —á–∞–Ω–∫—É–≤–∞–Ω–Ω—è, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ OpenAI —Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
"""

import logging
import re
import hashlib
import asyncio
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass
from django.db import transaction
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
import openai
import os

# PDF processing imports
try:
    import fitz  # PyMuPDF
    import tiktoken
    import numpy as np
    PDF_PROCESSING_AVAILABLE = True
except ImportError:
    PDF_PROCESSING_AVAILABLE = False

logger = logging.getLogger(__name__)



@dataclass
class DocumentChunk:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç—É"""
    content: str
    page_number: int
    chunk_index: int
    token_count: int
    chunk_type: str
    metadata: Dict

class VectorPDFService:
    """üîç –í–µ–∫—Ç–æ—Ä–Ω–∏–π —Å–µ—Ä–≤—ñ—Å –¥–ª—è –æ–±—Ä–æ–±–∫–∏ PDF –∑ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–º —á–∞–Ω–∫—É–≤–∞–Ω–Ω—è–º —Ç–∞ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏"""
    
    def __init__(self):
        self.openai_client = None
        self.encoding = None
        self._init_openai()
        self._init_tokenizer()
    
    def _init_openai(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞"""
        try:
            from .models import AISettings
            
            # Try environment variable first
            openai_api_key = os.getenv('OPENAI_API_KEY')
            
            if not openai_api_key:
                # Try database settings
                ai_settings = AISettings.objects.first()
                if ai_settings and ai_settings.openai_api_key:
                    openai_api_key = ai_settings.openai_api_key
            
            if openai_api_key:
                self.openai_client = openai.OpenAI(api_key=openai_api_key)
                logger.info("[VECTOR-PDF] OpenAI client initialized successfully")
            else:
                logger.error("[VECTOR-PDF] No OpenAI API key found")
                
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Failed to initialize OpenAI client: {e}")
    
    def _init_tokenizer(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è tiktoken encoder –¥–ª—è text-embedding-3-small"""
        try:
            if PDF_PROCESSING_AVAILABLE:
                self.encoding = tiktoken.get_encoding("cl100k_base")
                logger.info("[VECTOR-PDF] Tiktoken encoder initialized")
            else:
                logger.warning("[VECTOR-PDF] Tiktoken not available, using fallback token counting")
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Failed to initialize tokenizer: {e}")
    
    def extract_text_from_pdf_bytes(self, pdf_bytes: bytes, filename: str) -> Dict:
        """üìÑ Simple PDF extraction using PyMuPDF"""
        
        logger.info(f"[VECTOR-PDF] üìÑ Extracting text from PDF: {filename}")
        
        if not PDF_PROCESSING_AVAILABLE:
            raise ValueError("PDF processing libraries not available.")
        
        try:
            doc = fitz.open("pdf", pdf_bytes)
            pages_data = []
            all_text_parts = []
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                if text.strip():
                    pages_data.append({
                        'page_number': page_num + 1,
                        'text': text,
                        'char_count': len(text)
                    })
                    all_text_parts.append(text)
                    
                    logger.info(f"[VECTOR-PDF] Page {page_num + 1}: {len(text)} chars")
            
            doc.close()
            
            all_text = "\n\n".join(all_text_parts)
            
            # –ê–Ω–∞–ª—ñ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ PDF
            text_lower = all_text.lower()
            structure_indicators = {
                'inquiry_information': 'inquiry information:' in text_lower,
                'response_marker': 'response:' in text_lower,
                'example_marker': 'example #' in text_lower,
                'customer_names': len(re.findall(r'name:\s*[a-z]+ [a-z]\.?', text_lower)),
                'norma_signatures': len(re.findall(r'talk soon,?\s*norma', text_lower, re.IGNORECASE))
            }
            
            logger.info(f"[VECTOR-PDF] üìä PDF Structure Analysis:")
            logger.info(f"[VECTOR-PDF]   Text length: {len(all_text)} chars")
            logger.info(f"[VECTOR-PDF]   Pages extracted: {len(pages_data)}")
            for key, value in structure_indicators.items():
                logger.info(f"[VECTOR-PDF]   {key}: {value}")
            
            return {
                'success': True,
                'text': all_text,
                'pages': pages_data,
                'parser_used': 'pymupdf',
                'structure_preserved': True,
                'sections_detected': [f"{k}: {v}" for k, v in structure_indicators.items() if v],
                'structure_quality_score': sum(1 for v in structure_indicators.values() if v)
            }
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] PDF extraction error: {e}")
            return {
                'success': False,
                'text': '',
                'pages': [],
                'parser_used': None,
                'structure_preserved': False,
                'sections_detected': [],
                'errors': [str(e)]
            }
    

            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Fallback extraction error: {e}")
            return {
                'success': False,
                'text': '',
                'pages': [],
                'parser_used': None,
                'structure_preserved': False,
                'sections_detected': [],
                'errors': [str(e)]
            }
    

    
    def create_semantic_chunks(self, text: str, max_tokens: int = 800) -> List[DocumentChunk]:
        """üéØ –†–æ–∑—É–º–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è chunks –∑ –¥–µ—Ç–µ–∫—Ü—ñ—î—é Sample Replies —Å—Ç—Ä—É–∫—Ç—É—Ä–∏"""
        
        logger.info(f"[VECTOR-PDF] üß† SMART CHUNKING: Analyzing document structure...")
        logger.info(f"[VECTOR-PDF] Text length: {len(text)} chars, max_tokens: {max_tokens}")
        
        # –î–µ—Ç–µ–∫—Ü—ñ—è —á–∏ —Ü–µ Sample Replies –¥–æ–∫—É–º–µ–Ω—Ç
        is_sample_replies = self._detect_sample_replies_document(text)
        
        if is_sample_replies:
            logger.info(f"[VECTOR-PDF] üéØ SAMPLE REPLIES DETECTED - Using specialized chunker")
            return self._create_sample_replies_chunks(text, max_tokens)
        else:
            logger.info(f"[VECTOR-PDF] üìÑ REGULAR DOCUMENT - Using standard chunking")
            return self._create_standard_chunks(text, max_tokens)
    
    def _detect_sample_replies_document(self, text: str) -> bool:
        """–î–µ—Ç–µ–∫—Ç—É—î —á–∏ —Ü–µ –¥–æ–∫—É–º–µ–Ω—Ç Sample Replies"""
        
        text_lower = text.lower()
        
        # –ö–ª—é—á–æ–≤—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ Sample Replies
        indicators = [
            'inquiry information:' in text_lower,
            'response:' in text_lower,
            'example #' in text_lower or 'example#' in text_lower,
            bool(re.search(r'name:\s*[a-z]+ [a-z]\.?', text_lower)),  # Name: John D. - –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ boolean
            'talk soon' in text_lower
        ]
        
        detected_count = sum(indicators)
        is_sample_replies = detected_count >= 2  # –ú—ñ–Ω—ñ–º—É–º 2 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
        
        logger.info(f"[VECTOR-PDF] üîç Sample Replies detection:")
        logger.info(f"[VECTOR-PDF]   Indicators found: {detected_count}/5")
        logger.info(f"[VECTOR-PDF]   Is Sample Replies: {is_sample_replies}")
        
        return is_sample_replies
    
    def _create_sample_replies_chunks(self, text: str, max_tokens: int) -> List[DocumentChunk]:
        """üîÑ Simplified: Uses standard chunking with enhanced classification"""
        
        logger.info("[VECTOR-PDF] üîÑ Using simplified Sample Replies processing")
        return self._create_standard_chunks(text, max_tokens)

    def _create_standard_chunks(self, text: str, max_tokens: int) -> List[DocumentChunk]:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è chunks (original method)"""
        
        logger.info(f"[VECTOR-PDF] üìÑ Using standard chunking method")
        
        chunks = []
        chunk_index = 0
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Å–µ–∫—Ü—ñ—ó/–ø—Ä–∏–∫–ª–∞–¥–∏ —Å–ø–æ—á–∞—Ç–∫—É
        sections = self._split_by_sections(text)
        
        for section in sections:
            if not section.strip():
                continue
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —è–∫—â–æ —Å–µ–∫—Ü—ñ—è –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∞
            section_chunks = self._split_long_text(section, max_tokens)
            
            for chunk_text in section_chunks:
                token_count = self._count_tokens(chunk_text)
                
                if token_count > 0:
                    chunk_type = self._identify_chunk_type(chunk_text)
                    
                    logger.info(f"[VECTOR-PDF] üß© CREATING CHUNK #{chunk_index}:")
                    logger.info(f"[VECTOR-PDF]   Text length: {len(chunk_text)} chars")
                    logger.info(f"[VECTOR-PDF]   Token count: {token_count}")
                    logger.info(f"[VECTOR-PDF]   Classified as: {chunk_type}")
                    logger.info(f"[VECTOR-PDF]   Content preview: {chunk_text[:150]}...")
                    
                    # –î–µ—Ç–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ metadata
                    has_inquiry = 'Inquiry information:' in chunk_text or 'Name:' in chunk_text
                    has_response = 'Response:' in chunk_text
                    has_customer_name = bool(re.search(r'Name:\s*\w+', chunk_text))
                    has_service_type = any(word in chunk_text.lower() for word in ['roof', 'repair', 'construction', 'service'])
                    
                    logger.info(f"[VECTOR-PDF]   Metadata analysis:")
                    logger.info(f"[VECTOR-PDF]     - has_inquiry: {has_inquiry}")
                    logger.info(f"[VECTOR-PDF]     - has_response: {has_response}")
                    logger.info(f"[VECTOR-PDF]     - has_customer_name: {has_customer_name}")
                    logger.info(f"[VECTOR-PDF]     - has_service_type: {has_service_type}")
                    
                    chunks.append(DocumentChunk(
                        content=chunk_text.strip(),
                        page_number=1,  # Default for text input
                        chunk_index=chunk_index,
                        token_count=token_count,
                        chunk_type=chunk_type,
                        metadata={
                            'has_inquiry': has_inquiry,
                            'has_response': has_response,
                            'has_customer_name': has_customer_name,
                            'has_service_type': has_service_type,
                            'section_length': len(chunk_text),
                            'specialized_chunking': False
                        }
                    ))
                    chunk_index += 1
        
        logger.info(f"[VECTOR-PDF] Created {len(chunks)} standard chunks")
        return chunks
    
    def _split_by_sections(self, text: str) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç –Ω–∞ –ª–æ–≥—ñ—á–Ω—ñ —Å–µ–∫—Ü—ñ—ó"""
        
        logger.info(f"[VECTOR-PDF] üìÑ SPLITTING TEXT INTO SECTIONS:")
        logger.info(f"[VECTOR-PDF] Original text length: {len(text)} chars")
        logger.info(f"[VECTOR-PDF] Text preview (first 300 chars): {text[:300]}...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω–∏ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Sample Replies
        patterns = [
            r'Example\s*#\d+',
            r'Inquiry information:',
            r'Response:',
            r'\n\n