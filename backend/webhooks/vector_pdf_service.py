"""
üîç Vector-Enhanced PDF Processing Service for Sample Replies
–°–µ–º–∞–Ω—Ç–∏—á–Ω–µ —á–∞–Ω–∫—É–≤–∞–Ω–Ω—è, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ OpenAI —Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
"""

import logging
import re
import hashlib
import asyncio
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass
from django.db import transaction
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
import openai
import os

# PDF processing imports
try:
    import fitz  # PyMuPDF
    import tiktoken
    import numpy as np
    PDF_PROCESSING_AVAILABLE = True
except ImportError:
    PDF_PROCESSING_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç—É"""
    content: str
    page_number: int
    chunk_index: int
    token_count: int
    chunk_type: str
    metadata: Dict

class VectorPDFService:
    """üîç –í–µ–∫—Ç–æ—Ä–Ω–∏–π —Å–µ—Ä–≤—ñ—Å –¥–ª—è –æ–±—Ä–æ–±–∫–∏ PDF –∑ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–º —á–∞–Ω–∫—É–≤–∞–Ω–Ω—è–º —Ç–∞ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏"""
    
    def __init__(self):
        self.openai_client = None
        self.encoding = None
        self._init_openai()
        self._init_tokenizer()
    
    def _init_openai(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞"""
        try:
            from .models import AISettings
            
            # Try environment variable first
            openai_api_key = os.getenv('OPENAI_API_KEY')
            
            if not openai_api_key:
                # Try database settings
                ai_settings = AISettings.objects.first()
                if ai_settings and ai_settings.openai_api_key:
                    openai_api_key = ai_settings.openai_api_key
            
            if openai_api_key:
                self.openai_client = openai.OpenAI(api_key=openai_api_key)
                logger.info("[VECTOR-PDF] OpenAI client initialized successfully")
            else:
                logger.error("[VECTOR-PDF] No OpenAI API key found")
                
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Failed to initialize OpenAI client: {e}")
    
    def _init_tokenizer(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è tiktoken encoder –¥–ª—è text-embedding-3-small"""
        try:
            if PDF_PROCESSING_AVAILABLE:
                self.encoding = tiktoken.get_encoding("cl100k_base")
                logger.info("[VECTOR-PDF] Tiktoken encoder initialized")
            else:
                logger.warning("[VECTOR-PDF] Tiktoken not available, using fallback token counting")
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Failed to initialize tokenizer: {e}")
    
    def extract_text_from_pdf(self, file_path: str) -> List[Dict]:
        """–í–∏—Ç—è–≥—É—î —Ç–µ–∫—Å—Ç –∑ PDF –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""
        if not PDF_PROCESSING_AVAILABLE:
            raise ValueError("PDF processing libraries not available. Please install PyMuPDF, tiktoken, numpy.")
        
        try:
            logger.info(f"[VECTOR-PDF] Extracting text from PDF: {file_path}")
            
            doc = fitz.open(file_path)
            pages_data = []
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                if text.strip():  # Only add non-empty pages
                    pages_data.append({
                        'page_number': page_num + 1,
                        'text': text,
                        'char_count': len(text)
                    })
            
            doc.close()
            logger.info(f"[VECTOR-PDF] Extracted text from {len(pages_data)} pages")
            return pages_data
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Error extracting text from PDF: {e}")
            raise
    
    def extract_text_from_uploaded_file(self, file_content: bytes, filename: str) -> str:
        """–ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª—É –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é PDF —Ç–∞ text"""
        
        logger.info(f"[VECTOR-PDF] üìÑ Processing {filename} ({len(file_content)} bytes)")
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Ü–µ PDF —Ñ–∞–π–ª
            if b'%PDF' in file_content[:100] or filename.lower().endswith('.pdf'):
                if not PDF_PROCESSING_AVAILABLE:
                    logger.warning(f"[VECTOR-PDF] ‚ö†Ô∏è PDF detected but PyMuPDF not available: {filename}")
                    return "PDF_BINARY_DETECTED_NO_PARSER"
                
                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É –¥–ª—è PyMuPDF
                temp_path = f"temp_pdfs/{hashlib.md5(file_content).hexdigest()}_{filename}"
                saved_path = default_storage.save(temp_path, ContentFile(file_content))
                full_path = default_storage.path(saved_path)
                
                try:
                    pages_data = self.extract_text_from_pdf(full_path)
                    all_text = "\n\n".join(page['text'] for page in pages_data)
                    return all_text
                finally:
                    # –û—á–∏—â–µ–Ω–Ω—è —Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É
                    try:
                        default_storage.delete(saved_path)
                    except:
                        pass
            else:
                # –°–ø—Ä–æ–±–∞ –≤–∏—Ç—è–≥—Ç–∏ —Ç–µ–∫—Å—Ç —è–∫ plain text
                text_content = file_content.decode('utf-8', errors='ignore')
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Ñ–∞–π–ª –Ω–µ –ø–æ—Ä–æ–∂–Ω—ñ–π
                if not text_content.strip():
                    logger.warning(f"[VECTOR-PDF] ‚ö†Ô∏è Empty file: {filename}")
                    return "EMPTY_FILE"
                
                logger.info(f"[VECTOR-PDF] ‚úÖ Successfully extracted {len(text_content)} characters")
                return text_content
                
        except UnicodeDecodeError:
            logger.error(f"[VECTOR-PDF] ‚ùå Unicode decode error: {filename}")
            return "ENCODING_ERROR"
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] ‚ùå Error processing {filename}: {e}")
            return "PROCESSING_ERROR"
    
    def create_semantic_chunks(self, text: str, max_tokens: int = 800) -> List[DocumentChunk]:
        """–°—Ç–≤–æ—Ä—é—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω—ñ —á–∞–Ω–∫–∏ –∑ —Ç–µ–∫—Å—Ç—É"""
        logger.info(f"[VECTOR-PDF] Creating semantic chunks with max_tokens: {max_tokens}")
        
        chunks = []
        chunk_index = 0
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Å–µ–∫—Ü—ñ—ó/–ø—Ä–∏–∫–ª–∞–¥–∏ —Å–ø–æ—á–∞—Ç–∫—É
        sections = self._split_by_sections(text)
        
        for section in sections:
            if not section.strip():
                continue
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —è–∫—â–æ —Å–µ–∫—Ü—ñ—è –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∞
            section_chunks = self._split_long_text(section, max_tokens)
            
            for chunk_text in section_chunks:
                token_count = self._count_tokens(chunk_text)
                
                if token_count > 0:
                    chunks.append(DocumentChunk(
                        content=chunk_text.strip(),
                        page_number=1,  # Default for text input
                        chunk_index=chunk_index,
                        token_count=token_count,
                        chunk_type=self._identify_chunk_type(chunk_text),
                        metadata={
                            'has_inquiry': 'Inquiry information:' in chunk_text or 'Name:' in chunk_text,
                            'has_response': 'Response:' in chunk_text,
                            'has_customer_name': bool(re.search(r'Name:\s*\w+', chunk_text)),
                            'has_service_type': any(word in chunk_text.lower() for word in ['roof', 'repair', 'construction', 'service']),
                            'section_length': len(chunk_text)
                        }
                    ))
                    chunk_index += 1
        
        logger.info(f"[VECTOR-PDF] Created {len(chunks)} semantic chunks")
        return chunks
    
    def _split_by_sections(self, text: str) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç –Ω–∞ –ª–æ–≥—ñ—á–Ω—ñ —Å–µ–∫—Ü—ñ—ó"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω–∏ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Sample Replies
        patterns = [
            r'Example\s*#\d+',
            r'Inquiry information:',
            r'Response:',
            r'\n\n\n+',  # –ú–Ω–æ–∂–∏–Ω–Ω—ñ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        ]
        
        sections = [text]
        
        for pattern in patterns:
            new_sections = []
            for section in sections:
                parts = re.split(f'({pattern})', section, flags=re.IGNORECASE)
                for part in parts:
                    if part.strip():
                        new_sections.append(part.strip())
            sections = new_sections
        
        return [s for s in sections if len(s.strip()) > 20]  # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Å–µ–∫—Ü—ñ—ó
    
    def _split_long_text(self, text: str, max_tokens: int) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç —â–æ –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π –Ω–∞ –º–µ–Ω—à—ñ —á–∞–Ω–∫–∏"""
        if not self.encoding:
            # Fallback: —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ —Å–∏–º–≤–æ–ª–∞–º–∏
            max_chars = max_tokens * 4
            if len(text) <= max_chars:
                return [text]
            
            chunks = []
            for i in range(0, len(text), max_chars):
                chunks.append(text[i:i + max_chars])
            return chunks
        
        token_count = len(self.encoding.encode(text))
        
        if token_count <= max_tokens:
            return [text]
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ —Ä–µ—á–µ–Ω–Ω—è–º–∏
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            test_chunk = f"{current_chunk} {sentence}".strip()
            if len(self.encoding.encode(test_chunk)) <= max_tokens:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _count_tokens(self, text: str) -> int:
        """–ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤ –≤ —Ç–µ–∫—Å—Ç—ñ"""
        if not self.encoding:
            return len(text) // 4  # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
        return len(self.encoding.encode(text))
    
    def _identify_chunk_type(self, text: str) -> str:
        """ü§ñ ML-powered classification –∑ fallback –Ω–∞ pattern matching"""
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ ML classifier
            from .chunk_classifier_ml import ml_chunk_classifier
            
            classification_result = ml_chunk_classifier.classify_chunk(text)
            
            logger.info(f"[VECTOR-PDF] üß† ML Classification result: {classification_result}")
            return classification_result
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] ‚ùå ML classification failed: {e}")
            logger.warning("[VECTOR-PDF] üîÑ Falling back to pattern matching...")
            
            # Fallback pattern matching (simplified version)
            text_lower = text.lower().strip()
            
            if 'response:' in text_lower:
                return 'response'
            elif 'inquiry information:' in text_lower:
                return 'inquiry'
            elif ('inquiry information:' in text_lower and 'response:' in text_lower):
                return 'example'
            elif any(pattern in text_lower for pattern in ['good afternoon', 'thanks for reaching', "we'd be glad"]):
                return 'response'
            elif any(pattern in text_lower for pattern in ['name:', 'lead created:', 'what kind of']):
                return 'inquiry'
            else:
                return 'general'
    
    def generate_embeddings(self, chunks: List[DocumentChunk]) -> List[Tuple[DocumentChunk, List[float]]]:
        """–ì–µ–Ω–µ—Ä—É—î OpenAI –µ–º–±–µ–¥—ñ–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫—ñ–≤"""
        if not self.openai_client:
            raise ValueError("OpenAI client not initialized")
        
        logger.info(f"[VECTOR-PDF] Generating embeddings for {len(chunks)} chunks")
        
        embeddings_data = []
        batch_size = 100  # OpenAI batch limit
        
        try:
            for i in range(0, len(chunks), batch_size):
                batch_chunks = chunks[i:i + batch_size]
                texts = [chunk.content for chunk in batch_chunks]
                
                logger.info(f"[VECTOR-PDF] Processing batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}")
                
                # Generate embeddings using text-embedding-3-small
                response = self.openai_client.embeddings.create(
                    model="text-embedding-3-small",
                    input=texts,
                    dimensions=1536  # Standard dimension for text-embedding-3-small
                )
                
                for chunk, embedding_obj in zip(batch_chunks, response.data):
                    embeddings_data.append((chunk, embedding_obj.embedding))
            
            logger.info(f"[VECTOR-PDF] Generated {len(embeddings_data)} embeddings successfully")
            return embeddings_data
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Error generating embeddings: {e}")
            raise
    
    def calculate_file_hash(self, file_content: bytes) -> str:
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î SHA-256 —Ö–µ—à –≤–º—ñ—Å—Ç—É —Ñ–∞–π–ª—É"""
        return hashlib.sha256(file_content).hexdigest()
    
    def process_pdf_file(
        self, 
        file_content: bytes, 
        filename: str, 
        business_id: str,
        location_id: Optional[str] = None
    ) -> Dict:
        """–ì–æ–ª–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ PDF —Ñ–∞–π–ª—É —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏"""
        
        logger.info(f"[VECTOR-PDF] ======== PROCESSING PDF FOR VECTOR STORAGE ========")
        logger.info(f"[VECTOR-PDF] File: {filename}")
        logger.info(f"[VECTOR-PDF] Business ID: {business_id}")
        logger.info(f"[VECTOR-PDF] Location ID: {location_id}")
        logger.info(f"[VECTOR-PDF] File size: {len(file_content)} bytes")
        
        try:
            # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ —Ñ–∞–π–ª—É
            if b'%PDF' in file_content[:100] or filename.lower().endswith('.pdf'):
                if not PDF_PROCESSING_AVAILABLE:
                    raise ValueError("PDF processing not available. Please install required libraries.")
                
                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É –¥–ª—è PyMuPDF
                file_hash = self.calculate_file_hash(file_content)
                temp_path = f"temp_pdfs/{file_hash}_{filename}"
                saved_path = default_storage.save(temp_path, ContentFile(file_content))
                full_path = default_storage.path(saved_path)
                
                try:
                    pages_data = self.extract_text_from_pdf(full_path)
                    all_text = "\n\n".join(page['text'] for page in pages_data)
                    page_count = len(pages_data)
                finally:
                    # –û—á–∏—â–µ–Ω–Ω—è —Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É
                    try:
                        default_storage.delete(saved_path)
                    except:
                        pass
            else:
                # Plain text file
                all_text = file_content.decode('utf-8', errors='ignore')
                page_count = 1
                file_hash = self.calculate_file_hash(file_content)
            
            if not all_text.strip():
                raise ValueError("No text content found in file")
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤
            chunks = self.create_semantic_chunks(all_text)
            
            if not chunks:
                raise ValueError("No valid chunks created from document")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤
            embeddings_data = self.generate_embeddings(chunks)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö
            from .vector_models import VectorDocument, VectorChunk
            
            with transaction.atomic():
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
                document = VectorDocument.objects.create(
                    business_id=business_id,
                    location_id=location_id,
                    filename=filename,
                    file_hash=file_hash,
                    file_size=len(file_content),
                    page_count=page_count,
                    chunk_count=len(chunks),
                    total_tokens=sum(chunk.token_count for chunk in chunks),
                    processing_status='completed',
                    embedding_model='text-embedding-3-small',
                    embedding_dimensions=1536,
                    metadata={
                        'processing_info': {
                            'total_chunks': len(chunks),
                            'chunks_by_type': {
                                chunk_type: len([c for c in chunks if c.chunk_type == chunk_type])
                                for chunk_type in set(chunk.chunk_type for chunk in chunks)
                            },
                            'avg_tokens_per_chunk': sum(chunk.token_count for chunk in chunks) / len(chunks)
                        }
                    }
                )
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏
                chunk_objects = []
                for chunk, embedding in embeddings_data:
                    chunk_obj = VectorChunk(
                        document=document,
                        content=chunk.content,
                        page_number=chunk.page_number,
                        chunk_index=chunk.chunk_index,
                        token_count=chunk.token_count,
                        embedding=embedding,  # pgvector –æ–±—Ä–æ–±–∏—Ç—å —Ü–µ
                        chunk_type=chunk.chunk_type,
                        metadata=chunk.metadata
                    )
                    chunk_objects.append(chunk_obj)
                
                # Bulk create chunks
                VectorChunk.objects.bulk_create(chunk_objects)
                
                logger.info(f"[VECTOR-PDF] ‚úÖ Successfully processed PDF:")
                logger.info(f"[VECTOR-PDF] - Document ID: {document.id}")
                logger.info(f"[VECTOR-PDF] - Chunks created: {len(chunk_objects)}")
                logger.info(f"[VECTOR-PDF] - Total tokens: {sum(chunk.token_count for chunk in chunks)}")
                logger.info(f"[VECTOR-PDF] - Chunk types: {document.metadata['processing_info']['chunks_by_type']}")
            
            logger.info(f"[VECTOR-PDF] ================================================")
            
            return {
                'document_id': document.id,
                'chunks_count': len(chunk_objects),
                'total_tokens': sum(chunk.token_count for chunk in chunks),
                'pages_count': page_count,
                'chunk_types': document.metadata['processing_info']['chunks_by_type']
            }
            
        except Exception as e:
            logger.error(f"[VECTOR-PDF] Error processing PDF: {e}")
            logger.exception("PDF processing error details")
            raise
    
    def create_semantic_chunks(self, text: str, max_tokens: int = 800) -> List[DocumentChunk]:
        """–°—Ç–≤–æ—Ä—é—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω—ñ —á–∞–Ω–∫–∏ –∑ —Ç–µ–∫—Å—Ç—É"""
        logger.info(f"[VECTOR-PDF] Creating semantic chunks with max_tokens: {max_tokens}")
        
        chunks = []
        chunk_index = 0
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Å–µ–∫—Ü—ñ—ó/–ø—Ä–∏–∫–ª–∞–¥–∏ —Å–ø–æ—á–∞—Ç–∫—É
        sections = self._split_by_sections(text)
        
        for section in sections:
            if not section.strip():
                continue
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —è–∫—â–æ —Å–µ–∫—Ü—ñ—è –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∞
            section_chunks = self._split_long_text(section, max_tokens)
            
            for chunk_text in section_chunks:
                token_count = self._count_tokens(chunk_text)
                
                if token_count > 0:
                    chunks.append(DocumentChunk(
                        content=chunk_text.strip(),
                        page_number=1,  # Default for text input
                        chunk_index=chunk_index,
                        token_count=token_count,
                        chunk_type=self._identify_chunk_type(chunk_text),
                        metadata={
                            'has_inquiry': 'Inquiry information:' in chunk_text or 'Name:' in chunk_text,
                            'has_response': 'Response:' in chunk_text,
                            'has_customer_name': bool(re.search(r'Name:\s*\w+', chunk_text)),
                            'has_service_type': any(word in chunk_text.lower() for word in ['roof', 'repair', 'construction', 'service']),
                            'section_length': len(chunk_text)
                        }
                    ))
                    chunk_index += 1
        
        logger.info(f"[VECTOR-PDF] Created {len(chunks)} semantic chunks")
        return chunks
    
    def _split_by_sections(self, text: str) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç –Ω–∞ –ª–æ–≥—ñ—á–Ω—ñ —Å–µ–∫—Ü—ñ—ó"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω–∏ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Sample Replies
        patterns = [
            r'Example\s*#\d+',
            r'Inquiry information:',
            r'Response:',
            r'\n\n\n+',  # –ú–Ω–æ–∂–∏–Ω–Ω—ñ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        ]
        
        sections = [text]
        
        for pattern in patterns:
            new_sections = []
            for section in sections:
                parts = re.split(f'({pattern})', section, flags=re.IGNORECASE)
                for part in parts:
                    if part.strip():
                        new_sections.append(part.strip())
            sections = new_sections
        
        return [s for s in sections if len(s.strip()) > 20]  # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Å–µ–∫—Ü—ñ—ó
    
    def _split_long_text(self, text: str, max_tokens: int) -> List[str]:
        """–†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç —â–æ –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π –Ω–∞ –º–µ–Ω—à—ñ —á–∞–Ω–∫–∏"""
        if not self.encoding:
            # Fallback: —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ —Å–∏–º–≤–æ–ª–∞–º–∏
            max_chars = max_tokens * 4
            if len(text) <= max_chars:
                return [text]
            
            chunks = []
            for i in range(0, len(text), max_chars):
                chunks.append(text[i:i + max_chars])
            return chunks
        
        token_count = len(self.encoding.encode(text))
        
        if token_count <= max_tokens:
            return [text]
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ —Ä–µ—á–µ–Ω–Ω—è–º–∏
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            test_chunk = f"{current_chunk} {sentence}".strip()
            if len(self.encoding.encode(test_chunk)) <= max_tokens:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _count_tokens(self, text: str) -> int:
        """–ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤ –≤ —Ç–µ–∫—Å—Ç—ñ"""
        if not self.encoding:
            return len(text) // 4  # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
        return len(self.encoding.encode(text))
    
    def _identify_chunk_type(self, text: str) -> str:
        """–í–∏–∑–Ω–∞—á–∞—î —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å–µ–∫—Ü—ñ—ó"""
        text_lower = text.lower()
        
        if 'inquiry information:' in text_lower:
            return 'inquiry'
        elif 'response:' in text_lower:
            return 'response'  
        elif ('inquiry information:' in text_lower and 'response:' in text_lower):
            return 'example'
        else:
            return 'general'
    
    def format_sample_replies(self, raw_content: str) -> str:
        """–§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è sample replies –¥–ª—è AI –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è"""
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∫–æ–¥–∏ –ø–æ–º–∏–ª–æ–∫
        if raw_content in ["PDF_BINARY_DETECTED", "PROCESSING_ERROR", "EMPTY_FILE", "ENCODING_ERROR", "PDF_BINARY_DETECTED_NO_PARSER"]:
            return raw_content
        
        if not raw_content or not raw_content.strip():
            return "EMPTY_CONTENT"
        
        # –ë–∞–∑–æ–≤–µ –æ—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
        cleaned = raw_content.strip()
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)  # –ó–∞–º—ñ–Ω—é—î–º–æ 3+ –ø–µ—Ä–µ–Ω–æ—Å–∏ –Ω–∞ 2
        cleaned = re.sub(r' +', ' ', cleaned)  # –ó–∞–º—ñ–Ω—é—î–º–æ –±–∞–≥–∞—Ç–æ –ø—Ä–æ–±—ñ–ª—ñ–≤ –Ω–∞ –æ–¥–∏–Ω
        cleaned = re.sub(r'\t+', ' ', cleaned)  # –ó–∞–º—ñ–Ω—é—î–º–æ —Ç–∞–±—É–ª—è—Ü—ñ—ó –Ω–∞ –ø—Ä–æ–±—ñ–ª
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ä—è–¥–∫—ñ–≤
        lines = cleaned.split('\n')
        lines = [line.strip() for line in lines]
        cleaned = '\n'.join(lines)
        
        # –í–∏–¥–∞–ª—è—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ —Ä—è–¥–∫–∏ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ç–∞ –≤ –∫—ñ–Ω—Ü—ñ
        cleaned = cleaned.strip()
        
        logger.info(f"[VECTOR-PDF] üîß Formatted sample replies:")
        logger.info(f"[VECTOR-PDF] - Original length: {len(raw_content)} chars")
        logger.info(f"[VECTOR-PDF] - Cleaned length: {len(cleaned)} chars")
        logger.info(f"[VECTOR-PDF] - Preview: {cleaned[:200]}...")
        
        return cleaned
    
    def validate_sample_replies_content(self, content: str) -> tuple[bool, Optional[str]]:
        """–í–∞–ª—ñ–¥—É—î —á–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è Sample Replies"""
        
        if not content or not content.strip():
            return False, "–ö–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π"
        
        if len(content.strip()) < 50:
            return False, "–ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π (–º—ñ–Ω—ñ–º—É–º 50 —Å–∏–º–≤–æ–ª—ñ–≤)"
        
        if len(content) > 100000:  # –ó–±—ñ–ª—å—à–∏–ª–∏ –ª—ñ–º—ñ—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è
            return False, "–ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π (–º–∞–∫—Å–∏–º—É–º 100,000 —Å–∏–º–≤–æ–ª—ñ–≤)"
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —î —Ö–æ—á–∞ –± –¥–µ—è–∫—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è Sample Replies
        content_lower = content.lower()
        keywords = [
            'inquiry', 'response', 'customer', 'hello', 'thank you', 
            'service', 'business', 'contact', 'help', 'question',
            '–∑–∞–ø–∏—Ç', '–≤—ñ–¥–ø–æ–≤—ñ–¥—å', '–∫–ª—ñ—î–Ω—Ç', '–¥—è–∫—É—é', '–ø–æ—Å–ª—É–≥–∞'
        ]
        
        has_keywords = any(keyword in content_lower for keyword in keywords)
        
        if not has_keywords:
            return False, "–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ —Å—Ö–æ–∂–∏–π –Ω–∞ Sample Replies (–Ω–µ–º–∞—î –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤)"
        
        logger.info(f"[VECTOR-PDF] ‚úÖ Content validation passed")
        return True, None

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π —ñ–Ω—Å—Ç–∞–Ω—Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–µ—Ä–≤—ñ—Å—É
vector_pdf_service = VectorPDFService()

# Legacy —ñ–Ω—Å—Ç–∞–Ω—Å –¥–ª—è backward compatibility
simple_pdf_service = vector_pdf_service
