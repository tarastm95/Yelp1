import openai
import logging
import time
import json
import re
from typing import Optional, Dict, Any
from django.conf import settings
from django.utils import timezone
from django.core.cache import cache
from .models import LeadDetail, YelpBusiness, AISettings, AutoResponseSettings
import os

logger = logging.getLogger(__name__)

class OpenAIService:
    """–°–µ—Ä–≤—ñ—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ OpenAI API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å"""
    
    def __init__(self):
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞ –∑ –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å"""
        try:
            # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ API –∫–ª—é—á –∑ .env —Ñ–∞–π–ª–∞
            openai_api_key = os.getenv('OPENAI_API_KEY')
            
            if not openai_api_key:
                # –Ø–∫—â–æ –Ω–µ–º–∞—î –≤ .env, –ø—Ä–æ–±—É—î–º–æ –∑ AISettings –º–æ–¥–µ–ª—ñ
                ai_settings = AISettings.objects.first()
                if ai_settings and ai_settings.openai_api_key:
                    openai_api_key = ai_settings.openai_api_key
            
            if openai_api_key:
                self.client = openai.OpenAI(
                    api_key=openai_api_key
                )
                logger.info("[AI-SERVICE] OpenAI client initialized successfully")
            else:
                logger.error("[AI-SERVICE] No OpenAI API key found in environment variables or database")
                self.client = None
                
        except Exception as e:
            logger.error(f"[AI-SERVICE] Failed to initialize OpenAI client: {e}")
            self.client = None
    
    def is_available(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –¥–æ—Å—Ç—É–ø–Ω–∏–π AI —Å–µ—Ä–≤—ñ—Å"""
        return self.client is not None
    
    def _get_ai_settings(self, business_ai_settings: Optional['AutoResponseSettings'] = None) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º—É—î AI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑ business-specific fallback –Ω–∞ global"""
        # –û—Ç—Ä–∏–º—É—î–º–æ –≥–ª–æ–±–∞–ª—å–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —è–∫ fallback
        global_ai_settings = AISettings.objects.first()
        
        # –ú–æ–¥–µ–ª—å
        if business_ai_settings and business_ai_settings.ai_model:
            model = business_ai_settings.ai_model
            logger.info(f"[AI-SERVICE] Using business-specific model: {model}")
        else:
            model = global_ai_settings.openai_model if global_ai_settings else "gpt-4o"
            logger.info(f"[AI-SERVICE] Using fallback model: {model}")
        
        # Temperature
        if business_ai_settings and business_ai_settings.ai_temperature is not None:
            temperature = business_ai_settings.ai_temperature
            logger.info(f"[AI-SERVICE] Using business-specific temperature: {temperature}")
        else:
            temperature = global_ai_settings.default_temperature if global_ai_settings else 0.7
            logger.info(f"[AI-SERVICE] Using fallback temperature: {temperature}")
        
        # Max message length
        if business_ai_settings and business_ai_settings.ai_max_message_length > 0:
            max_length = business_ai_settings.ai_max_message_length
            logger.info(f"[AI-SERVICE] Using business-specific max length: {max_length}")
        else:
            max_length = global_ai_settings.max_message_length if global_ai_settings else 160
            logger.info(f"[AI-SERVICE] Using fallback max length: {max_length}")
        
        return {
            'model': model,
            'temperature': temperature,
            'max_length': max_length,
            'global_settings': global_ai_settings
        }
    
    def check_rate_limit(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î rate limit –¥–ª—è AI –∑–∞–ø–∏—Ç—ñ–≤"""
        ai_settings = AISettings.objects.first()
        if not ai_settings:
            return True
        
        cache_key = "ai_requests_count"
        current_count = cache.get(cache_key, 0)
        
        if current_count >= ai_settings.requests_per_minute:
            logger.warning(f"[AI-SERVICE] Rate limit exceeded: {current_count}/{ai_settings.requests_per_minute}")
            return False
        
        # –ó–±—ñ–ª—å—à—É—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫ –Ω–∞ 1 —Ö–≤–∏–ª–∏–Ω—É
        cache.set(cache_key, current_count + 1, 60)
        return True
    
    def generate_greeting_message(
        self, 
        lead_detail: LeadDetail, 
        business: Optional[YelpBusiness] = None,
        is_off_hours: bool = False,
        # response_style removed - AI learns style from PDF examples
        include_location: bool = False,
        mention_response_time: bool = False,
        custom_prompt: Optional[str] = None,
        business_data_settings: Optional[Dict[str, bool]] = None,
        max_length: Optional[int] = None,
        business_ai_settings: Optional['AutoResponseSettings'] = None
    ) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä—É—î AI-powered greeting –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ª—ñ–¥–∞"""
        
        logger.info(f"[AI-SERVICE] ============= AI GREETING GENERATION =============")
        logger.info(f"[AI-SERVICE] Starting AI greeting generation")
        logger.info(f"[AI-SERVICE] Input parameters:")
        logger.info(f"[AI-SERVICE] - Lead ID: {lead_detail.lead_id}")
        logger.info(f"[AI-SERVICE] - Customer name: {lead_detail.user_display_name}")
        logger.info(f"[AI-SERVICE] - Business: {business.name if business else 'None'}")
        logger.info(f"[AI-SERVICE] - is_off_hours: {is_off_hours}")
        # logger response_style removed - AI learns style from PDF examples
        logger.info(f"[AI-SERVICE] - include_location: {include_location}")
        logger.info(f"[AI-SERVICE] - mention_response_time: {mention_response_time}")
        logger.info(f"[AI-SERVICE] - custom_prompt provided: {custom_prompt is not None}")
        logger.info(f"[AI-SERVICE] - business_data_settings: {business_data_settings}")
        
        if not self.is_available():
            logger.error("[AI-SERVICE] ‚ùå OpenAI client not available")
            return None
        
        logger.info(f"[AI-SERVICE] ‚úÖ OpenAI client is available")
        
        if not self.check_rate_limit():
            logger.warning("[AI-SERVICE] ‚ö†Ô∏è Rate limit exceeded, skipping AI generation")
            return None
        
        logger.info(f"[AI-SERVICE] ‚úÖ Rate limit check passed")
        
        try:
            logger.info(f"[AI-SERVICE] üîÑ Preparing lead context...")
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è AI
            context = self._prepare_lead_context(
                lead_detail, business, is_off_hours, 
                include_location, mention_response_time, business_data_settings, custom_prompt
            )
            
            logger.info(f"[AI-SERVICE] ‚úÖ Lead context prepared:")
            logger.info(f"[AI-SERVICE] - Customer name: {context.get('customer_name')}")
            logger.info(f"[AI-SERVICE] - Services: {context.get('services')}")
            logger.info(f"[AI-SERVICE] - Business name: {context.get('business_name')}")
            logger.info(f"[AI-SERVICE] - Phone number: {context.get('phone_number', 'Not provided')}")
            logger.info(f"[AI-SERVICE] - Additional info: {context.get('additional_info', 'None')[:50]}...")
            
            logger.info(f"[AI-SERVICE] üîÑ Creating prompt...")
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–æ–º–ø—Ç–∞
            prompt = self._create_greeting_prompt(
                context, custom_prompt
            )
            
            logger.info(f"[AI-SERVICE] ‚úÖ Prompt created (length: {len(prompt)} characters)")
            logger.info(f"[AI-SERVICE] Prompt preview: {prompt[:200]}...")
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è AI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ business-specific fallback
            ai_config = self._get_ai_settings(business_ai_settings)
            model = ai_config['model']
            temperature = ai_config['temperature']
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä max_length —è–∫—â–æ –Ω–∞–¥–∞–Ω–∏–π, —ñ–Ω–∞–∫—à–µ business/global
            if max_length is not None and max_length > 0:
                message_length = max_length
                logger.info(f"[AI-SERVICE] Using parameter max length: {message_length}")
            else:
                message_length = ai_config['max_length']
                logger.info(f"[AI-SERVICE] Using configured max length: {message_length}")
            
            logger.info(f"[AI-SERVICE] ü§ñ Final AI configuration:")
            logger.info(f"[AI-SERVICE] - Model: {model}")
            logger.info(f"[AI-SERVICE] - Temperature: {temperature}")
            logger.info(f"[AI-SERVICE] - Max tokens: {message_length}")
            
            logger.info(f"[AI-SERVICE] ü§ñ Calling OpenAI API...")
            logger.info(f"[AI-SERVICE] ========== OPENAI API CALL ==========")
            logger.info(f"[AI-SERVICE] About to call OpenAI chat completion...")
            logger.info(f"[AI-SERVICE] API parameters:")
            logger.info(f"[AI-SERVICE] - Model: {model}")
            logger.info(f"[AI-SERVICE] - Temperature: {temperature}")
            logger.info(f"[AI-SERVICE] - Max tokens: {message_length}")
            
            # üéØ –î–ª—è contextual AI analysis –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ custom prompt —è–∫ system prompt
            system_prompt = self._get_system_prompt(custom_prompt)
            logger.info(f"[AI-SERVICE] - System prompt length: {len(system_prompt)} characters")
            logger.info(f"[AI-SERVICE] - User prompt length: {len(prompt)} characters")
            
            logger.info(f"[AI-SERVICE] System prompt preview: {system_prompt[:200]}..." + ("" if len(system_prompt) <= 200 else " (truncated)"))
            logger.info(f"[AI-SERVICE] User prompt preview: {prompt[:200]}..." + ("" if len(prompt) <= 200 else " (truncated)"))
            logger.info(f"[AI-SERVICE] üöÄ Making OpenAI API request...")
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ
            messages = self._prepare_messages_for_model(model, system_prompt, prompt)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å –º–æ–¥–µ–ª—ñ
            api_params = self._get_api_params_for_model(model, messages, message_length, temperature)
            
            # –í–∏–∫–ª–∏–∫ OpenAI API
            response = self.client.chat.completions.create(**api_params)
            logger.info(f"[AI-SERVICE] ‚úÖ OpenAI API call completed successfully")
            logger.info(f"[AI-SERVICE] ‚úÖ OpenAI API responded successfully")
            
            ai_message = response.choices[0].message.content.strip()
            original_length = len(ai_message)
            
            logger.info(f"[AI-SERVICE] Raw AI response:")
            logger.info(f"[AI-SERVICE] - Original message: '{ai_message}'")
            logger.info(f"[AI-SERVICE] - Original length: {original_length} characters")
            
            # OpenAI –≤–∂–µ –æ–±–º–µ–∂–∏–≤ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —á–µ—Ä–µ–∑ max_tokens, –¥–æ–¥–∞—Ç–∫–æ–≤–µ –æ–±—Ä—ñ–∑–∞–Ω–Ω—è –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ
            logger.info(f"[AI-SERVICE] ‚úÖ Message generated within token limit")
            
            logger.info(f"[AI-SERVICE] üéâ FINAL AI GREETING:")
            logger.info(f"[AI-SERVICE] - Final message: '{ai_message}'")
            logger.info(f"[AI-SERVICE] - Final length: {len(ai_message)} characters")
            logger.info(f"[AI-SERVICE] Generated greeting for lead {lead_detail.lead_id}: {ai_message[:50]}...")
            logger.info(f"[AI-SERVICE] ==============================================")
            
            return ai_message
            
        except Exception as e:
            logger.error(f"[AI-SERVICE] ‚ùå Error generating AI greeting: {e}")
            logger.exception(f"[AI-SERVICE] AI generation exception details")
            logger.info(f"[AI-SERVICE] ==============================================")
            return None
    
    def generate_preview_message(
        self,
        business,  # YelpBusiness object
        # response_style removed - AI learns style from PDF examples
        include_location: bool = False,
        mention_response_time: bool = False,
        custom_prompt: Optional[str] = None,
        business_data_settings: Optional[Dict[str, bool]] = None,
        max_length: Optional[int] = None,
        custom_preview_text: Optional[str] = None,  # üéØ –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        business_ai_settings: Optional['AutoResponseSettings'] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä—É—î –ø—Ä–µ–≤ º—é –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å"""
        
        if not self.is_available():
            return "AI service not available. Please check your settings."
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å
        logger.info(f"[AI-SERVICE] üìä Received business_data_settings: {business_data_settings}")
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –¥–ª—è –ø—Ä–µ–≤ º—é
        if not business_data_settings:
            logger.info(f"[AI-SERVICE] ‚ö†Ô∏è No business_data_settings provided, using defaults")
            business_data_settings = {
                "include_rating": True,
                "include_categories": True,
                "include_phone": True,
                "include_website": False,
                "include_price_range": True,
                "include_hours": True,
                "include_reviews_count": True,
                "include_address": False,
                "include_transactions": False
            }
        else:
            logger.info(f"[AI-SERVICE] ‚úÖ Using provided business_data_settings")
        
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –†–ï–ê–õ–¨–ù–Ü –¥–∞–Ω—ñ –±—ñ–∑–Ω–µ—Å—É
            real_business_data = {
                "name": business.name,
                "location": business.location if (include_location and business.location) else "",
                "time_zone": business.time_zone or "",
                "open_days": business.open_days or "",
                "open_hours": business.open_hours or ""
            }
            
            # –î–æ–¥–∞—î–º–æ —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑ business.details JSON —è–∫—â–æ –≤–æ–Ω–∏ —î
            if business.details and isinstance(business.details, dict):
                logger.info(f"[AI-SERVICE] üìã Business details available, extracting data...")
                # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑ Yelp API
                details_data = {
                    "rating": business.details.get("rating"),
                    "review_count": business.details.get("review_count"),
                    "categories": business.details.get("categories", []),
                    "phone": business.details.get("display_phone") or business.details.get("phone"),
                    "website": business.details.get("url"),
                    "price": business.details.get("price"),
                    "address": business.details.get("location", {}).get("display_address", []),
                    "city": business.details.get("location", {}).get("city"),
                    "state": business.details.get("location", {}).get("state"),
                    "zip_code": business.details.get("location", {}).get("zip_code"),
                    "transactions": business.details.get("transactions", [])
                }
                
                # –õ–æ–≥—É–≤–∞–Ω–Ω—è –Ω–∞—è–≤–Ω–∏—Ö –¥–∞–Ω–∏—Ö
                for key, value in details_data.items():
                    if value:
                        logger.info(f"[AI-SERVICE] üìä {key}: {value if isinstance(value, (str, int, float)) else f'{type(value).__name__} with {len(value)} items' if hasattr(value, '__len__') else type(value).__name__}")
                
                real_business_data.update(details_data)
            else:
                logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è No business details available or invalid format")
            
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ç—ñ —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ, —è–∫—ñ —É–≤—ñ–º–∫–Ω–µ–Ω—ñ
            filtered_business_data = {"name": real_business_data["name"]}
            logger.info(f"[AI-SERVICE] üîç FILTERING: Starting with base business name: {real_business_data['name']}")
            
            # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ç—ñ –¥–∞–Ω—ñ, —è–∫—ñ —î —ñ —É–≤—ñ–º–∫–Ω–µ–Ω—ñ
            if business_data_settings.get("include_rating"):
                logger.info(f"[AI-SERVICE] üîç RATING CHECK: include_rating={business_data_settings.get('include_rating')}, rating_data={real_business_data.get('rating')}")
                if real_business_data.get("rating"):
                    filtered_business_data["rating"] = real_business_data["rating"]
                    logger.info(f"[AI-SERVICE] ‚úÖ RATING ADDED: {real_business_data['rating']}")
                    if business_data_settings.get("include_reviews_count") and real_business_data.get("review_count"):
                        filtered_business_data["review_count"] = real_business_data["review_count"]
                        logger.info(f"[AI-SERVICE] ‚úÖ REVIEW COUNT ADDED: {real_business_data['review_count']}")
                else:
                    logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è RATING: Enabled but no rating data available")
            else:
                logger.info(f"[AI-SERVICE] ‚è≠Ô∏è RATING: Skipped (disabled in settings)")
            
            if business_data_settings.get("include_categories"):
                logger.info(f"[AI-SERVICE] üîç CATEGORIES CHECK: include_categories={business_data_settings.get('include_categories')}, categories_data={real_business_data.get('categories')}")
                if real_business_data.get("categories"):
                    filtered_business_data["categories"] = real_business_data["categories"]
                    logger.info(f"[AI-SERVICE] ‚úÖ CATEGORIES ADDED: {real_business_data['categories']}")
                else:
                    logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è CATEGORIES: Enabled but no categories data available")
            else:
                logger.info(f"[AI-SERVICE] ‚è≠Ô∏è CATEGORIES: Skipped (disabled in settings)")
            
            if business_data_settings.get("include_phone"):
                logger.info(f"[AI-SERVICE] üîç PHONE CHECK: include_phone={business_data_settings.get('include_phone')}, phone_data={real_business_data.get('phone')}")
                if real_business_data.get("phone"):
                    filtered_business_data["phone"] = real_business_data["phone"]
                    logger.info(f"[AI-SERVICE] ‚úÖ PHONE ADDED: {real_business_data['phone']}")
                else:
                    logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è PHONE: Enabled but no phone data available")
            else:
                logger.info(f"[AI-SERVICE] ‚è≠Ô∏è PHONE: Skipped (disabled in settings)")
            
            if business_data_settings.get("include_website") and real_business_data.get("website"):
                filtered_business_data["website"] = real_business_data["website"]
            
            if business_data_settings.get("include_price_range") and real_business_data.get("price"):
                filtered_business_data["price"] = real_business_data["price"]
            
            if business_data_settings.get("include_address") and real_business_data.get("address"):
                filtered_business_data["address"] = real_business_data["address"]
                if real_business_data.get("city"):
                    filtered_business_data["city"] = real_business_data["city"]
                if real_business_data.get("state"):
                    filtered_business_data["state"] = real_business_data["state"]
                if real_business_data.get("zip_code"):
                    filtered_business_data["zip_code"] = real_business_data["zip_code"]
            
            if business_data_settings.get("include_hours"):
                if real_business_data.get("open_hours"):
                    filtered_business_data["open_hours"] = real_business_data["open_hours"]
                if real_business_data.get("open_days"):
                    filtered_business_data["open_days"] = real_business_data["open_days"]
            
            if business_data_settings.get("include_transactions") and real_business_data.get("transactions"):
                filtered_business_data["transactions"] = real_business_data["transactions"]
            
            # –î–æ–¥–∞—î–º–æ location —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            if include_location and real_business_data.get("location"):
                filtered_business_data["location"] = real_business_data["location"]
            
            # –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            logger.info(f"[AI-SERVICE] üîç Filtered business_data keys: {list(filtered_business_data.keys())}")
            logger.info(f"[AI-SERVICE] üìä Business data includes:")
            for key, value in filtered_business_data.items():
                if key != "name":  # –ù–µ –ª–æ–≥—É—î–º–æ —ñ–º'—è –±—ñ–∑–Ω–µ—Å—É
                    logger.info(f"[AI-SERVICE] - {key}: {'‚úÖ included' if value else '‚ùå empty'}")
            
            # üéØ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ custom preview text —è–∫—â–æ –Ω–∞–¥–∞–Ω–æ, —ñ–Ω–∞–∫—à–µ –º–æ–∫–æ–≤—ñ –¥–∞–Ω—ñ
            if custom_preview_text:
                original_customer_text = custom_preview_text
                logger.info(f"[AI-SERVICE] üéØ Using custom preview text: {original_customer_text[:100]}...")
            else:
                original_customer_text = "[Customer Message]"
                logger.info(f"[AI-SERVICE] Using default placeholder customer text for preview")
            
            context = {
                "customer_name": "[Client Name]",  # –ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –∑–∞–º—ñ—Å—Ç—å –º–æ–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
                "services": "[Services Requested]",  # –ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –∑–∞–º—ñ—Å—Ç—å –º–æ–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
                "additional_info": "[Additional Information]",  # –ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –∑–∞–º—ñ—Å—Ç—å –º–æ–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
                "phone_number": "[Phone Number]",  # –ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –∑–∞–º—ñ—Å—Ç—å –º–æ–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
                "business_name": filtered_business_data["name"],  # –†–ï–ê–õ–¨–ù–ï —ñ–º'—è –±—ñ–∑–Ω–µ—Å—É
                "business_location": filtered_business_data.get("location", ""),  # –†–µ–∞–ª—å–Ω–∞ –ª–æ–∫–∞—Ü—ñ—è
                "is_off_hours": False,
                "mention_response_time": mention_response_time,
                "business_data": filtered_business_data,  # –¢—ñ–ª—å–∫–∏ —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –±—ñ–∑–Ω–µ—Å—É
                "business_data_settings": business_data_settings,
                "original_customer_text": original_customer_text  # üéØ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ custom –∞–±–æ –º–æ–∫–æ–≤—ñ –¥–∞–Ω—ñ
            }
            
            # üîç –ù–û–í–ò–ô VECTOR SEARCH INTEGRATION for Custom Preview Text
            if custom_preview_text and business_ai_settings and business_ai_settings.use_sample_replies:
                logger.info(f"[AI-SERVICE] üîç PREVIEW: Using NEW inquiry‚Üíresponse pair matching")
                logger.info(f"[AI-SERVICE] Custom text: {custom_preview_text[:100]}...")
                
                try:
                    # üéØ –í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ–ú–û –ù–û–í–ò–ô –ü–Ü–î–•–Ü–î - Sample Replies Response
                    logger.info(f"[AI-SERVICE] üöÄ PREVIEW: Calling generate_sample_replies_response with new pair matching...")
                    
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–∫ LeadDetail –¥–ª—è preview
                    from .models import LeadDetail
                    mock_lead = LeadDetail(
                        lead_id="preview_test",
                        user_display_name="Test Customer",
                        business_id=business.business_id
                    )
                    
                    # –î–æ–¥–∞—î–º–æ custom text —è–∫ project additional_info –¥–ª—è _get_lead_text()
                    mock_lead.project = {"additional_info": custom_preview_text}
                    
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ–≤–∏–π Sample Replies –ø—ñ–¥—Ö—ñ–¥
                    vector_response = self.generate_sample_replies_response(
                        lead_detail=mock_lead,
                        business=business,
                        max_length=max_length or 160,
                        business_ai_settings=business_ai_settings,
                        use_vector_search=True
                    )
                    
                    if vector_response:
                        logger.info(f"[AI-SERVICE] üéâ PREVIEW: Generated with NEW inquiry‚Üíresponse pairs!")
                        logger.info(f"[AI-SERVICE] Preview response: {vector_response}")
                        return vector_response  # ‚úÖ –ü–û–í–ï–†–¢–ê–Ñ–ú–û –û–î–†–ê–ó–£ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ–≤–∏–π –ø—ñ–¥—Ö—ñ–¥
                    else:
                        logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è PREVIEW: New approach failed, using fallback...")
                        
                except Exception as vector_error:
                    logger.error(f"[AI-SERVICE] ‚ùå PREVIEW: New vector approach failed: {vector_error}")
                    logger.info(f"[AI-SERVICE] üîÑ PREVIEW: Falling back to old approach...")
            
            # üîÑ FALLBACK: –°—Ç–∞—Ä–∏–π –ø—ñ–¥—Ö—ñ–¥ —è–∫—â–æ –Ω–æ–≤–∏–π –Ω–µ –ø—Ä–∞—Ü—é—î –∞–±–æ Sample Replies –≤–∏–º–∫–Ω–µ–Ω—ñ
            context["vector_search_enabled"] = False
            if not business_ai_settings:
                logger.info(f"[AI-SERVICE] PREVIEW: No business AI settings - using fallback")
            elif not business_ai_settings.use_sample_replies:
                logger.info(f"[AI-SERVICE] PREVIEW: Sample Replies disabled - using Custom Instructions")
            elif not custom_preview_text:
                logger.info(f"[AI-SERVICE] PREVIEW: No custom preview text - using mock data")
            
            prompt = self._create_greeting_prompt(context, custom_prompt)
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è AI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ business-specific fallback –¥–ª—è preview
            ai_config = self._get_ai_settings(business_ai_settings)
            model = ai_config['model']
            temperature = ai_config['temperature']
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è debug
            logger.info(f"[AI-SERVICE] Selected model: {model}")
            logger.info(f"[AI-SERVICE] Temperature: {temperature}")
            
            # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–ª—è GPT-5 –º–æ–¥–µ–ª–µ–π
            if model.startswith('gpt-5'):
                logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è Using GPT-5 model: {model}")
                logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è Note: GPT-5 models may not be available in all OpenAI accounts yet")
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä max_length —è–∫—â–æ –Ω–∞–¥–∞–Ω–∏–π, —ñ–Ω–∞–∫—à–µ business/global
            # üîß –ö–û–ù–í–ï–†–¢–ê–¶–Ü–Ø –°–ò–ú–í–û–õ–Ü–í ‚Üí –¢–û–ö–ï–ù–ò (–ø–µ—Ä–µ–¥ API –≤–∏–∫–ª–∏–∫–æ–º)
            if max_length is not None and max_length > 0:
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Å–∏–º–≤–æ–ª–∏ –≤ —Ç–æ–∫–µ–Ω–∏ –¥–ª—è OpenAI API
                # –ü—Ä–∏–±–ª–∏–∑–Ω–æ: 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∏ –¥–ª—è –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏
                estimated_tokens = max(1, max_length // 4)  # –ú—ñ–Ω—ñ–º—É–º 1 —Ç–æ–∫–µ–Ω
                message_length = estimated_tokens
                logger.info(f"[AI-SERVICE] Preview max length: {max_length} chars ‚Üí {estimated_tokens} tokens")
            else:
                message_length = ai_config['max_length']
                logger.info(f"[AI-SERVICE] Preview using configured max length: {message_length} tokens")
            
            # üéØ –î–ª—è contextual AI analysis –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ custom prompt —è–∫ system prompt
            system_prompt = self._get_system_prompt(custom_prompt)
            
            # –î–ª—è GPT-5 –º–æ–¥–µ–ª–µ–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ–≤–Ω–∏–π –∫–∞—Å—Ç–æ–º–Ω–∏–π –ø—Ä–æ–º—Ç –±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å
            if model.startswith('gpt-5'):
                logger.info(f"[AI-SERVICE] GPT-5: Using full custom system prompt (length: {len(system_prompt)})")
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ
            messages = self._prepare_messages_for_model(model, system_prompt, prompt)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å –º–æ–¥–µ–ª—ñ
            api_params = self._get_api_params_for_model(model, messages, message_length, temperature)
            
            logger.info(f"[AI-SERVICE] Making API call with params: {api_params}")
            
            try:
                response = self.client.chat.completions.create(**api_params)
                
                logger.info(f"[AI-SERVICE] Response received successfully")
                logger.info(f"[AI-SERVICE] Response choices count: {len(response.choices)}")
                
                if response.choices and len(response.choices) > 0:
                    choice = response.choices[0]
                    # –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    logger.info(f"[AI-SERVICE] Finish reason: {choice.finish_reason}")
                    logger.info(f"[AI-SERVICE] Message role: {choice.message.role}")
                    logger.info(f"[AI-SERVICE] Tool calls: {choice.message.tool_calls}")
                    logger.info(f"[AI-SERVICE] Usage: {response.usage}")
                    
                    content = choice.message.content
                    logger.info(f"[AI-SERVICE] Response content length: {len(content) if content else 0}")
                    logger.info(f"[AI-SERVICE] Response content: '{content}'")
                    
                    if not content:
                        logger.warning(f"[AI-SERVICE] Empty content received from {model}")
                        return "Empty response received from AI model."
                    
                    return content.strip()
                else:
                    logger.error(f"[AI-SERVICE] No choices in response from {model}")
                    return "No response choices received from AI model."
                    
            except Exception as api_error:
                logger.error(f"[AI-SERVICE] API call failed for {model}: {api_error}")
                
                # Fallback –¥–ª—è GPT-5 –º–æ–¥–µ–ª–µ–π –¥–æ gpt-4o
                if model.startswith('gpt-5'):
                    logger.warning(f"[AI-SERVICE] üîÑ GPT-5 failed, trying fallback to gpt-4o")
                    try:
                        fallback_params = self._get_api_params_for_model('gpt-4o', messages, message_length, temperature)
                        logger.info(f"[AI-SERVICE] Making fallback API call with gpt-4o")
                        
                        fallback_response = self.client.chat.completions.create(**fallback_params)
                        content = fallback_response.choices[0].message.content
                        
                        if content:
                            logger.info(f"[AI-SERVICE] ‚úÖ Fallback successful with gpt-4o")
                            return content.strip()
                        else:
                            logger.error(f"[AI-SERVICE] Fallback also returned empty content")
                            return "Fallback model also returned empty response."
                            
                    except Exception as fallback_error:
                        logger.error(f"[AI-SERVICE] Fallback to gpt-4o also failed: {fallback_error}")
                        return f"Both {model} and fallback failed: {str(api_error)}"
                else:
                    # –î–ª—è —ñ–Ω—à–∏—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç–æ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ–º–∏–ª–∫—É
                    return f"API call failed: {str(api_error)}"
            
        except Exception as e:
            logger.error(f"[AI-SERVICE] Error generating preview: {e}")
            return f"Error generating preview: {str(e)}"

    def _prepare_messages_for_model(self, model: str, system_prompt: str, user_prompt: str) -> list:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        
        if model.startswith("o1"):
            # o1 –º–æ–¥–µ–ª—ñ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å system role
            # –ö–æ–º–±—ñ–Ω—É—î–º–æ system prompt –∑ user prompt
            combined_prompt = f"{system_prompt}\n\nUser request: {user_prompt}"
            logger.info(f"[AI-SERVICE] o1 model detected: combining system and user prompts")
            return [{"role": "user", "content": combined_prompt}]
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å system role
            return [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

    def _get_api_params_for_model(self, model: str, messages: list, max_tokens: int, temperature: float) -> dict:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å –º–æ–¥–µ–ª—ñ"""
        
        params = {
            "model": model,
            "messages": messages
        }
        
        if model.startswith("o1"):
            # o1 –º–æ–¥–µ–ª—ñ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å temperature —Ç–∞ max_tokens
            logger.info(f"[AI-SERVICE] o1 model: skipping temperature and max_tokens parameters")
        elif model.startswith("gpt-5"):
            # GPT-5 –º–æ–¥–µ–ª—ñ –º–∞—é—Ç—å –ø—Ä–æ–±–ª–µ–º–∏ –∑ temperature —ñ max_completion_tokens
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π max_tokens —ñ –Ω–µ –ø–µ—Ä–µ–¥–∞—î–º–æ temperature
            params["max_tokens"] = max_tokens
            logger.info(f"[AI-SERVICE] GPT-5 model: using max_tokens without temperature")
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å max_tokens
            params["max_tokens"] = max_tokens
            params["temperature"] = temperature
            logger.info(f"[AI-SERVICE] Standard model: using max_tokens and temperature")
        
        return params
    
    def _prepare_lead_context(
        self, 
        lead_detail: LeadDetail, 
        business: Optional[YelpBusiness],
        is_off_hours: bool,
        include_location: bool,
        mention_response_time: bool,
        business_data_settings: Optional[Dict[str, bool]] = None,
        custom_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ª—ñ–¥–∞ –¥–ª—è AI –∑ –ø–æ–≤–Ω–æ—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –±—ñ–∑–Ω–µ—Å"""
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        if not business_data_settings:
            business_data_settings = {
                "include_rating": True,
                "include_categories": True,
                "include_phone": True,
                "include_website": False,
                "include_price_range": True,
                "include_hours": True,
                "include_reviews_count": True,
                "include_address": False,
                "include_transactions": False
            }
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç –∫–ª—ñ—î–Ω—Ç–∞ –¥–ª—è contextual analysis
        original_customer_text = self._get_lead_text(lead_detail)
        
        # –ë–∞–∑–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∫–ª—ñ—î–Ω—Ç–∞
        context = {
            "customer_name": lead_detail.user_display_name or "there",
            "services": ", ".join(lead_detail.project.get("job_names", [])) if lead_detail.project else "",
            "phone_number": getattr(lead_detail, 'phone_number', None),
            "additional_info": getattr(lead_detail, 'additional_notes', '') or '',
            "created_at": lead_detail.created_at if hasattr(lead_detail, 'created_at') else timezone.now(),
            "is_off_hours": is_off_hours,
            "mention_response_time": mention_response_time,
            "original_customer_text": original_customer_text  # üéØ –î–ª—è contextual AI analysis
        }
        
        if not business:
            context.update({
                "business_name": "our business",
                "business_location": "",
                "business_data": {}
            })
            return context
        
        # –ü–û–í–ù–ê —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –±—ñ–∑–Ω–µ—Å –∑ YelpBusiness
        business_data = {
            "name": business.name,
            "location": business.location,
            "time_zone": business.time_zone,
            "open_days": business.open_days,
            "open_hours": business.open_hours,
            "business_id": business.business_id
        }
        
        # –î–æ–¥–∞—î–º–æ –¥–∞–Ω—ñ –∑ details JSON –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å
        if business.details:
            details = business.details
            
            # –†–µ–π—Ç–∏–Ω–≥ —Ç–∞ –≤—ñ–¥–≥—É–∫–∏
            if business_data_settings.get("include_rating") and details.get("rating"):
                business_data["rating"] = details.get("rating", 0)
                if business_data_settings.get("include_reviews_count"):
                    business_data["review_count"] = details.get("review_count", 0)
            
            # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó –±—ñ–∑–Ω–µ—Å—É
            if business_data_settings.get("include_categories") and details.get("categories"):
                business_data["categories"] = [cat.get("title", "") for cat in details.get("categories", [])]
                business_data["category_aliases"] = [cat.get("alias", "") for cat in details.get("categories", [])]
            
            # –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
            if business_data_settings.get("include_phone") and details.get("phone"):
                business_data["phone"] = details.get("phone", "")
            
            if business_data_settings.get("include_website") and details.get("url"):
                business_data["website"] = details.get("url", "")
            
            # –¶—ñ–Ω–æ–≤–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω
            if business_data_settings.get("include_price_range") and details.get("price"):
                business_data["price"] = details.get("price", "")
            
            # –ê–¥—Ä–µ—Å–∞
            if business_data_settings.get("include_address") and details.get("location"):
                location_data = details.get("location", {})
                business_data["address"] = location_data.get("display_address", [])
                business_data["city"] = location_data.get("city", "")
                business_data["state"] = location_data.get("state", "")
                business_data["zip_code"] = location_data.get("zip_code", "")
            
            # –†–æ–±–æ—á—ñ –≥–æ–¥–∏–Ω–∏
            if business_data_settings.get("include_hours") and details.get("hours"):
                business_data["hours_detailed"] = details.get("hours", [])
                business_data["is_open_now"] = details.get("hours", [{}])[0].get("is_open_now", False) if details.get("hours") else False
            
            # –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó —Ç–∞ –ø–æ—Å–ª—É–≥–∏
            if business_data_settings.get("include_transactions") and details.get("transactions"):
                business_data["transactions"] = details.get("transactions", [])
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –¥–∞–Ω—ñ (–∑–∞–≤–∂–¥–∏ –≤–∫–ª—é—á–∞—î–º–æ –¥–ª—è —Ä–æ–∑—à–∏—Ä–µ–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π)
            business_data.update({
                "coordinates": details.get("coordinates", {}),
                "image_url": details.get("image_url", ""),
                "is_claimed": details.get("is_claimed", False),
                "is_closed": details.get("is_closed", False),
                "yelp_url": details.get("url", "")
            })
        
        context.update({
            "business_name": business.name,
            "business_location": business.location if include_location else "",
            "business_data": business_data,
            "business_data_settings": business_data_settings
        })
        
        # üéØ CONTEXTUAL AI: –ë—ñ–ª—å—à–µ –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω –ø–∞—Ä—Å–∏–Ω–≥ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ñ–≤ 
        # AI —Å–∞–º –∞–Ω–∞–ª—ñ–∑—É—î original_customer_text –∑–≥—ñ–¥–Ω–æ –∑ custom prompt
        
        return context
    
    def _get_system_prompt(self, custom_prompt: Optional[str] = None) -> str:
        """–û—Ç—Ä–∏–º—É—î —Å–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç (—Ç—ñ–ª—å–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–∏–π)"""
        if custom_prompt:
            return custom_prompt
        
        ai_settings = AISettings.objects.first()
        if ai_settings and ai_settings.base_system_prompt:
            return ai_settings.base_system_prompt
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Ä—è–¥–æ–∫ –∑–∞–º—ñ—Å—Ç—å –∑–∞—Ö–∞—Ä–¥–∫–æ–¥–∂–µ–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        return ""
    
    def _create_greeting_prompt(
        self, 
        context: Dict[str, Any], 
        # response_style removed - AI learns style from PDF examples
        custom_prompt: Optional[str] = None
    ) -> str:
        """–°—Ç–≤–æ—Ä—é—î –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑ –ø–æ–≤–Ω–∏–º–∏ –±—ñ–∑–Ω–µ—Å-–¥–∞–Ω–∏–º–∏"""
        
        # üéØ CONTEXTUAL AI ANALYSIS: –Ø–∫—â–æ —î custom prompt - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ —è–∫ system prompt
        if custom_prompt:
            # –û—Ç—Ä–∏–º—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç –∫–ª—ñ—î–Ω—Ç–∞
            customer_text = context.get('original_customer_text', '')
            customer_name = context.get('customer_name', 'there')
            business_name = context.get('business_name', 'our business')
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∏–π user prompt –¥–ª—è AI –∞–Ω–∞–ª—ñ–∑—É –∑ business data
            business_data = context.get('business_data', {})
            business_data_settings = context.get('business_data_settings', {})
            
            # –§–æ—Ä–º—É—î–º–æ business information –¥–ª—è contextual prompt
            business_info_parts = []
            
            # Address information
            if business_data_settings.get('include_address'):
                address_parts = []
                city = business_data.get('city', '')
                state = business_data.get('state', '')
                zip_code = business_data.get('zip_code', '')
                
                # –Ø–∫—â–æ —î –æ–∫—Ä–µ–º—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∞–¥—Ä–µ—Å–∏
                if city and state and zip_code:
                    address_parts.append(f"{city}, {state} {zip_code}")
                elif city and state:
                    address_parts.append(f"{city}, {state}")
                elif city:
                    address_parts.append(city)
                
                # –Ø–∫—â–æ —î –¥–æ–¥–∞—Ç–∫–æ–≤–∞ –∞–¥—Ä–µ—Å–∞ –∑ Yelp API (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≤—É–ª–∏—Ü—è)
                if business_data.get('address'):
                    yelp_address = ", ".join(str(addr) for addr in business_data['address']) if isinstance(business_data['address'], list) else str(business_data['address'])
                    # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —Ü–µ –Ω–µ –¥—É–±–ª—é—î city
                    if yelp_address and yelp_address.lower() != city.lower():
                        address_parts.insert(0, yelp_address)  # –í—É–ª–∏—Ü—è —Å–ø–µ—Ä–µ–¥—É
                
                if address_parts:
                    full_address = ", ".join(address_parts)
                    business_info_parts.append(f"Address: {full_address}")
            
            # Phone
            if business_data_settings.get('include_phone') and business_data.get('phone'):
                business_info_parts.append(f"Phone: {business_data['phone']}")
            
            # Rating and reviews
            if business_data_settings.get('include_rating') and business_data.get('rating'):
                rating = business_data['rating']
                review_count = business_data.get('review_count', 0)
                if business_data_settings.get('include_reviews_count') and review_count > 0:
                    business_info_parts.append(f"Rating: {rating}/5 stars ({review_count} reviews)")
                else:
                    business_info_parts.append(f"Rating: {rating}/5 stars")
            
            # Categories
            if business_data_settings.get('include_categories') and business_data.get('categories'):
                category_titles = []
                for cat in business_data['categories'][:3]:
                    if isinstance(cat, dict):
                        category_titles.append(cat.get('title', cat.get('alias', str(cat))))
                    else:
                        category_titles.append(str(cat))
                if category_titles:
                    business_info_parts.append(f"Specializes in: {', '.join(category_titles)}")
            
            # Website
            if business_data_settings.get('include_website') and business_data.get('website'):
                business_info_parts.append("Website available")
            
            # Price range
            if business_data_settings.get('include_price_range') and business_data.get('price'):
                business_info_parts.append(f"Price range: {business_data['price']}")
            
            # Business hours and status
            if business_data_settings.get('include_hours'):
                if context.get('is_off_hours'):
                    business_info_parts.append("Currently closed - outside business hours")
                elif business_data.get('is_open_now'):
                    business_info_parts.append("Currently open")
            
            # Transactions/services
            if business_data_settings.get('include_transactions') and business_data.get('transactions'):
                transaction_items = [str(item) for item in business_data['transactions'][:3]]
                if transaction_items:
                    business_info_parts.append(f"Available services: {', '.join(transaction_items)}")
            
            business_info = "\n".join(business_info_parts) if business_info_parts else "No additional business information configured."
            
            # üîç VECTOR SEARCH CONTEXT for Custom Preview Text
            vector_context = ""
            if context.get('sample_replies_context'):
                logger.info(f"[AI-SERVICE] üîç Adding vector search context to preview prompt")
                similar_chunks = context['sample_replies_context']
                
                vector_parts = []
                for i, chunk in enumerate(similar_chunks[:3]):  # Top 3 most similar
                    similarity_score = chunk['similarity_score']
                    chunk_type = chunk['chunk_type']
                    content = chunk['content']
                    
                    vector_parts.append(
                        f"Similar Example {i+1} (similarity: {similarity_score:.2f}, type: {chunk_type}):\n{content}\n"
                    )
                
                vector_context = f"""
RELEVANT SAMPLE REPLIES (found via vector search):
{chr(10).join(vector_parts)}

STYLE GUIDANCE: Use the tone, approach, and communication style from the similar examples above. The higher the similarity score, the more closely you should match that style.
"""
                logger.info(f"[AI-SERVICE] üìù Added {len(similar_chunks)} vector chunks to preview context")
            
            contextual_prompt = f"""Customer message:
"{customer_text}"

Customer name: {customer_name}
Business name: {business_name}

Business Information:
{business_info}{vector_context}

Please analyze the customer's request and respond according to the instructions provided in the system prompt. Use the business information provided above when generating your response. Generate a complete, personalized response."""
            
            logger.info(f"[AI-SERVICE] üéØ Using contextual AI analysis with custom prompt")
            logger.info(f"[AI-SERVICE] Customer text length: {len(customer_text)} characters")
            logger.info(f"[AI-SERVICE] Business info parts: {len(business_info_parts)}")
            
            return contextual_prompt
        
        # üîç –ë–ï–ó CUSTOM INSTRUCTIONS - –∞–ª–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î Vector Search context
        customer_text = context.get('original_customer_text', '')
        customer_name = context.get('customer_name', 'there')
        business_name = context.get('business_name', 'our business')
        
        # –Ø–∫—â–æ —î Vector Search context - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ –Ω–∞–≤—ñ—Ç—å –±–µ–∑ custom prompt
        if context.get('sample_replies_context'):
            logger.info(f"[AI-SERVICE] üîç No custom instructions, but using Vector Search context for prompt")
            
            vector_context = ""
            similar_chunks = context['sample_replies_context']
            
            vector_parts = []
            for i, chunk in enumerate(similar_chunks[:3]):  # Top 3 most similar
                similarity_score = chunk['similarity_score']
                chunk_type = chunk['chunk_type']
                content = chunk['content']
                
                vector_parts.append(
                    f"Example {i+1} (similarity: {similarity_score:.2f}):\n{content}\n"
                )
            
            vector_context = f"""
RELEVANT SAMPLE REPLIES:
{chr(10).join(vector_parts)}

INSTRUCTIONS: Generate a professional response in the style of the examples above. Use the tone, approach, and communication patterns shown in the similar examples.
"""
            
            basic_prompt = f"""Customer message:
"{customer_text}"

Customer name: {customer_name}
Business name: {business_name}
{vector_context}

Generate a personalized, professional response to the customer using the style guidance from the examples above."""
            
            logger.info(f"[AI-SERVICE] ‚úÖ Using Vector Search context without custom prompt")
            return basic_prompt
        
        # –Ø–∫—â–æ –Ω–µ–º–∞—î –Ω—ñ custom prompt, –Ω—ñ vector context
        logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è No custom instructions and no vector context - returning empty prompt")
        return ""
    
    def _fallback_message(self, context: Dict[str, Any]) -> str:
        """Fallback –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —è–∫—â–æ AI –Ω–µ –ø—Ä–∞—Ü—é—î"""
        name = context.get('customer_name', '')
        services = context.get('services', 'our services')
        business = context.get('business_name', 'our business')
        
        if name and name != 'there':
            return f"Hello {name}! Thank you for your inquiry about {services}. We'll get back to you soon!"
        else:
            return f"Hello! Thank you for your inquiry about {services}. We'll get back to you soon!"
    
    def _parse_lead_data(self, lead_detail: LeadDetail, custom_prompt: Optional[str] = None) -> Dict[str, str]:
        """ü§ñ AI-powered –ø–∞—Ä—Å–∏–Ω–≥ —â–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—Ç—è–≥—É—î –±—É–¥—å-—è–∫—ñ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∏ –∑ custom prompt"""
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç –∑ additional_info –∞–±–æ –∑ job_names
        text = self._get_lead_text(lead_detail)
        
        if not text:
            logger.info(f"[AI-SERVICE] No text found in lead data")
            return {}
        
        logger.info(f"[AI-SERVICE] AI-powered parsing from text: {text[:100]}...")
        
        # –Ø–∫—â–æ –Ω–µ–º–∞—î custom prompt - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ fallback –ø–∞—Ä—Å–∏–Ω–≥
        if not custom_prompt:
            return self._fallback_parsing(text)
        
        # –í–∏—Ç—è–≥—É—î–º–æ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∏ –∑ custom prompt
        placeholders = re.findall(r'\{(\w+)\}', custom_prompt)
        
        if not placeholders:
            logger.info(f"[AI-SERVICE] No placeholders found in custom prompt")
            return self._fallback_parsing(text)
        
        logger.info(f"[AI-SERVICE] Found placeholders: {placeholders}")
        
        # –ü—Ä–æ–±—É—î–º–æ AI extraction
        try:
            return self._ai_extract_fields(text, placeholders)
        except Exception as e:
            logger.error(f"[AI-SERVICE] AI extraction failed: {e}")
            return self._fallback_parsing(text)
    
    def _get_lead_text(self, lead_detail: LeadDetail) -> str:
        """üìù –û—Ç—Ä–∏–º—É—î —Ç–µ–∫—Å—Ç –∑ –ø–µ—Ä—à–æ–≥–æ TEXT —ñ–≤–µ–Ω—Ç—É –≤—ñ–¥ CONSUMER –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É"""
        from .models import LeadEvent
        
        # üéØ –ü–†–Ü–û–†–ò–¢–ï–¢: –®—É–∫–∞—î–º–æ –ø–µ—Ä—à–∏–π TEXT —ñ–≤–µ–Ω—Ç –≤—ñ–¥ CONSUMER
        first_consumer_text = LeadEvent.objects.filter(
            lead_id=lead_detail.lead_id,
            event_type="TEXT",
            user_type="CONSUMER",
            from_backend=False  # –ù–µ –Ω–∞—à–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        ).order_by('time_created').first()
        
        if first_consumer_text and first_consumer_text.text:
            logger.info(f"[AI-SERVICE] üìù Using TEXT event from CONSUMER: {first_consumer_text.text[:100]}...")
            return first_consumer_text.text
        
        # üîÑ FALLBACK 1: project.additional_info
        project_data = lead_detail.project or {}
        additional_info = project_data.get("additional_info", "")
        
        if additional_info:
            logger.info(f"[AI-SERVICE] üîÑ Fallback to project.additional_info: {additional_info[:100]}...")
            return additional_info
        
        # üîÑ FALLBACK 2: project.job_names
        job_names = project_data.get("job_names", [])
        if job_names:
            text = " ".join(job_names)
            logger.info(f"[AI-SERVICE] üîÑ Fallback to project.job_names: {text}")
            return text
        
        logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è No text found for lead {lead_detail.lead_id}")
        return ""
    
    def _ai_extract_fields(self, text: str, placeholders: list) -> Dict[str, str]:
        """–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î AI –¥–ª—è –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø–æ–ª—ñ–≤ –∑ —Ç–µ–∫—Å—Ç—É"""
        
        # –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–∞—Ö–∞—Ä–¥–∫–æ–¥–∂–µ–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏ –¥–ª—è extraction
        extraction_prompt = f"Customer message: {text}"
        
        logger.info(f"[AI-SERVICE] Sending extraction prompt to AI...")
        
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±—ñ–ª—å—à –¥–µ—à–µ–≤—É –º–æ–¥–µ–ª—å –¥–ª—è extraction
            extraction_model = "gpt-4o-mini"
            system_prompt = ""  # –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–∞—Ö–∞—Ä–¥–∫–æ–¥–∂–µ–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ
            messages = self._prepare_messages_for_model(extraction_model, system_prompt, extraction_prompt)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å –º–æ–¥–µ–ª—ñ
            api_params = self._get_api_params_for_model(extraction_model, messages, 200, 0.1)
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–π –∂–µ OpenAI –∫–ª—ñ—î–Ω—Ç
            response = self.client.chat.completions.create(**api_params)
            
            ai_response = response.choices[0].message.content.strip()
            logger.info(f"[AI-SERVICE] AI extraction response: {ai_response}")
            
            # –û—á–∏—â–∞—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –º–æ–∂–ª–∏–≤–æ–≥–æ markdown
            if ai_response.startswith('```'):
                ai_response = ai_response.split('\n', 1)[1].rsplit('\n', 1)[0]
            
            # –ü–∞—Ä—Å–∏–º–æ JSON
            result = json.loads(ai_response)
            
            # –í–∞–ª—ñ–¥—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if isinstance(result, dict):
                # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–∞–ø–∏—Ç—É–≤–∞–Ω—ñ –ø–æ–ª—è
                filtered_result = {key: str(value) for key, value in result.items() if key in placeholders}
                logger.info(f"[AI-SERVICE] ‚úÖ AI extraction successful: {filtered_result}")
                return filtered_result
            else:
                logger.warning(f"[AI-SERVICE] AI returned non-dict result: {result}")
                return {}
                
        except json.JSONDecodeError as e:
            logger.error(f"[AI-SERVICE] Failed to parse AI JSON response: {e}")
            return {}
        except Exception as e:
            logger.error(f"[AI-SERVICE] AI extraction error: {e}")
            return {}
    
    def _fallback_parsing(self, text: str) -> Dict[str, str]:
        """Fallback –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –±–∞–∑–æ–≤–∏—Ö –ø–æ–ª—ñ–≤ (contracting –±—ñ–∑–Ω–µ—Å)"""
        
        result = {}
        text_lower = text.lower()
        
        # –ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ service_type
        if "structural repair" in text_lower or "structure repair" in text_lower:
            result["service_type"] = "Structural repair"
        elif "remodeling" in text_lower or "remodel" in text_lower:
            result["service_type"] = "Remodeling"
        elif "addition" in text_lower:
            result["service_type"] = "Additions to an existing structure"
        elif "new structure" in text_lower or "new construction" in text_lower:
            result["service_type"] = "New structure construction"
        elif "design" in text_lower:
            result["service_type"] = "Construction design services"
        
        # –ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ sub_option –¥–ª—è structural repair
        if result.get("service_type") == "Structural repair":
            if "foundation" in text_lower:
                result["sub_option"] = "Foundation"
            elif "roof" in text_lower:
                result["sub_option"] = "Roof frame"
            elif "wall" in text_lower:
                result["sub_option"] = "Walls"
        
        # –ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ timeline
        if "as soon as possible" in text_lower or "asap" in text_lower:
            result["timeline"] = "As soon as possible"
        elif "flexible" in text_lower:
            result["timeline"] = "I'm flexible"
        
        # –ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ ZIP
        zip_match = re.search(r'\b\d{5}\b', text)
        if zip_match:
            result["zip"] = zip_match.group()
        
        logger.info(f"[AI-SERVICE] üîÑ Fallback parsing result: {result}")
        return result 

    def generate_sample_replies_response(
        self, 
        lead_detail: LeadDetail, 
        business: Optional[YelpBusiness] = None,
        max_length: Optional[int] = None,
        business_ai_settings: Optional['AutoResponseSettings'] = None,
        use_vector_search: bool = True
    ) -> Optional[str]:
        """üîç –†–µ–∂–∏–º 2: –ì–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ—à—É–∫—É Sample Replies (—Ç—ñ–ª—å–∫–∏ –¥–ª—è AI Generated)"""
        
        if not business:
            logger.warning("[AI-SERVICE] No business provided for Mode 2 vector search")
            return None
        
        logger.info(f"[AI-SERVICE] ========== MODE 2: VECTOR SAMPLE REPLIES AI GENERATION ==========")
        logger.info(f"[AI-SERVICE] Lead ID: {lead_detail.lead_id}")
        logger.info(f"[AI-SERVICE] Business: {business.name} ({business.business_id})")
        logger.info(f"[AI-SERVICE] Use vector search: {use_vector_search}")
        
        if not self.is_available():
            logger.error("[AI-SERVICE] ‚ùå OpenAI client not available")
            return None
        
        if not self.check_rate_limit():
            logger.warning("[AI-SERVICE] ‚ö†Ô∏è Rate limit exceeded, skipping Sample Replies AI generation")
            return None
        
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç –ª—ñ–¥–∞ –¥–ª—è –ø–æ—à—É–∫—É
            lead_inquiry = self._get_lead_text(lead_detail)
            
            if not lead_inquiry:
                logger.warning("[AI-SERVICE] No lead inquiry text found")
                return None
            
            logger.info(f"[AI-SERVICE] Lead inquiry: {lead_inquiry[:200]}...")
            
            customer_name = lead_detail.user_display_name or "there"
            response_length = max_length or 160
            
            if use_vector_search:
                # üîç –í–ï–ö–¢–û–†–ù–ò–ô –ü–û–®–£–ö: –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂—ñ —á–∞–Ω–∫–∏
                try:
                    from .vector_search_service import vector_search_service
                    
                    # üéØ –ù–û–í–ò–ô –ü–Ü–î–•–Ü–î: Inquiry‚ÜíResponse pair matching
                    inquiry_response_pairs = vector_search_service.search_inquiry_response_pairs(
                        query_text=lead_inquiry,
                        business_id=business.business_id,
                        location_id=None,  # TODO: Add location support if needed
                        limit=business_ai_settings.vector_search_limit if business_ai_settings else 5,
                        similarity_threshold=business_ai_settings.vector_similarity_threshold if business_ai_settings else 0.6
                    )
                    
                    if not inquiry_response_pairs:
                        logger.warning("[AI-SERVICE] No similar inquiry‚Üíresponse pairs found via vector search")
                        return None
                    
                    logger.info(f"[AI-SERVICE] Found {len(inquiry_response_pairs)} inquiry‚Üíresponse pairs via vector search")
                    for i, pair in enumerate(inquiry_response_pairs[:3]):
                        logger.info(f"[AI-SERVICE] Pair {i+1}: similarity={pair['pair_similarity']:.3f}, quality={pair['pair_quality']}")
                        logger.info(f"[AI-SERVICE]   Inquiry: {pair['inquiry']['content'][:80]}...")
                        logger.info(f"[AI-SERVICE]   Response: {pair['response']['content'][:80]}...")
                    
                    # ü§ñ –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ö–û–ù–¢–ï–ö–°–¢–£–ê–õ–¨–ù–û–á –í–Ü–î–ü–û–í–Ü–î–Ü –ó –ü–ê–†
                    contextual_response = vector_search_service.generate_contextual_response_from_pairs(
                        lead_inquiry=lead_inquiry,
                        customer_name=customer_name,
                        inquiry_response_pairs=inquiry_response_pairs,
                        business_name=business.name,
                        max_response_length=response_length
                    )
                    
                    if contextual_response:
                        logger.info(f"[AI-SERVICE] üéâ MODE 2 VECTOR: Generated contextual response:")
                        logger.info(f"[AI-SERVICE] - Length: {len(contextual_response)} chars")
                        logger.info(f"[AI-SERVICE] - Response: '{contextual_response}'")
                        logger.info(f"[AI-SERVICE] - Based on {len(inquiry_response_pairs)} inquiry‚Üíresponse pairs")
                        logger.info(f"[AI-SERVICE] ==============================================")
                        
                        return contextual_response
                    else:
                        logger.warning("[AI-SERVICE] Contextual response generation failed")
                        return None
                        
                except Exception as vector_error:
                    logger.error(f"[AI-SERVICE] Vector search failed: {vector_error}")
                    logger.warning("[AI-SERVICE] Falling back to legacy Sample Replies method...")
                    # Fallback to legacy method if vector search fails
                    use_vector_search = False
            
            # üìÑ LEGACY FALLBACK: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞—Ä–∏–π –º–µ—Ç–æ–¥ –∑ –ø–æ–≤–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º
            if not use_vector_search and business_ai_settings and business_ai_settings.sample_replies_content:
                logger.info("[AI-SERVICE] Using legacy Sample Replies method as fallback...")
                return self._generate_legacy_sample_replies_response(
                    lead_detail, business, business_ai_settings.sample_replies_content, 
                    response_length, business_ai_settings
                )
            
            logger.warning("[AI-SERVICE] No Sample Replies available for Mode 2")
            return None
            
        except Exception as e:
            logger.error(f"[AI-SERVICE] ‚ùå Error in Sample Replies generation (Mode 2): {e}")
            logger.exception("Sample replies generation error details")
            return None
    
    def _generate_legacy_sample_replies_response(
        self,
        lead_detail: LeadDetail,
        business: YelpBusiness,
        sample_replies_content: str,
        max_length: int,
        business_ai_settings: 'AutoResponseSettings'
    ) -> Optional[str]:
        """Legacy –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑ –ø–æ–≤–Ω–∏–º Sample Replies —Ç–µ–∫—Å—Ç–æ–º (fallback)"""
        
        try:
            lead_inquiry = self._get_lead_text(lead_detail)
            customer_name = lead_detail.user_display_name or "there"
            
            # –û—Ç—Ä–∏–º—É—î–º–æ AI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
            ai_config = self._get_ai_settings(business_ai_settings)
            model = ai_config['model']
            temperature = ai_config['temperature']
            
            # –°–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è legacy —Ä–µ–∂–∏–º—É
            system_prompt = f"""You are a professional business communication assistant for {business.name}.

MODE 2: AI GENERATED - LEGACY SAMPLE REPLIES METHOD

TASK: Generate a personalized response using the full sample replies content as training data.

SAMPLE REPLIES TRAINING DATA:
{sample_replies_content}

INSTRUCTIONS:
1. Study the sample replies to understand communication style and approach
2. Generate a NEW response matching this learned style for the current inquiry
3. Keep under {max_length} characters
4. Be professional and personalized"""
            
            user_prompt = f"""Customer: {customer_name}
Inquiry: "{lead_inquiry}"

Generate a response in the style of the sample replies provided."""
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–∞ –≤–∏–∫–ª–∏–∫ API
            messages = self._prepare_messages_for_model(model, system_prompt, user_prompt)
            api_params = self._get_api_params_for_model(model, messages, max_length, temperature)
            
            response = self.client.chat.completions.create(**api_params)
            ai_message = response.choices[0].message.content.strip()
            
            logger.info(f"[AI-SERVICE] üìÑ Legacy Sample Replies response: {ai_message}")
            return ai_message
            
        except Exception as e:
            logger.error(f"[AI-SERVICE] Legacy Sample Replies generation failed: {e}")
            return None