import openai
import logging
import time
import json
import re
from typing import Optional, Dict, Any
from django.conf import settings
from django.utils import timezone
from django.core.cache import cache
from .models import LeadDetail, YelpBusiness, AISettings, AutoResponseSettings
import os

logger = logging.getLogger(__name__)

class OpenAIService:
    """–°–µ—Ä–≤—ñ—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ OpenAI API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å"""
    
    def __init__(self):
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI –∫–ª—ñ—î–Ω—Ç–∞ –∑ –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å"""
        try:
            # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ API –∫–ª—é—á –∑ .env —Ñ–∞–π–ª–∞
            openai_api_key = os.getenv('OPENAI_API_KEY')
            
            if not openai_api_key:
                # –Ø–∫—â–æ –Ω–µ–º–∞—î –≤ .env, –ø—Ä–æ–±—É—î–º–æ –∑ AISettings –º–æ–¥–µ–ª—ñ
                ai_settings = AISettings.objects.first()
                if ai_settings and ai_settings.openai_api_key:
                    openai_api_key = ai_settings.openai_api_key
            
            if openai_api_key:
                self.client = openai.OpenAI(
                    api_key=openai_api_key
                )
                logger.info("[AI-SERVICE] OpenAI client initialized successfully")
            else:
                logger.error("[AI-SERVICE] No OpenAI API key found in environment variables or database")
                self.client = None
                
        except Exception as e:
            logger.error(f"[AI-SERVICE] Failed to initialize OpenAI client: {e}")
            self.client = None
    
    def is_available(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –¥–æ—Å—Ç—É–ø–Ω–∏–π AI —Å–µ—Ä–≤—ñ—Å"""
        return self.client is not None
    
    def _get_ai_settings(self, business_ai_settings: Optional['AutoResponseSettings'] = None) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º—É—î AI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑ business-specific fallback –Ω–∞ global"""
        # –û—Ç—Ä–∏–º—É—î–º–æ –≥–ª–æ–±–∞–ª—å–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —è–∫ fallback
        global_ai_settings = AISettings.objects.first()
        
        # –ú–æ–¥–µ–ª—å
        if business_ai_settings and business_ai_settings.ai_model:
            model = business_ai_settings.ai_model
            logger.info(f"[AI-SERVICE] Using business-specific model: {model}")
        else:
            model = global_ai_settings.openai_model if global_ai_settings else "gpt-4o"
            logger.info(f"[AI-SERVICE] Using fallback model: {model}")
        
        # Temperature
        if business_ai_settings and business_ai_settings.ai_temperature is not None:
            temperature = business_ai_settings.ai_temperature
            logger.info(f"[AI-SERVICE] Using business-specific temperature: {temperature}")
        else:
            temperature = global_ai_settings.default_temperature if global_ai_settings else 0.7
            logger.info(f"[AI-SERVICE] Using fallback temperature: {temperature}")
        
        # Max message length
        if business_ai_settings and business_ai_settings.ai_max_message_length > 0:
            max_length = business_ai_settings.ai_max_message_length
            logger.info(f"[AI-SERVICE] Using business-specific max length: {max_length}")
        else:
            max_length = global_ai_settings.max_message_length if global_ai_settings else 160
            logger.info(f"[AI-SERVICE] Using fallback max length: {max_length}")
        
        return {
            'model': model,
            'temperature': temperature,
            'max_length': max_length,
            'global_settings': global_ai_settings
        }
    
    def check_rate_limit(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î rate limit –¥–ª—è AI –∑–∞–ø–∏—Ç—ñ–≤"""
        ai_settings = AISettings.objects.first()
        if not ai_settings:
            return True
        
        cache_key = "ai_requests_count"
        current_count = cache.get(cache_key, 0)
        
        if current_count >= ai_settings.requests_per_minute:
            logger.warning(f"[AI-SERVICE] Rate limit exceeded: {current_count}/{ai_settings.requests_per_minute}")
            return False
        
        # –ó–±—ñ–ª—å—à—É—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫ –Ω–∞ 1 —Ö–≤–∏–ª–∏–Ω—É
        cache.set(cache_key, current_count + 1, 60)
        return True
    
    def generate_greeting_message(
        self, 
        lead_detail: LeadDetail, 
        business: Optional[YelpBusiness] = None,
        is_off_hours: bool = False,
        # All AI behavior controlled via Custom Instructions (location, response time, business data, style)
        custom_prompt: Optional[str] = None,
        business_data_settings: Optional[Dict[str, bool]] = None,
        max_length: Optional[int] = None,
        business_ai_settings: Optional['AutoResponseSettings'] = None
    ) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä—É—î AI-powered greeting –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ª—ñ–¥–∞"""
        
        logger.info(f"[AI-SERVICE] ============= AI GREETING GENERATION =============")
        logger.info(f"[AI-SERVICE] Starting AI greeting generation")
        logger.info(f"[AI-SERVICE] Input parameters:")
        logger.info(f"[AI-SERVICE] - Lead ID: {lead_detail.lead_id}")
        logger.info(f"[AI-SERVICE] - Customer name: {lead_detail.user_display_name}")
        logger.info(f"[AI-SERVICE] - Business: {business.name if business else 'None'}")
        logger.info(f"[AI-SERVICE] - is_off_hours: {is_off_hours}")
        # All AI behavior controlled via Custom Instructions
        logger.info(f"[AI-SERVICE] - custom_prompt provided: {custom_prompt is not None}")
        logger.info(f"[AI-SERVICE] - business_data_settings: {business_data_settings}")
        
        if not self.is_available():
            logger.error("[AI-SERVICE] ‚ùå OpenAI client not available")
            return None
        
        logger.info(f"[AI-SERVICE] ‚úÖ OpenAI client is available")
        
        if not self.check_rate_limit():
            logger.warning("[AI-SERVICE] ‚ö†Ô∏è Rate limit exceeded, skipping AI generation")
            return None
        
        logger.info(f"[AI-SERVICE] ‚úÖ Rate limit check passed")
        
        try:
            logger.info(f"[AI-SERVICE] üîÑ Preparing lead context...")
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è AI
            context = self._prepare_lead_context(
                lead_detail, business, is_off_hours, 
                business_data_settings, custom_prompt
            )
            
            logger.info(f"[AI-SERVICE] ‚úÖ Lead context prepared:")
            logger.info(f"[AI-SERVICE] - Customer name: {context.get('customer_name')}")
            logger.info(f"[AI-SERVICE] - Services: {context.get('services')}")
            logger.info(f"[AI-SERVICE] - Business name: {context.get('business_name')}")
            logger.info(f"[AI-SERVICE] - Phone number: {context.get('phone_number', 'Not provided')}")
            logger.info(f"[AI-SERVICE] - Additional info: {context.get('additional_info', 'None')[:50]}...")
            
            logger.info(f"[AI-SERVICE] üîÑ Creating prompt...")
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–æ–º–ø—Ç–∞
            prompt = self._create_greeting_prompt(
                context, custom_prompt, max_length
            )
            
            logger.info(f"[AI-SERVICE] ‚úÖ Prompt created (length: {len(prompt)} characters)")
            logger.info(f"[AI-SERVICE] Prompt preview: {prompt[:200]}...")
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è AI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ business-specific fallback
            ai_config = self._get_ai_settings(business_ai_settings)
            model = ai_config['model']
            temperature = ai_config['temperature']
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä max_length —è–∫—â–æ –Ω–∞–¥–∞–Ω–∏–π, —ñ–Ω–∞–∫—à–µ business/global
            if max_length is not None and max_length > 0:
                message_length = max_length
                logger.info(f"[AI-SERVICE] Using parameter max length: {message_length}")
            else:
                message_length = ai_config['max_length']
                logger.info(f"[AI-SERVICE] Using configured max length: {message_length}")
            
            logger.info(f"[AI-SERVICE] ü§ñ Final AI configuration:")
            logger.info(f"[AI-SERVICE] - Model: {model}")
            logger.info(f"[AI-SERVICE] - Temperature: {temperature}")
            logger.info(f"[AI-SERVICE] - Max tokens: {message_length}")
            
            logger.info(f"[AI-SERVICE] ü§ñ Calling OpenAI API...")
            logger.info(f"[AI-SERVICE] ========== OPENAI API CALL ==========")
            logger.info(f"[AI-SERVICE] About to call OpenAI chat completion...")
            logger.info(f"[AI-SERVICE] API parameters:")
            logger.info(f"[AI-SERVICE] - Model: {model}")
            logger.info(f"[AI-SERVICE] - Temperature: {temperature}")
            logger.info(f"[AI-SERVICE] - Max tokens: {message_length}")
            
            # üéØ –î–ª—è contextual AI analysis –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ custom prompt —è–∫ system prompt
            system_prompt = self._get_system_prompt(custom_prompt)
            logger.info(f"[AI-SERVICE] - System prompt length: {len(system_prompt)} characters")
            logger.info(f"[AI-SERVICE] - User prompt length: {len(prompt)} characters")
            
            logger.info(f"[AI-SERVICE] System prompt preview: {system_prompt[:200]}..." + ("" if len(system_prompt) <= 200 else " (truncated)"))
            logger.info(f"[AI-SERVICE] User prompt preview: {prompt[:200]}..." + ("" if len(prompt) <= 200 else " (truncated)"))
            logger.info(f"[AI-SERVICE] üöÄ Making OpenAI API request...")
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ
            messages = self._prepare_messages_for_model(model, system_prompt, prompt)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å –º–æ–¥–µ–ª—ñ
            api_params = self._get_api_params_for_model(model, messages, message_length, temperature)
            
            # –í–∏–∫–ª–∏–∫ OpenAI API
            response = self.client.chat.completions.create(**api_params)
            logger.info(f"[AI-SERVICE] ‚úÖ OpenAI API call completed successfully")
            logger.info(f"[AI-SERVICE] ‚úÖ OpenAI API responded successfully")
            
            ai_message = response.choices[0].message.content.strip()
            original_length = len(ai_message)
            
            logger.info(f"[AI-SERVICE] Raw AI response:")
            logger.info(f"[AI-SERVICE] - Original message: '{ai_message}'")
            logger.info(f"[AI-SERVICE] - Original length: {original_length} characters")
            
            # OpenAI –≤–∂–µ –æ–±–º–µ–∂–∏–≤ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —á–µ—Ä–µ–∑ max_tokens, –¥–æ–¥–∞—Ç–∫–æ–≤–µ –æ–±—Ä—ñ–∑–∞–Ω–Ω—è –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ
            logger.info(f"[AI-SERVICE] ‚úÖ Message generated within token limit")
            
            logger.info(f"[AI-SERVICE] üéâ FINAL AI GREETING:")
            logger.info(f"[AI-SERVICE] - Final message: '{ai_message}'")
            logger.info(f"[AI-SERVICE] - Final length: {len(ai_message)} characters")
            logger.info(f"[AI-SERVICE] Generated greeting for lead {lead_detail.lead_id}: {ai_message[:50]}...")
            logger.info(f"[AI-SERVICE] ==============================================")
            
            return ai_message
            
        except Exception as e:
            logger.error(f"[AI-SERVICE] ‚ùå Error generating AI greeting: {e}")
            logger.exception(f"[AI-SERVICE] AI generation exception details")
            logger.info(f"[AI-SERVICE] ==============================================")
            return None
    
    def generate_preview_message(
        self,
        business,  # YelpBusiness object
        # All AI behavior controlled via Custom Instructions (location, response time, business data, style)
        custom_prompt: Optional[str] = None,
        max_length: Optional[int] = None,
        custom_preview_text: Optional[str] = None,  # üéØ –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        business_ai_settings: Optional['AutoResponseSettings'] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä—É—î –ø—Ä–µ–≤ º—é –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å"""
        
        if not self.is_available():
            return "AI service not available. Please check your settings."
        
        # üéØ –ù–û–í–ê –°–ü–†–û–©–ï–ù–ê –ê–†–•–Ü–¢–ï–ö–¢–£–†–ê AI –°–ò–°–¢–ï–ú–ò
        logger.info(f"[AI-SERVICE] üéØ SIMPLIFIED AI: Vector Search ‚Üí Custom Instructions")
        logger.info(f"[AI-SERVICE] Business: {business.name}")
        logger.info(f"[AI-SERVICE] Has custom_prompt: {bool(custom_prompt)}")
        logger.info(f"[AI-SERVICE] Has custom_preview_text: {bool(custom_preview_text)}")
        
        try:
            # üéØ STEP 1: –°–ü–†–û–ë–£–Ñ–ú–û VECTOR SEARCH (—è–∫—â–æ —î Sample Replies + custom preview text)
            if custom_preview_text and business_ai_settings and business_ai_settings.use_sample_replies:
                logger.info(f"[AI-SERVICE] üîç STEP 1: Trying Vector Search (Sample Replies)")
                logger.info(f"[AI-SERVICE] Custom preview text: {custom_preview_text[:100]}...")
                
                try:
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Vector Search approach
                    from .models import LeadDetail
                    mock_lead = LeadDetail(
                        lead_id="preview_test",
                        user_display_name="Test Customer",
                        business_id=business.business_id
                    )
                    mock_lead.project = {"additional_info": custom_preview_text}
                    
                    vector_response = self.generate_sample_replies_response(
                        lead_detail=mock_lead,
                        business=business,
                        max_length=max_length or 160,
                        business_ai_settings=business_ai_settings,
                        use_vector_search=True
                    )
                    
                    if vector_response:
                        logger.info(f"[AI-SERVICE] üéâ STEP 1 SUCCESS: Vector Search generated response")
                        return vector_response
                    else:
                        logger.info(f"[AI-SERVICE] üìã STEP 1 FAILED: Vector Search found no results, trying Custom Instructions")
                        
                except Exception as vector_error:
                    logger.warning(f"[AI-SERVICE] ‚ùå STEP 1 ERROR: Vector Search failed: {vector_error}")
                    logger.info(f"[AI-SERVICE] üîÑ STEP 1 FALLBACK: Using Custom Instructions")
            
            # üéØ STEP 2: CUSTOM INSTRUCTIONS (Primary prompt approach)
            if not custom_prompt:
                logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è STEP 2: No Custom Instructions provided - using minimal prompt")
                return "Please provide Custom Instructions to generate personalized responses. Custom Instructions should specify what business information to include and how to respond to customers."
            
            logger.info(f"[AI-SERVICE] üìù STEP 2: Using Custom Instructions as primary prompt")
            logger.info(f"[AI-SERVICE] Custom prompt length: {len(custom_prompt)} characters")
            
            # üè¢ –ü–Ü–î–ì–û–¢–û–í–ö–ê BUSINESS CONTEXT (–≤—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è Custom Instructions)
            business_context = {
                "name": business.name,
                "location": business.location or "",
                "time_zone": business.time_zone or "",
                "open_days": business.open_days or "",
                "open_hours": business.open_hours or ""
            }
            
            # –î–æ–¥–∞—î–º–æ –≤—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ business.details (Custom Instructions —Å–∞–º –≤–∏–±–µ—Ä–µ —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏)
            if business.details and isinstance(business.details, dict):
                logger.info(f"[AI-SERVICE] üìã Business details available for Custom Instructions")
                business_context.update({
                    "rating": business.details.get("rating"),
                    "review_count": business.details.get("review_count"),
                    "categories": business.details.get("categories", []),
                    "phone": business.details.get("display_phone") or business.details.get("phone"),
                    "website": business.details.get("url"),
                    "price": business.details.get("price"),
                    "address": business.details.get("location", {}).get("display_address", []),
                    "city": business.details.get("location", {}).get("city"),
                    "state": business.details.get("location", {}).get("state"),
                    "zip_code": business.details.get("location", {}).get("zip_code"),
                    "transactions": business.details.get("transactions", [])
                })
                
                # –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è Custom Instructions
                available_data = []
                for key, value in business_context.items():
                    if value and key != "name":
                        available_data.append(key)
                logger.info(f"[AI-SERVICE] üìä Available business data for Custom Instructions: {', '.join(available_data)}")
            else:
                logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è No business details available")
            
            # üéØ –°–ü–†–û–©–ï–ù–ò–ô CONTEXT –î–õ–Ø CUSTOM INSTRUCTIONS
            customer_text = custom_preview_text or "I need help with your services"
            customer_name = "[Client Name]" 
            business_name = business.name
            
            # –§–æ—Ä–º–∞—Ç—É—î–º–æ –≤—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ business data –¥–ª—è Custom Instructions
            business_info_parts = []
            
            if business_context.get("location"):
                business_info_parts.append(f"Location: {business_context['location']}")
            
            if business_context.get("rating"):
                rating_str = f"Rating: {business_context['rating']}/5 stars"
                if business_context.get("review_count"):
                    rating_str += f" ({business_context['review_count']} reviews)"
                business_info_parts.append(rating_str)
            
            if business_context.get("categories"):
                categories = business_context["categories"]
                if categories:
                    category_titles = [cat.get('title', str(cat)) if isinstance(cat, dict) else str(cat) for cat in categories[:3]]
                    business_info_parts.append(f"Specializes in: {', '.join(category_titles)}")
            
            if business_context.get("phone"):
                business_info_parts.append(f"Phone: {business_context['phone']}")
            
            if business_context.get("price"):
                business_info_parts.append(f"Price range: {business_context['price']}")
            
            if business_context.get("website"):
                business_info_parts.append(f"Website: {business_context['website']}")
            
            business_info = "\n".join(business_info_parts) if business_info_parts else "Basic business information available."
            
            logger.info(f"[AI-SERVICE] üìã Formatted business info: {len(business_info_parts)} items")
            logger.info(f"[AI-SERVICE] üí¨ Customer text: {customer_text[:50]}...")
            
            # üéØ –°–¢–í–û–†–Æ–Ñ–ú–û –°–ü–†–û–©–ï–ù–ò–ô PROMPT –ó CUSTOM INSTRUCTIONS + BUSINESS DATA
            # Length instruction
            length_instruction = ""
            if max_length and max_length > 0:
                if max_length <= 160:
                    length_instruction = f"\n\nIMPORTANT: Keep response under {max_length} characters. Be concise but complete."
                elif max_length <= 250:
                    length_instruction = f"\n\nIMPORTANT: Keep response under {max_length} characters."
                else:
                    length_instruction = f"\n\nKeep response under {max_length} characters."
            
            # üéØ –°–ü–†–û–©–ï–ù–ò–ô PROMPT –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ó CUSTOM INSTRUCTIONS
            simplified_prompt = f"""Customer message:
"{customer_text}"

Customer name: {customer_name}
Business name: {business_name}

Available Business Information:
{business_info}{length_instruction}

SYSTEM INSTRUCTIONS:
{custom_prompt}

Based on your system instructions above, generate a personalized response to the customer. Use any relevant business information that fits your instructions. Be helpful and professional."""
            
            logger.info(f"[AI-SERVICE] üìù Generated simplified prompt (length: {len(simplified_prompt)} chars)")
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ simplified prompt –∑–∞–º—ñ—Å—Ç—å —Å–∫–ª–∞–¥–Ω–æ–≥–æ _create_greeting_prompt
            prompt = simplified_prompt
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è AI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ business-specific fallback –¥–ª—è preview
            ai_config = self._get_ai_settings(business_ai_settings)
            model = ai_config['model']
            temperature = ai_config['temperature']
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è debug
            logger.info(f"[AI-SERVICE] Selected model: {model}")
            logger.info(f"[AI-SERVICE] Temperature: {temperature}")
            
            # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–ª—è GPT-5 –º–æ–¥–µ–ª–µ–π
            if model.startswith('gpt-5'):
                logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è Using GPT-5 model: {model}")
                logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è Note: GPT-5 models may not be available in all OpenAI accounts yet")
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä max_length —è–∫—â–æ –Ω–∞–¥–∞–Ω–∏–π, —ñ–Ω–∞–∫—à–µ business/global
            # üîß –ü–û–ö–†–ê–©–ï–ù–ê –ö–û–ù–í–ï–†–¢–ê–¶–Ü–Ø –°–ò–ú–í–û–õ–Ü–í ‚Üí –¢–û–ö–ï–ù–ò
            if max_length is not None and max_length > 0:
                # üîß –ü–û–ö–†–ê–©–ï–ù–ê –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è (–±—ñ–ª—å—à–µ —Ç–æ–∫–µ–Ω—ñ–≤):
                if max_length <= 160:
                    # –ö–æ—Ä–æ—Ç–∫—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: –¥–∞—î–º–æ –±—ñ–ª—å—à–µ —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
                    estimated_tokens = max(1, max_length // 3)  # 1 token ‚âà 3 chars (53 tokens –¥–ª—è 160)
                elif max_length <= 320:
                    # –°–µ—Ä–µ–¥–Ω—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è
                    estimated_tokens = max(1, max_length // 4)  # 1 token ‚âà 4 chars  
                else:
                    # –î–æ–≤–≥—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: –º–µ–Ω—à –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è
                    estimated_tokens = max(1, max_length // 3)  # 1 token ‚âà 3 chars
                    
                message_length = estimated_tokens
                logger.info(f"[AI-SERVICE] Smart conversion: {max_length} chars ‚Üí {estimated_tokens} tokens (ratio: 1:{max_length/estimated_tokens:.1f})")
            else:
                message_length = ai_config['max_length']
                logger.info(f"[AI-SERVICE] Preview using configured max length: {message_length} tokens")
            
            # üéØ –î–ª—è contextual AI analysis –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ custom prompt —è–∫ system prompt
            system_prompt = self._get_system_prompt(custom_prompt)
            
            # –î–ª—è GPT-5 –º–æ–¥–µ–ª–µ–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ–≤–Ω–∏–π –∫–∞—Å—Ç–æ–º–Ω–∏–π –ø—Ä–æ–º—Ç –±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å
            if model.startswith('gpt-5'):
                logger.info(f"[AI-SERVICE] GPT-5: Using full custom system prompt (length: {len(system_prompt)})")
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ
            messages = self._prepare_messages_for_model(model, system_prompt, prompt)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å –º–æ–¥–µ–ª—ñ
            api_params = self._get_api_params_for_model(model, messages, message_length, temperature)
            
            logger.info(f"[AI-SERVICE] Making API call with params: {api_params}")
            
            try:
                response = self.client.chat.completions.create(**api_params)
                
                logger.info(f"[AI-SERVICE] Response received successfully")
                logger.info(f"[AI-SERVICE] Response choices count: {len(response.choices)}")
                
                if response.choices and len(response.choices) > 0:
                    choice = response.choices[0]
                    # –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    logger.info(f"[AI-SERVICE] Finish reason: {choice.finish_reason}")
                    logger.info(f"[AI-SERVICE] Message role: {choice.message.role}")
                    logger.info(f"[AI-SERVICE] Tool calls: {choice.message.tool_calls}")
                    logger.info(f"[AI-SERVICE] Usage: {response.usage}")
                    
                    content = choice.message.content
                    logger.info(f"[AI-SERVICE] Response content length: {len(content) if content else 0}")
                    logger.info(f"[AI-SERVICE] Response content: '{content}'")
                    
                    if not content:
                        logger.warning(f"[AI-SERVICE] Empty content received from {model}")
                        return "Empty response received from AI model."
                    
                    return content.strip()
                else:
                    logger.error(f"[AI-SERVICE] No choices in response from {model}")
                    return "No response choices received from AI model."
                    
            except Exception as api_error:
                logger.error(f"[AI-SERVICE] API call failed for {model}: {api_error}")
                
                # Fallback –¥–ª—è GPT-5 –º–æ–¥–µ–ª–µ–π –¥–æ gpt-4o
                if model.startswith('gpt-5'):
                    logger.warning(f"[AI-SERVICE] üîÑ GPT-5 failed, trying fallback to gpt-4o")
                    try:
                        fallback_params = self._get_api_params_for_model('gpt-4o', messages, message_length, temperature)
                        logger.info(f"[AI-SERVICE] Making fallback API call with gpt-4o")
                        
                        fallback_response = self.client.chat.completions.create(**fallback_params)
                        content = fallback_response.choices[0].message.content
                        
                        if content:
                            logger.info(f"[AI-SERVICE] ‚úÖ Fallback successful with gpt-4o")
                            return content.strip()
                        else:
                            logger.error(f"[AI-SERVICE] Fallback also returned empty content")
                            return "Fallback model also returned empty response."
                            
                    except Exception as fallback_error:
                        logger.error(f"[AI-SERVICE] Fallback to gpt-4o also failed: {fallback_error}")
                        return f"Both {model} and fallback failed: {str(api_error)}"
                else:
                    # –î–ª—è —ñ–Ω—à–∏—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç–æ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ–º–∏–ª–∫—É
                    return f"API call failed: {str(api_error)}"
            
        except Exception as e:
            logger.error(f"[AI-SERVICE] Error generating preview: {e}")
            return f"Error generating preview: {str(e)}"

    def _prepare_messages_for_model(self, model: str, system_prompt: str, user_prompt: str) -> list:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        
        if model.startswith("o1"):
            # o1 –º–æ–¥–µ–ª—ñ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å system role
            # –ö–æ–º–±—ñ–Ω—É—î–º–æ system prompt –∑ user prompt
            combined_prompt = f"{system_prompt}\n\nUser request: {user_prompt}"
            logger.info(f"[AI-SERVICE] o1 model detected: combining system and user prompts")
            return [{"role": "user", "content": combined_prompt}]
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å system role
            return [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

    def _get_api_params_for_model(self, model: str, messages: list, max_tokens: int, temperature: float) -> dict:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å –º–æ–¥–µ–ª—ñ"""
        
        params = {
            "model": model,
            "messages": messages
        }
        
        if model.startswith("o1"):
            # o1 –º–æ–¥–µ–ª—ñ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å temperature —Ç–∞ max_tokens
            logger.info(f"[AI-SERVICE] o1 model: skipping temperature and max_tokens parameters")
        elif model.startswith("gpt-5"):
            # GPT-5 –º–æ–¥–µ–ª—ñ –º–∞—é—Ç—å –ø—Ä–æ–±–ª–µ–º–∏ –∑ temperature —ñ max_completion_tokens
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π max_tokens —ñ –Ω–µ –ø–µ—Ä–µ–¥–∞—î–º–æ temperature
            params["max_tokens"] = max_tokens
            logger.info(f"[AI-SERVICE] GPT-5 model: using max_tokens without temperature")
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å max_tokens
            params["max_tokens"] = max_tokens
            params["temperature"] = temperature
            logger.info(f"[AI-SERVICE] Standard model: using max_tokens and temperature")
        
        return params
    
    def _prepare_lead_context(
        self, 
        lead_detail: LeadDetail, 
        business: Optional[YelpBusiness],
        is_off_hours: bool,
        # All AI behavior controlled via Custom Instructions (location, response time, business data, style)
        business_data_settings: Optional[Dict[str, bool]] = None,
        custom_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ª—ñ–¥–∞ –¥–ª—è AI –∑ –ø–æ–≤–Ω–æ—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –±—ñ–∑–Ω–µ—Å"""
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        if not business_data_settings:
            business_data_settings = {
                "include_rating": True,
                "include_categories": True,
                "include_phone": True,
                "include_website": False,
                "include_price_range": True,
                "include_hours": True,
                "include_reviews_count": True,
                "include_address": False,
                "include_transactions": False
            }
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç –∫–ª—ñ—î–Ω—Ç–∞ –¥–ª—è contextual analysis
        original_customer_text = self._get_lead_text(lead_detail)
        
        # –ë–∞–∑–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∫–ª—ñ—î–Ω—Ç–∞
        context = {
            "customer_name": lead_detail.user_display_name or "there",
            "services": ", ".join(lead_detail.project.get("job_names", [])) if lead_detail.project else "",
            "phone_number": getattr(lead_detail, 'phone_number', None),
            "additional_info": getattr(lead_detail, 'additional_notes', '') or '',
            "created_at": lead_detail.created_at if hasattr(lead_detail, 'created_at') else timezone.now(),
            "is_off_hours": is_off_hours,
            # mention_response_time removed - controlled via Custom Instructions
            "original_customer_text": original_customer_text  # üéØ –î–ª—è contextual AI analysis
        }
        
        if not business:
            context.update({
                "business_name": "our business",
                "business_location": "",
                "business_data": {}
            })
            return context
        
        # –ü–û–í–ù–ê —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –±—ñ–∑–Ω–µ—Å –∑ YelpBusiness
        business_data = {
            "name": business.name,
            "location": business.location,
            "time_zone": business.time_zone,
            "open_days": business.open_days,
            "open_hours": business.open_hours,
            "business_id": business.business_id
        }
        
        # –î–æ–¥–∞—î–º–æ –¥–∞–Ω—ñ –∑ details JSON –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å
        if business.details:
            details = business.details
            
            # –†–µ–π—Ç–∏–Ω–≥ —Ç–∞ –≤—ñ–¥–≥—É–∫–∏
            if business_data_settings.get("include_rating") and details.get("rating"):
                business_data["rating"] = details.get("rating", 0)
                if business_data_settings.get("include_reviews_count"):
                    business_data["review_count"] = details.get("review_count", 0)
            
            # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó –±—ñ–∑–Ω–µ—Å—É
            if business_data_settings.get("include_categories") and details.get("categories"):
                business_data["categories"] = [cat.get("title", "") for cat in details.get("categories", [])]
                business_data["category_aliases"] = [cat.get("alias", "") for cat in details.get("categories", [])]
            
            # –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
            if business_data_settings.get("include_phone") and details.get("phone"):
                business_data["phone"] = details.get("phone", "")
            
            if business_data_settings.get("include_website") and details.get("url"):
                business_data["website"] = details.get("url", "")
            
            # –¶—ñ–Ω–æ–≤–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω
            if business_data_settings.get("include_price_range") and details.get("price"):
                business_data["price"] = details.get("price", "")
            
            # –ê–¥—Ä–µ—Å–∞
            if business_data_settings.get("include_address") and details.get("location"):
                location_data = details.get("location", {})
                business_data["address"] = location_data.get("display_address", [])
                business_data["city"] = location_data.get("city", "")
                business_data["state"] = location_data.get("state", "")
                business_data["zip_code"] = location_data.get("zip_code", "")
            
            # –†–æ–±–æ—á—ñ –≥–æ–¥–∏–Ω–∏
            if business_data_settings.get("include_hours") and details.get("hours"):
                business_data["hours_detailed"] = details.get("hours", [])
                business_data["is_open_now"] = details.get("hours", [{}])[0].get("is_open_now", False) if details.get("hours") else False
            
            # –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó —Ç–∞ –ø–æ—Å–ª—É–≥–∏
            if business_data_settings.get("include_transactions") and details.get("transactions"):
                business_data["transactions"] = details.get("transactions", [])
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –¥–∞–Ω—ñ (–∑–∞–≤–∂–¥–∏ –≤–∫–ª—é—á–∞—î–º–æ –¥–ª—è —Ä–æ–∑—à–∏—Ä–µ–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π)
            business_data.update({
                "coordinates": details.get("coordinates", {}),
                "image_url": details.get("image_url", ""),
                "is_claimed": details.get("is_claimed", False),
                "is_closed": details.get("is_closed", False),
                "yelp_url": details.get("url", "")
            })
        
        context.update({
            "business_name": business.name,
            "business_location": business.location if include_location else "",
            "business_data": business_data,
            "business_data_settings": business_data_settings
        })
        
        # üéØ CONTEXTUAL AI: –ë—ñ–ª—å—à–µ –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω –ø–∞—Ä—Å–∏–Ω–≥ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ñ–≤ 
        # AI —Å–∞–º –∞–Ω–∞–ª—ñ–∑—É—î original_customer_text –∑–≥—ñ–¥–Ω–æ –∑ custom prompt
        
        return context
    
    def _get_system_prompt(self, custom_prompt: Optional[str] = None) -> str:
        """–û—Ç—Ä–∏–º—É—î —Å–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç (—Ç—ñ–ª—å–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–∏–π)"""
        if custom_prompt:
            return custom_prompt
        
        ai_settings = AISettings.objects.first()
        if ai_settings and ai_settings.base_system_prompt:
            return ai_settings.base_system_prompt
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Ä—è–¥–æ–∫ –∑–∞–º—ñ—Å—Ç—å –∑–∞—Ö–∞—Ä–¥–∫–æ–¥–∂–µ–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        return ""
    
    def _create_greeting_prompt(
        self, 
        context: Dict[str, Any], 
        # response_style removed - AI learns style from PDF examples
        custom_prompt: Optional[str] = None,
        max_length: Optional[int] = None  # üîß –î–æ–¥–∞—î–º–æ max_length –¥–ª—è —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π –ø—Ä–æ –¥–æ–≤–∂–∏–Ω—É
    ) -> str:
        """–°—Ç–≤–æ—Ä—é—î –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑ –ø–æ–≤–Ω–∏–º–∏ –±—ñ–∑–Ω–µ—Å-–¥–∞–Ω–∏–º–∏"""
        
        # üéØ CONTEXTUAL AI ANALYSIS: –Ø–∫—â–æ —î custom prompt - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ —è–∫ system prompt
        if custom_prompt:
            # –û—Ç—Ä–∏–º—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç –∫–ª—ñ—î–Ω—Ç–∞
            customer_text = context.get('original_customer_text', '')
            customer_name = context.get('customer_name', 'there')
            business_name = context.get('business_name', 'our business')
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∏–π user prompt –¥–ª—è AI –∞–Ω–∞–ª—ñ–∑—É –∑ business data
            business_data = context.get('business_data', {})
            business_data_settings = context.get('business_data_settings', {})
            
            # –§–æ—Ä–º—É—î–º–æ business information –¥–ª—è contextual prompt
            business_info_parts = []
            
            # Address information
            if business_data_settings.get('include_address'):
                address_parts = []
                city = business_data.get('city', '')
                state = business_data.get('state', '')
                zip_code = business_data.get('zip_code', '')
                
                # –Ø–∫—â–æ —î –æ–∫—Ä–µ–º—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∞–¥—Ä–µ—Å–∏
                if city and state and zip_code:
                    address_parts.append(f"{city}, {state} {zip_code}")
                elif city and state:
                    address_parts.append(f"{city}, {state}")
                elif city:
                    address_parts.append(city)
                
                # –Ø–∫—â–æ —î –¥–æ–¥–∞—Ç–∫–æ–≤–∞ –∞–¥—Ä–µ—Å–∞ –∑ Yelp API (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≤—É–ª–∏—Ü—è)
                if business_data.get('address'):
                    yelp_address = ", ".join(str(addr) for addr in business_data['address']) if isinstance(business_data['address'], list) else str(business_data['address'])
                    # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —Ü–µ –Ω–µ –¥—É–±–ª—é—î city
                    if yelp_address and yelp_address.lower() != city.lower():
                        address_parts.insert(0, yelp_address)  # –í—É–ª–∏—Ü—è —Å–ø–µ—Ä–µ–¥—É
                
                if address_parts:
                    full_address = ", ".join(address_parts)
                    business_info_parts.append(f"Address: {full_address}")
            
            # Phone
            if business_data_settings.get('include_phone') and business_data.get('phone'):
                business_info_parts.append(f"Phone: {business_data['phone']}")
            
            # Rating and reviews
            if business_data_settings.get('include_rating') and business_data.get('rating'):
                rating = business_data['rating']
                review_count = business_data.get('review_count', 0)
                if business_data_settings.get('include_reviews_count') and review_count > 0:
                    business_info_parts.append(f"Rating: {rating}/5 stars ({review_count} reviews)")
                else:
                    business_info_parts.append(f"Rating: {rating}/5 stars")
            
            # Categories
            if business_data_settings.get('include_categories') and business_data.get('categories'):
                category_titles = []
                for cat in business_data['categories'][:3]:
                    if isinstance(cat, dict):
                        category_titles.append(cat.get('title', cat.get('alias', str(cat))))
                    else:
                        category_titles.append(str(cat))
                if category_titles:
                    business_info_parts.append(f"Specializes in: {', '.join(category_titles)}")
            
            # Website
            if business_data_settings.get('include_website') and business_data.get('website'):
                business_info_parts.append("Website available")
            
            # Price range
            if business_data_settings.get('include_price_range') and business_data.get('price'):
                business_info_parts.append(f"Price range: {business_data['price']}")
            
            # Business hours and status
            if business_data_settings.get('include_hours'):
                if context.get('is_off_hours'):
                    business_info_parts.append("Currently closed - outside business hours")
                elif business_data.get('is_open_now'):
                    business_info_parts.append("Currently open")
            
            # Transactions/services
            if business_data_settings.get('include_transactions') and business_data.get('transactions'):
                transaction_items = [str(item) for item in business_data['transactions'][:3]]
                if transaction_items:
                    business_info_parts.append(f"Available services: {', '.join(transaction_items)}")
            
            business_info = "\n".join(business_info_parts) if business_info_parts else "No additional business information configured."
            
            # üîç VECTOR SEARCH CONTEXT for Custom Preview Text
            vector_context = ""
            if context.get('sample_replies_context'):
                logger.info(f"[AI-SERVICE] üîç Adding vector search context to preview prompt")
                similar_chunks = context['sample_replies_context']
                
                vector_parts = []
                for i, chunk in enumerate(similar_chunks[:3]):  # Top 3 most similar
                    similarity_score = chunk['similarity_score']
                    chunk_type = chunk['chunk_type']
                    content = chunk['content']
                    
                    vector_parts.append(
                        f"Similar Example {i+1} (similarity: {similarity_score:.2f}, type: {chunk_type}):\n{content}\n"
                    )
                
                vector_context = f"""
RELEVANT SAMPLE REPLIES (found via vector search):
{chr(10).join(vector_parts)}

STYLE GUIDANCE: Use the tone, approach, and communication style from the similar examples above. The higher the similarity score, the more closely you should match that style.
"""
                logger.info(f"[AI-SERVICE] üìù Added {len(similar_chunks)} vector chunks to preview context")
            
            # üîß –ü–û–ö–†–ê–©–ï–ù–Ü –Ü–ù–°–¢–†–£–ö–¶–Ü–á –ü–†–û –î–û–í–ñ–ò–ù–£ (–∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∏–π prompt)
            length_instruction = ""
            if max_length and max_length > 0:
                if max_length <= 160:
                    length_instruction = f"\n\nCRITICAL: Response MUST be under {max_length} characters (1-2 sentences). Be extremely concise but complete."
                elif max_length <= 250:
                    length_instruction = f"\n\nIMPORTANT: Keep under {max_length} characters (2-3 sentences). Concise but informative."
                elif max_length <= 320:
                    length_instruction = f"\n\nINSTRUCTION: Under {max_length} characters (3-4 sentences). Balanced detail."
                else:
                    length_instruction = f"\n\nGUIDANCE: Under {max_length} characters. Detailed but within limit."
            
            contextual_prompt = f"""Customer message:
"{customer_text}"

Customer name: {customer_name}
Business name: {business_name}

Business Information:
{business_info}{vector_context}{length_instruction}

Please analyze the customer's request and respond according to the instructions provided in the system prompt. Use the business information provided above when generating your response. Generate a complete, personalized response that fits the length requirement."""
            
            logger.info(f"[AI-SERVICE] üéØ Using contextual AI analysis with custom prompt")
            logger.info(f"[AI-SERVICE] Customer text length: {len(customer_text)} characters")
            logger.info(f"[AI-SERVICE] Business info parts: {len(business_info_parts)}")
            
            return contextual_prompt
        
        # üîç –ë–ï–ó CUSTOM INSTRUCTIONS - –∞–ª–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î Vector Search context
        customer_text = context.get('original_customer_text', '')
        customer_name = context.get('customer_name', 'there')
        business_name = context.get('business_name', 'our business')
        
        # –Ø–∫—â–æ —î Vector Search context - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ –Ω–∞–≤—ñ—Ç—å –±–µ–∑ custom prompt
        if context.get('sample_replies_context'):
            logger.info(f"[AI-SERVICE] üîç No custom instructions, but using Vector Search context for prompt")
            
            vector_context = ""
            similar_chunks = context['sample_replies_context']
            
            vector_parts = []
            for i, chunk in enumerate(similar_chunks[:3]):  # Top 3 most similar
                similarity_score = chunk['similarity_score']
                chunk_type = chunk['chunk_type']
                content = chunk['content']
                
                vector_parts.append(
                    f"Example {i+1} (similarity: {similarity_score:.2f}):\n{content}\n"
                )
            
            vector_context = f"""
RELEVANT SAMPLE REPLIES:
{chr(10).join(vector_parts)}

INSTRUCTIONS: Generate a professional response in the style of the examples above. Use the tone, approach, and communication patterns shown in the similar examples.
"""
            
            # üîß –ü–û–ö–†–ê–©–ï–ù–Ü –Ü–ù–°–¢–†–£–ö–¶–Ü–á –ü–†–û –î–û–í–ñ–ò–ù–£ (vector context prompt)
            length_instruction = ""
            if max_length and max_length > 0:
                if max_length <= 160:
                    length_instruction = f"\n\nCRITICAL: Response MUST be under {max_length} characters (1-2 sentences). Be extremely concise."
                elif max_length <= 250:
                    length_instruction = f"\n\nIMPORTANT: Keep under {max_length} characters (2-3 sentences). Concise but informative."
                elif max_length <= 320:
                    length_instruction = f"\n\nINSTRUCTION: Under {max_length} characters (3-4 sentences). Balanced detail."
                else:
                    length_instruction = f"\n\nGUIDANCE: Under {max_length} characters. Detailed but within limit."
            
            basic_prompt = f"""Customer message:
"{customer_text}"

Customer name: {customer_name}
Business name: {business_name}
{vector_context}{length_instruction}

Generate a personalized, professional response to the customer using the style guidance from the examples above. Make sure your response is complete and fits the length requirement."""
            
            logger.info(f"[AI-SERVICE] ‚úÖ Using Vector Search context without custom prompt")
            return basic_prompt
        
        # üîß –Ø–∫—â–æ –Ω–µ–º–∞—î –Ω—ñ custom prompt, –Ω—ñ vector context - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±–∞–∑–æ–≤–∏–π prompt –∑ business data
        logger.info(f"[AI-SERVICE] üíº No custom instructions/vector context - using basic prompt with business data")
        
        # –§–æ—Ä–º—É—î–º–æ –±–∞–∑–æ–≤–∏–π prompt –∑ business data
        customer_text = context.get('original_customer_text', '')
        customer_name = context.get('customer_name', 'there')
        business_name = context.get('business_name', 'our business')
        business_data = context.get('business_data', {})
        business_data_settings = context.get('business_data_settings', {})
        
        # –§–æ—Ä–º—É—î–º–æ business information –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ prompt
        business_info_parts = []
        
        # Rating and reviews
        if business_data_settings.get('include_rating') and business_data.get('rating'):
            rating = business_data['rating']
            review_count = business_data.get('review_count', 0)
            if business_data_settings.get('include_reviews_count') and review_count > 0:
                business_info_parts.append(f"Rating: {rating}/5 stars ({review_count} reviews)")
            else:
                business_info_parts.append(f"Rating: {rating}/5 stars")
        
        # Categories
        if business_data_settings.get('include_categories') and business_data.get('categories'):
            category_titles = []
            for cat in business_data['categories'][:3]:
                if isinstance(cat, dict):
                    category_titles.append(cat.get('title', cat.get('alias', str(cat))))
                else:
                    category_titles.append(str(cat))
            if category_titles:
                business_info_parts.append(f"Specializes in: {', '.join(category_titles)}")
        
        # Phone
        if business_data_settings.get('include_phone') and business_data.get('phone'):
            business_info_parts.append(f"Phone: {business_data['phone']}")
        
        # Price range
        if business_data_settings.get('include_price_range') and business_data.get('price'):
            business_info_parts.append(f"Price range: {business_data['price']}")
        
        business_info = "\n".join(business_info_parts) if business_info_parts else "No additional business information configured."
        
        # üîß –ü–û–ö–†–ê–©–ï–ù–Ü –Ü–ù–°–¢–†–£–ö–¶–Ü–á –ü–†–û –î–û–í–ñ–ò–ù–£ (–æ—Å–Ω–æ–≤–Ω–∏–π prompt)
        length_instruction = ""
        if max_length and max_length > 0:
            if max_length <= 160:
                length_instruction = f"\n\nCRITICAL: Response MUST be under {max_length} characters (1-2 sentences max). Be extremely concise but complete. Every word counts."
            elif max_length <= 250:
                length_instruction = f"\n\nIMPORTANT: Keep under {max_length} characters (2-3 sentences). Concise but informative."
            elif max_length <= 320:
                length_instruction = f"\n\nINSTRUCTION: Under {max_length} characters (3-4 sentences). Balanced detail."
            else:
                length_instruction = f"\n\nGUIDANCE: Under {max_length} characters. Detailed information within limit."
        
        basic_prompt = f"""Customer message:
"{customer_text}"

Customer name: {customer_name}
Business name: {business_name}

Business Information:
{business_info}{length_instruction}

Generate a personalized, professional response to the customer. Use the business information provided above. Make sure your response is complete and fits the length requirement."""
        
        logger.info(f"[AI-SERVICE] ‚úÖ Generated basic prompt with business data (length instructions: {bool(max_length)})")
        return basic_prompt
    
    def _fallback_message(self, context: Dict[str, Any]) -> str:
        """Fallback –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —è–∫—â–æ AI –Ω–µ –ø—Ä–∞—Ü—é—î"""
        name = context.get('customer_name', '')
        services = context.get('services', 'our services')
        business = context.get('business_name', 'our business')
        
        if name and name != 'there':
            return f"Hello {name}! Thank you for your inquiry about {services}. We'll get back to you soon!"
        else:
            return f"Hello! Thank you for your inquiry about {services}. We'll get back to you soon!"
    
    def _parse_lead_data(self, lead_detail: LeadDetail, custom_prompt: Optional[str] = None) -> Dict[str, str]:
        """ü§ñ AI-powered –ø–∞—Ä—Å–∏–Ω–≥ —â–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—Ç—è–≥—É—î –±—É–¥—å-—è–∫—ñ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∏ –∑ custom prompt"""
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç –∑ additional_info –∞–±–æ –∑ job_names
        text = self._get_lead_text(lead_detail)
        
        if not text:
            logger.info(f"[AI-SERVICE] No text found in lead data")
            return {}
        
        logger.info(f"[AI-SERVICE] AI-powered parsing from text: {text[:100]}...")
        
        # –Ø–∫—â–æ –Ω–µ–º–∞—î custom prompt - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ fallback –ø–∞—Ä—Å–∏–Ω–≥
        if not custom_prompt:
            return self._fallback_parsing(text)
        
        # –í–∏—Ç—è–≥—É—î–º–æ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∏ –∑ custom prompt
        placeholders = re.findall(r'\{(\w+)\}', custom_prompt)
        
        if not placeholders:
            logger.info(f"[AI-SERVICE] No placeholders found in custom prompt")
            return self._fallback_parsing(text)
        
        logger.info(f"[AI-SERVICE] Found placeholders: {placeholders}")
        
        # –ü—Ä–æ–±—É—î–º–æ AI extraction
        try:
            return self._ai_extract_fields(text, placeholders)
        except Exception as e:
            logger.error(f"[AI-SERVICE] AI extraction failed: {e}")
            return self._fallback_parsing(text)
    
    def _get_lead_text(self, lead_detail: LeadDetail) -> str:
        """üìù –û—Ç—Ä–∏–º—É—î —Ç–µ–∫—Å—Ç –∑ –ø–µ—Ä—à–æ–≥–æ TEXT —ñ–≤–µ–Ω—Ç—É –≤—ñ–¥ CONSUMER –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É"""
        from .models import LeadEvent
        
        # üéØ –ü–†–Ü–û–†–ò–¢–ï–¢: –®—É–∫–∞—î–º–æ –ø–µ—Ä—à–∏–π TEXT —ñ–≤–µ–Ω—Ç –≤—ñ–¥ CONSUMER
        first_consumer_text = LeadEvent.objects.filter(
            lead_id=lead_detail.lead_id,
            event_type="TEXT",
            user_type="CONSUMER",
            from_backend=False  # –ù–µ –Ω–∞—à–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        ).order_by('time_created').first()
        
        if first_consumer_text and first_consumer_text.text:
            logger.info(f"[AI-SERVICE] üìù Using TEXT event from CONSUMER: {first_consumer_text.text[:100]}...")
            return first_consumer_text.text
        
        # üîÑ FALLBACK 1: project.additional_info
        project_data = lead_detail.project or {}
        additional_info = project_data.get("additional_info", "")
        
        if additional_info:
            logger.info(f"[AI-SERVICE] üîÑ Fallback to project.additional_info: {additional_info[:100]}...")
            return additional_info
        
        # üîÑ FALLBACK 2: project.job_names
        job_names = project_data.get("job_names", [])
        if job_names:
            text = " ".join(job_names)
            logger.info(f"[AI-SERVICE] üîÑ Fallback to project.job_names: {text}")
            return text
        
        logger.warning(f"[AI-SERVICE] ‚ö†Ô∏è No text found for lead {lead_detail.lead_id}")
        return ""
    
    def _ai_extract_fields(self, text: str, placeholders: list) -> Dict[str, str]:
        """–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î AI –¥–ª—è –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø–æ–ª—ñ–≤ –∑ —Ç–µ–∫—Å—Ç—É"""
        
        # –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–∞—Ö–∞—Ä–¥–∫–æ–¥–∂–µ–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏ –¥–ª—è extraction
        extraction_prompt = f"Customer message: {text}"
        
        logger.info(f"[AI-SERVICE] Sending extraction prompt to AI...")
        
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±—ñ–ª—å—à –¥–µ—à–µ–≤—É –º–æ–¥–µ–ª—å –¥–ª—è extraction
            extraction_model = "gpt-4o-mini"
            system_prompt = ""  # –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–∞—Ö–∞—Ä–¥–∫–æ–¥–∂–µ–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ
            messages = self._prepare_messages_for_model(extraction_model, system_prompt, extraction_prompt)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å –º–æ–¥–µ–ª—ñ
            api_params = self._get_api_params_for_model(extraction_model, messages, 200, 0.1)
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–π –∂–µ OpenAI –∫–ª—ñ—î–Ω—Ç
            response = self.client.chat.completions.create(**api_params)
            
            ai_response = response.choices[0].message.content.strip()
            logger.info(f"[AI-SERVICE] AI extraction response: {ai_response}")
            
            # –û—á–∏—â–∞—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –º–æ–∂–ª–∏–≤–æ–≥–æ markdown
            if ai_response.startswith('```'):
                ai_response = ai_response.split('\n', 1)[1].rsplit('\n', 1)[0]
            
            # –ü–∞—Ä—Å–∏–º–æ JSON
            result = json.loads(ai_response)
            
            # –í–∞–ª—ñ–¥—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if isinstance(result, dict):
                # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–∞–ø–∏—Ç—É–≤–∞–Ω—ñ –ø–æ–ª—è
                filtered_result = {key: str(value) for key, value in result.items() if key in placeholders}
                logger.info(f"[AI-SERVICE] ‚úÖ AI extraction successful: {filtered_result}")
                return filtered_result
            else:
                logger.warning(f"[AI-SERVICE] AI returned non-dict result: {result}")
                return {}
                
        except json.JSONDecodeError as e:
            logger.error(f"[AI-SERVICE] Failed to parse AI JSON response: {e}")
            return {}
        except Exception as e:
            logger.error(f"[AI-SERVICE] AI extraction error: {e}")
            return {}
    
    def _fallback_parsing(self, text: str) -> Dict[str, str]:
        """Fallback –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –±–∞–∑–æ–≤–∏—Ö –ø–æ–ª—ñ–≤ (contracting –±—ñ–∑–Ω–µ—Å)"""
        
        result = {}
        text_lower = text.lower()
        
        # –ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ service_type
        if "structural repair" in text_lower or "structure repair" in text_lower:
            result["service_type"] = "Structural repair"
        elif "remodeling" in text_lower or "remodel" in text_lower:
            result["service_type"] = "Remodeling"
        elif "addition" in text_lower:
            result["service_type"] = "Additions to an existing structure"
        elif "new structure" in text_lower or "new construction" in text_lower:
            result["service_type"] = "New structure construction"
        elif "design" in text_lower:
            result["service_type"] = "Construction design services"
        
        # –ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ sub_option –¥–ª—è structural repair
        if result.get("service_type") == "Structural repair":
            if "foundation" in text_lower:
                result["sub_option"] = "Foundation"
            elif "roof" in text_lower:
                result["sub_option"] = "Roof frame"
            elif "wall" in text_lower:
                result["sub_option"] = "Walls"
        
        # –ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ timeline
        if "as soon as possible" in text_lower or "asap" in text_lower:
            result["timeline"] = "As soon as possible"
        elif "flexible" in text_lower:
            result["timeline"] = "I'm flexible"
        
        # –ë–∞–∑–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥ ZIP
        zip_match = re.search(r'\b\d{5}\b', text)
        if zip_match:
            result["zip"] = zip_match.group()
        
        logger.info(f"[AI-SERVICE] üîÑ Fallback parsing result: {result}")
        return result 

    def generate_sample_replies_response(
        self, 
        lead_detail: LeadDetail, 
        business: Optional[YelpBusiness] = None,
        max_length: Optional[int] = None,
        business_ai_settings: Optional['AutoResponseSettings'] = None,
        use_vector_search: bool = True
    ) -> Optional[str]:
        """üîç –†–µ–∂–∏–º 2: –ì–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ—à—É–∫—É Sample Replies (—Ç—ñ–ª—å–∫–∏ –¥–ª—è AI Generated)"""
        
        if not business:
            logger.warning("[AI-SERVICE] No business provided for Mode 2 vector search")
            return None
        
        logger.info(f"[AI-SERVICE] ========== MODE 2: VECTOR SAMPLE REPLIES AI GENERATION ==========")
        logger.info(f"[AI-SERVICE] Lead ID: {lead_detail.lead_id}")
        logger.info(f"[AI-SERVICE] Business: {business.name} ({business.business_id})")
        logger.info(f"[AI-SERVICE] Use vector search: {use_vector_search}")
        
        if not self.is_available():
            logger.error("[AI-SERVICE] ‚ùå OpenAI client not available")
            return None
        
        if not self.check_rate_limit():
            logger.warning("[AI-SERVICE] ‚ö†Ô∏è Rate limit exceeded, skipping Sample Replies AI generation")
            return None
        
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç –ª—ñ–¥–∞ –¥–ª—è –ø–æ—à—É–∫—É
            lead_inquiry = self._get_lead_text(lead_detail)
            
            if not lead_inquiry:
                logger.warning("[AI-SERVICE] No lead inquiry text found")
                return None
            
            logger.info(f"[AI-SERVICE] Lead inquiry: {lead_inquiry[:200]}...")
            
            customer_name = lead_detail.user_display_name or "there"
            response_length = max_length or 160
            
            if use_vector_search:
                # üîç –í–ï–ö–¢–û–†–ù–ò–ô –ü–û–®–£–ö: –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂—ñ —á–∞–Ω–∫–∏
                try:
                    from .vector_search_service import vector_search_service
                    
                    # üéØ –ù–û–í–ò–ô –ü–Ü–î–•–Ü–î: Inquiry‚ÜíResponse pair matching
                    inquiry_response_pairs = vector_search_service.search_inquiry_response_pairs(
                        query_text=lead_inquiry,
                        business_id=business.business_id,
                        location_id=None,  # TODO: Add location support if needed
                        limit=business_ai_settings.vector_search_limit if business_ai_settings else 5,
                        similarity_threshold=business_ai_settings.vector_similarity_threshold if business_ai_settings else 0.6
                    )
                    
                    if not inquiry_response_pairs:
                        logger.warning("[AI-SERVICE] No similar inquiry‚Üíresponse pairs found via vector search")
                        return None
                    
                    logger.info(f"[AI-SERVICE] Found {len(inquiry_response_pairs)} inquiry‚Üíresponse pairs via vector search")
                    for i, pair in enumerate(inquiry_response_pairs[:3]):
                        logger.info(f"[AI-SERVICE] Pair {i+1}: similarity={pair['pair_similarity']:.3f}, quality={pair['pair_quality']}")
                        logger.info(f"[AI-SERVICE]   Inquiry: {pair['inquiry']['content'][:80]}...")
                        logger.info(f"[AI-SERVICE]   Response: {pair['response']['content'][:80]}...")
                    
                    # ü§ñ –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ö–û–ù–¢–ï–ö–°–¢–£–ê–õ–¨–ù–û–á –í–Ü–î–ü–û–í–Ü–î–Ü –ó –ü–ê–†
                    contextual_response = vector_search_service.generate_contextual_response_from_pairs(
                        lead_inquiry=lead_inquiry,
                        customer_name=customer_name,
                        inquiry_response_pairs=inquiry_response_pairs,
                        business_name=business.name,
                        max_response_length=response_length
                    )
                    
                    if contextual_response:
                        logger.info(f"[AI-SERVICE] üéâ MODE 2 VECTOR: Generated contextual response:")
                        logger.info(f"[AI-SERVICE] - Length: {len(contextual_response)} chars")
                        logger.info(f"[AI-SERVICE] - Response: '{contextual_response}'")
                        logger.info(f"[AI-SERVICE] - Based on {len(inquiry_response_pairs)} inquiry‚Üíresponse pairs")
                        logger.info(f"[AI-SERVICE] ==============================================")
                        
                        return contextual_response
                    else:
                        logger.warning("[AI-SERVICE] Contextual response generation failed")
                        return None
                        
                except Exception as vector_error:
                    logger.error(f"[AI-SERVICE] Vector search failed: {vector_error}")
                    logger.warning("[AI-SERVICE] Falling back to legacy Sample Replies method...")
                    # Fallback to legacy method if vector search fails
                    use_vector_search = False
            
            # üìÑ LEGACY FALLBACK: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞—Ä–∏–π –º–µ—Ç–æ–¥ –∑ –ø–æ–≤–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º
            if not use_vector_search and business_ai_settings and business_ai_settings.sample_replies_content:
                logger.info("[AI-SERVICE] Using legacy Sample Replies method as fallback...")
                return self._generate_legacy_sample_replies_response(
                    lead_detail, business, business_ai_settings.sample_replies_content, 
                    response_length, business_ai_settings
                )
            
            logger.warning("[AI-SERVICE] No Sample Replies available for Mode 2")
            return None
            
        except Exception as e:
            logger.error(f"[AI-SERVICE] ‚ùå Error in Sample Replies generation (Mode 2): {e}")
            logger.exception("Sample replies generation error details")
            return None
    
    def _generate_legacy_sample_replies_response(
        self,
        lead_detail: LeadDetail,
        business: YelpBusiness,
        sample_replies_content: str,
        max_length: int,
        business_ai_settings: 'AutoResponseSettings'
    ) -> Optional[str]:
        """Legacy –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑ –ø–æ–≤–Ω–∏–º Sample Replies —Ç–µ–∫—Å—Ç–æ–º (fallback)"""
        
        try:
            lead_inquiry = self._get_lead_text(lead_detail)
            customer_name = lead_detail.user_display_name or "there"
            
            # –û—Ç—Ä–∏–º—É—î–º–æ AI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
            ai_config = self._get_ai_settings(business_ai_settings)
            model = ai_config['model']
            temperature = ai_config['temperature']
            
            # –°–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è legacy —Ä–µ–∂–∏–º—É
            system_prompt = f"""You are a professional business communication assistant for {business.name}.

MODE 2: AI GENERATED - LEGACY SAMPLE REPLIES METHOD

TASK: Generate a personalized response using the full sample replies content as training data.

SAMPLE REPLIES TRAINING DATA:
{sample_replies_content}

INSTRUCTIONS:
1. Study the sample replies to understand communication style and approach
2. Generate a NEW response matching this learned style for the current inquiry
3. Keep under {max_length} characters
4. Be professional and personalized"""
            
            user_prompt = f"""Customer: {customer_name}
Inquiry: "{lead_inquiry}"

Generate a response in the style of the sample replies provided."""
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–∞ –≤–∏–∫–ª–∏–∫ API
            messages = self._prepare_messages_for_model(model, system_prompt, user_prompt)
            api_params = self._get_api_params_for_model(model, messages, max_length, temperature)
            
            response = self.client.chat.completions.create(**api_params)
            ai_message = response.choices[0].message.content.strip()
            
            logger.info(f"[AI-SERVICE] üìÑ Legacy Sample Replies response: {ai_message}")
            return ai_message
            
        except Exception as e:
            logger.error(f"[AI-SERVICE] Legacy Sample Replies generation failed: {e}")
            return None